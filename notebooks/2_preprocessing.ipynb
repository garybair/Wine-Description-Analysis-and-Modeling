{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing - Wine.com\n",
    "\n",
    "### Summary\n",
    "The code contained within this file aims to reconcile and process data scraped from Wine.com.\n",
    "\n",
    "This implementation observes the following considerations:\n",
    "1. Given that scrapes can and will fail, this script assumes that all files present in the raw folder are to be processed and combined into a master record. Redundant records and processing are prevented through the use of a site map that tracks successfully parsed and written pages.\n",
    "2. Ideally, this data would best be stored in a database with a mininum of two tables, product info and critical reviews, however as this analysis is predominantly interested in review data, a SQL style left join has been deemed adequate for this analysis despite the effect on the overall file size. This is a case of not letting perfect get in the way of good enough or having an \"agile\" mindset.\n",
    "3. The preprocess_text.py file contains the preprocess_text Class to be used in pipeline when addressing novel data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransformerMixin\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mainenv\\lib\\site-packages\\sklearn\\__init__.py:82\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _distributor_init  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __check_build  \u001b[38;5;66;03m# noqa: F401\u001b[39;00m\n\u001b[1;32m---> 82\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m clone\n\u001b[0;32m     83\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_show_versions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m show_versions\n\u001b[0;32m     85\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     86\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcalibration\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m     87\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    128\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mshow_versions\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    129\u001b[0m ]\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mainenv\\lib\\site-packages\\sklearn\\base.py:17\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m __version__\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_config\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_config\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _IS_32BIT\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_set_output\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _SetOutputMixin\n\u001b[0;32m     19\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_tags\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     20\u001b[0m     _DEFAULT_TAGS,\n\u001b[0;32m     21\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mainenv\\lib\\site-packages\\sklearn\\utils\\__init__.py:25\u001b[0m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdiscovery\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m all_estimators\n\u001b[1;32m---> 25\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfixes\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m parse_version, threadpool_info\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_estimator_html_repr\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m estimator_html_repr\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mvalidation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     as_float_array,\n\u001b[0;32m     29\u001b[0m     assert_all_finite,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     38\u001b[0m     _is_arraylike_not_scalar,\n\u001b[0;32m     39\u001b[0m )\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mainenv\\lib\\site-packages\\sklearn\\utils\\fixes.py:19\u001b[0m\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\n\u001b[1;32m---> 19\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mthreadpoolctl\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdeprecation\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m deprecated\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mainenv\\lib\\site-packages\\scipy\\stats\\__init__.py:494\u001b[0m\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m mstats\n\u001b[0;32m    493\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m qmc\n\u001b[1;32m--> 494\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_multivariate\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[0;32m    495\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m contingency\n\u001b[0;32m    496\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcontingency\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m chi2_contingency\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mainenv\\lib\\site-packages\\scipy\\stats\\_multivariate.py:17\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlinalg\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlapack\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_lapack_funcs\n\u001b[0;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_discrete_distns\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m binom\n\u001b[1;32m---> 17\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m _mvn, _covariance, _rcont\n\u001b[0;32m     19\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmultivariate_normal\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     20\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatrix_normal\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     21\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdirichlet\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     31\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrandom_table\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m     32\u001b[0m            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124muniform_direction\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     34\u001b[0m _LOG_2PI \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mlog(\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m*\u001b[39m np\u001b[38;5;241m.\u001b[39mpi)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\mainenv\\lib\\site-packages\\scipy\\stats\\_rcont\\__init__.py:3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# -*- coding: utf-8 -*-\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrcont\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m rvs_rcont1, rvs_rcont2\n\u001b[0;32m      5\u001b[0m __all__ \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrvs_rcont1\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrvs_rcont2\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:389\u001b[0m, in \u001b[0;36mparent\u001b[1;34m(self)\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "from sklearn.base import TransformerMixin\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set directory locations\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "raw_folder = parent_directory + '/data/wine-com/raw/'\n",
    "processed_folder = parent_directory + '/data/wine-com/processed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1677386048.5362737.txt loading\n",
      "1677417882.7459548.txt loading\n"
     ]
    }
   ],
   "source": [
    "#instantiate data structure to hold initial raw data/critical critical reviews\n",
    "data = dict()\n",
    "review_data = dict()\n",
    "#iterate through wine.com raw data folder\n",
    "for filename in os.listdir(raw_folder):\n",
    "    #determine if file has already been processed - if true pass\n",
    "    if filename not in os.listdir(processed_folder):\n",
    "        print(f'{filename} loading')\n",
    "        data[filename] = dict()\n",
    "        review_data[filename] = dict()\n",
    "        #open file readlines to raw dictionary\n",
    "        with open(raw_folder + filename, newline='\\r\\n') as file:\n",
    "            header = next(file)\n",
    "            data[filename]['lines'] = file.readlines()\n",
    "        #create list to hold parsed records\n",
    "        data[filename]['data'] = []\n",
    "        review_data[filename]['data'] = []\n",
    "        #iterate through records to parse raw data\n",
    "        for line in data[filename]['lines']:\n",
    "            values = line.strip().split('|')\n",
    "            #while redundancy is built into the scraper to ensure that the correct format is achieved,\n",
    "            #this prevents rogue html from causing a bad data load\n",
    "            if len(values) == 9:\n",
    "                row = {\n",
    "                    'product_url': values[0],\n",
    "                    'product_name': values[1],\n",
    "                    'product_variety': values[2],\n",
    "                    'product_origin': values[3],\n",
    "                    'product_family': values[4],\n",
    "                    'user_avg_rating': values[5],\n",
    "                    'user_rating_count': values[6],\n",
    "                    'winemaker_description': values[7]\n",
    "                    #'critical_reviews' is parsed loaded below\n",
    "                }\n",
    "                #write to list to be loaded to dataframe\n",
    "                data[filename]['data'].append(row)\n",
    "                \n",
    "                #parse critical reviews - should be loaded as list\n",
    "                reviews = values[8].split(';')\n",
    "                for review in reviews:\n",
    "                    try:\n",
    "                        reviewer_name, reviewer_rating, reviewer_text = review.split(',')\n",
    "                        review = {\n",
    "                        'product_url': values[0],\n",
    "                        'reviewer_name': reviewer_name,\n",
    "                        'reviewer_rating': reviewer_rating,\n",
    "                        'reviewer_text': reviewer_text\n",
    "                        }\n",
    "                        \n",
    "                        review_data[filename]['data'].append(review)\n",
    "                    except Exception:\n",
    "                        pass      \n",
    "                    \n",
    "        # Convert the list of dictionaries to a Pandas DataFrame\n",
    "        data[filename]['unmerged_df'] = pd.DataFrame(data[filename]['data'])\n",
    "        review_data[filename]['df'] = pd.DataFrame(review_data[filename]['data'])\n",
    "        \n",
    "        #perform sql-style left join of review data onto main df\n",
    "        data[filename]['merged_df'] = pd.merge(data[filename]['unmerged_df'], \n",
    "                                               review_data[filename]['df'], \n",
    "                                               on='product_url', \n",
    "                                               how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a text preprocessor class is created to manage preprocessing of text fields\n",
    "# inheriting from TransformerMixin of Sklearn, this should allow for tying into large sklearn pipeline\n",
    "class preprocess_text(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if isinstance(X, pd.Series):\n",
    "            #standardize type\n",
    "            X = X.astype(str)\n",
    "            # lower text\n",
    "            X = X.str.lower()\n",
    "            # remove punctuation\n",
    "            X = X.str.replace('\\W+', ' ', regex = True)\n",
    "            # tokenize text into individual words\n",
    "            X = X.str.split()\n",
    "            # remove stopwords\n",
    "            X = X.apply(lambda x: [word for word in x if word not in (self.stop_words)])\n",
    "            # join words\n",
    "            X = X.apply(lambda x: ' '.join(x))\n",
    "            return X\n",
    "        elif isinstance(X, pd.DataFrame):\n",
    "            colnames = X.columns\n",
    "            for col in colnames:\n",
    "                #standardize type\n",
    "                X[col] = X[col].astype(str)\n",
    "                # convert text to lowercase\n",
    "                X[col] = X[col].str.lower()\n",
    "                # remove punctuation\n",
    "                X[col] = X[col].str.replace('\\W+', ' ', regex = True)\n",
    "                # tokenize text into individual words\n",
    "                X[col] = X[col].str.split()\n",
    "                # remove stopwords\n",
    "                X[col] = X[col].apply(lambda x: [word for word in x if word not in (self.stop_words)])\n",
    "                # join words\n",
    "                X[col] = X[col].apply(lambda x: ' '.join(x))\n",
    "            return X\n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean & Write Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "processor = preprocess_text()\n",
    "\n",
    "output_path = processed_folder + f'{time.time()}.txt'\n",
    "\n",
    "for filename in data:\n",
    "    data[filename]['merged_df']['winemaker_description'] = processor.transform(data[filename]['merged_df']['winemaker_description'])\n",
    "    data[filename]['merged_df']['reviewer_text'] = processor.transform(data[filename]['merged_df']['reviewer_text'])\n",
    "    data[filename]['merged_df'].to_csv(output_path,\n",
    "                                       mode='a',\n",
    "                                       header = not os.path.exists(output_path),\n",
    "                                       sep = '|',\n",
    "                                       line_terminator = '\\r\\n',\n",
    "                                       index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
