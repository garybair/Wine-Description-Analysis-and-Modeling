{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Topic Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import nltk\n",
    "#nltk.download('brown')\n",
    "from nltk.corpus import brown\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "import pyLDAvis\n",
    "import pyLDAvis.sklearn\n",
    "import pyLDAvis.gensim_models\n",
    "\n",
    "import spacy\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.decomposition import NMF, TruncatedSVD, LatentDirichletAllocation\n",
    "\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "\n",
    "from collections import Counter, defaultdict\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory locations\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "processed_folder = parent_directory + '/data/wine-com/processed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LoadData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(processed_folder + '1677432096.083379.txt', sep = '|')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>product_url</th>\n",
       "      <th>product_name</th>\n",
       "      <th>product_variety</th>\n",
       "      <th>product_origin</th>\n",
       "      <th>product_family</th>\n",
       "      <th>user_avg_rating</th>\n",
       "      <th>user_rating_count</th>\n",
       "      <th>winemaker_description</th>\n",
       "      <th>reviewer_name</th>\n",
       "      <th>reviewer_rating</th>\n",
       "      <th>reviewer_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.wine.com/product/proyecto-salvaje-...</td>\n",
       "      <td>Proyecto Salvaje del Moncayo Garnacha 2020</td>\n",
       "      <td>Grenache</td>\n",
       "      <td>from Navarra, Spain</td>\n",
       "      <td>Red Wine</td>\n",
       "      <td>4.8</td>\n",
       "      <td>19</td>\n",
       "      <td>bright burgundy wine medium depth tobacco wild...</td>\n",
       "      <td>Decanter</td>\n",
       "      <td>92.0</td>\n",
       "      <td>part proyecto garnachas de españa collection s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.wine.com/product/proyecto-salvaje-...</td>\n",
       "      <td>Proyecto Salvaje del Moncayo Garnacha 2020</td>\n",
       "      <td>Grenache</td>\n",
       "      <td>from Navarra, Spain</td>\n",
       "      <td>Red Wine</td>\n",
       "      <td>4.8</td>\n",
       "      <td>19</td>\n",
       "      <td>bright burgundy wine medium depth tobacco wild...</td>\n",
       "      <td>Wilfred Wong of Wine.com</td>\n",
       "      <td>91.0</td>\n",
       "      <td>commentary 2020 proyecto garnachas salvaje del...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.wine.com/product/domaine-du-terme-...</td>\n",
       "      <td>Domaine du Terme Gigondas 2019</td>\n",
       "      <td>Rhone Red Blends</td>\n",
       "      <td>from Gigondas, Rhone, France</td>\n",
       "      <td>Red Wine</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Wine &amp; Spirits</td>\n",
       "      <td>96.0</td>\n",
       "      <td>spectacular gigondas wine red cherry flavors s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.wine.com/product/domaine-du-terme-...</td>\n",
       "      <td>Domaine du Terme Gigondas 2019</td>\n",
       "      <td>Rhone Red Blends</td>\n",
       "      <td>from Gigondas, Rhone, France</td>\n",
       "      <td>Red Wine</td>\n",
       "      <td>4.0</td>\n",
       "      <td>17</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Decanter</td>\n",
       "      <td>94.0</td>\n",
       "      <td>straight first sniff clear going special soari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.wine.com/product/scott-harvey-moun...</td>\n",
       "      <td>Scott Harvey Mountain Selection Zinfandel 2019</td>\n",
       "      <td>Zinfandel</td>\n",
       "      <td>from Amador, Sierra Foothills, California</td>\n",
       "      <td>Red Wine</td>\n",
       "      <td>4.3</td>\n",
       "      <td>39</td>\n",
       "      <td>fruit forward rich full flavors expressing var...</td>\n",
       "      <td>Wine Enthusiast</td>\n",
       "      <td>93.0</td>\n",
       "      <td>fresh smelling full bodied flavor packed wine ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         product_url  \\\n",
       "0  https://www.wine.com/product/proyecto-salvaje-...   \n",
       "1  https://www.wine.com/product/proyecto-salvaje-...   \n",
       "2  https://www.wine.com/product/domaine-du-terme-...   \n",
       "3  https://www.wine.com/product/domaine-du-terme-...   \n",
       "4  https://www.wine.com/product/scott-harvey-moun...   \n",
       "\n",
       "                                     product_name   product_variety  \\\n",
       "0      Proyecto Salvaje del Moncayo Garnacha 2020          Grenache   \n",
       "1      Proyecto Salvaje del Moncayo Garnacha 2020          Grenache   \n",
       "2                  Domaine du Terme Gigondas 2019  Rhone Red Blends   \n",
       "3                  Domaine du Terme Gigondas 2019  Rhone Red Blends   \n",
       "4  Scott Harvey Mountain Selection Zinfandel 2019         Zinfandel   \n",
       "\n",
       "                               product_origin product_family  user_avg_rating  \\\n",
       "0                         from Navarra, Spain       Red Wine              4.8   \n",
       "1                         from Navarra, Spain       Red Wine              4.8   \n",
       "2                from Gigondas, Rhone, France       Red Wine              4.0   \n",
       "3                from Gigondas, Rhone, France       Red Wine              4.0   \n",
       "4   from Amador, Sierra Foothills, California       Red Wine              4.3   \n",
       "\n",
       "   user_rating_count                              winemaker_description  \\\n",
       "0                 19  bright burgundy wine medium depth tobacco wild...   \n",
       "1                 19  bright burgundy wine medium depth tobacco wild...   \n",
       "2                 17                                                NaN   \n",
       "3                 17                                                NaN   \n",
       "4                 39  fruit forward rich full flavors expressing var...   \n",
       "\n",
       "              reviewer_name  reviewer_rating  \\\n",
       "0                  Decanter             92.0   \n",
       "1  Wilfred Wong of Wine.com             91.0   \n",
       "2            Wine & Spirits             96.0   \n",
       "3                  Decanter             94.0   \n",
       "4           Wine Enthusiast             93.0   \n",
       "\n",
       "                                       reviewer_text  \n",
       "0  part proyecto garnachas de españa collection s...  \n",
       "1  commentary 2020 proyecto garnachas salvaje del...  \n",
       "2  spectacular gigondas wine red cherry flavors s...  \n",
       "3  straight first sniff clear going special soari...  \n",
       "4  fresh smelling full bodied flavor packed wine ...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function comes from the BTAP repo.\n",
    "def display_topics(model, features, no_top_words=5):\n",
    "    for topic, words in enumerate(model.components_):\n",
    "        total = words.sum()\n",
    "        largest = words.argsort()[::-1] # invert sort order\n",
    "        print(\"\\nTopic %02d\" % topic)\n",
    "        for i in range(0, no_top_words):\n",
    "            print(\"  %s (%2.2f)\" % (features[largest[i]], abs(words[largest[i]]*100.0/total)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what type of wine in the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Red Wine we have 12603 reviews.\n",
      "For White Wine we have 5128 reviews.\n",
      "For Champagne & Sparkling we have 1961 reviews.\n",
      "For Rosé Wine we have 479 reviews.\n",
      "For Dessert, Sherry, & Port we have 817 reviews.\n"
     ]
    }
   ],
   "source": [
    "# Check if the 'category' column exists in the DataFrame\n",
    "if 'product_family' in df.columns:\n",
    "    # Get a list of all unique categories in the DataFrame\n",
    "    categories = df['product_family'].unique()\n",
    "\n",
    "    # Loop through each category and print the number of articles in the DataFrame for that category\n",
    "    for category in categories:\n",
    "        num_reviews = len(df[df['product_family'] == category])\n",
    "        print(f\"For {category} we have {num_reviews} reviews.\")\n",
    "else:\n",
    "    print(\"The 'product_family' column does not exist in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## what review topics of wine in the df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 14494 records in the DataFrame.\n"
     ]
    }
   ],
   "source": [
    "# Check if the 'reviews' column exists in the DataFrame\n",
    "if 'reviewer_text' in df.columns:\n",
    "    # Count the number of non-null values in the 'reviews' column\n",
    "    num_reviews = df['reviewer_text'].count()\n",
    "\n",
    "    # Print the total number of reviews in the DataFrame\n",
    "    print(f\"There are {num_reviews} records in the DataFrame.\")\n",
    "else:\n",
    "    print(\"The 'reviews' column does not exist in the DataFrame.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20988, 4)\n"
     ]
    }
   ],
   "source": [
    "# Define the wine types to search for\n",
    "wine_types = df['product_family'].unique()\n",
    "\n",
    "# Define empty lists to hold the category, id, text, and review data for each wine type\n",
    "category_list = []\n",
    "id_list = []\n",
    "text_list = []\n",
    "review_list = []\n",
    "\n",
    "# Loop through each wine type\n",
    "for wine_type in wine_types:\n",
    "    # Filter the DataFrame to select only the rows that match the wine type\n",
    "    filtered_df = df[df['product_family'].str.contains(wine_type, case=False)]\n",
    "\n",
    "    # Loop through each row in the filtered DataFrame\n",
    "    for index, row in filtered_df.iterrows():\n",
    "        # Add the category, id, text, and review data to their respective lists\n",
    "        category_list.append(wine_type)\n",
    "        id_list.append(row['product_name'])\n",
    "        text_list.append(row['winemaker_description'])\n",
    "        review_list.append(row['reviewer_text'])\n",
    "\n",
    "# Create a new DataFrame from the category, id, text, and review lists\n",
    "wine_df = pd.DataFrame({\n",
    "    'category': category_list,\n",
    "    'id': id_list,\n",
    "    'text': text_list,\n",
    "    'reviews': review_list\n",
    "})\n",
    "\n",
    "# Print the shape of the new DataFrame\n",
    "print(wine_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>id</th>\n",
       "      <th>text</th>\n",
       "      <th>reviews</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Red Wine</td>\n",
       "      <td>Proyecto Salvaje del Moncayo Garnacha 2020</td>\n",
       "      <td>bright burgundy wine medium depth tobacco wild...</td>\n",
       "      <td>part proyecto garnachas de españa collection s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Red Wine</td>\n",
       "      <td>Proyecto Salvaje del Moncayo Garnacha 2020</td>\n",
       "      <td>bright burgundy wine medium depth tobacco wild...</td>\n",
       "      <td>commentary 2020 proyecto garnachas salvaje del...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Red Wine</td>\n",
       "      <td>Domaine du Terme Gigondas 2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>spectacular gigondas wine red cherry flavors s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Red Wine</td>\n",
       "      <td>Domaine du Terme Gigondas 2019</td>\n",
       "      <td>NaN</td>\n",
       "      <td>straight first sniff clear going special soari...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Red Wine</td>\n",
       "      <td>Scott Harvey Mountain Selection Zinfandel 2019</td>\n",
       "      <td>fruit forward rich full flavors expressing var...</td>\n",
       "      <td>fresh smelling full bodied flavor packed wine ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   category                                              id  \\\n",
       "0  Red Wine      Proyecto Salvaje del Moncayo Garnacha 2020   \n",
       "1  Red Wine      Proyecto Salvaje del Moncayo Garnacha 2020   \n",
       "2  Red Wine                  Domaine du Terme Gigondas 2019   \n",
       "3  Red Wine                  Domaine du Terme Gigondas 2019   \n",
       "4  Red Wine  Scott Harvey Mountain Selection Zinfandel 2019   \n",
       "\n",
       "                                                text  \\\n",
       "0  bright burgundy wine medium depth tobacco wild...   \n",
       "1  bright burgundy wine medium depth tobacco wild...   \n",
       "2                                                NaN   \n",
       "3                                                NaN   \n",
       "4  fruit forward rich full flavors expressing var...   \n",
       "\n",
       "                                             reviews  \n",
       "0  part proyecto garnachas de españa collection s...  \n",
       "1  commentary 2020 proyecto garnachas salvaje del...  \n",
       "2  spectacular gigondas wine red cherry flavors s...  \n",
       "3  straight first sniff clear going special soari...  \n",
       "4  fresh smelling full bodied flavor packed wine ...  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wine_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Review "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df['reviews'] = wine_df['reviews'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(20988, 5575)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_reviews_vectorizer = CountVectorizer(stop_words=stopwords, min_df=5, max_df=0.7)\n",
    "count_reviews_vectors = count_reviews_vectorizer.fit_transform(wine_df[\"reviews\"])\n",
    "count_reviews_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20988, 5575)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tfidf_reviews_vectorizer = TfidfVectorizer(stop_words=stopwords, min_df=5, max_df=0.7)\n",
    "tfidf_reviews_vectors = tfidf_reviews_vectorizer.fit_transform(wine_df[\"reviews\"])\n",
    "tfidf_reviews_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a Non-Negative Matrix Factorization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\decomposition\\_nmf.py:289: FutureWarning: The 'init' value, when 'init=None' and n_components is less than n_samples and n_features, will be changed from 'nndsvd' to 'nndsvda' in 1.1 (renaming of 0.26).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "nmf_reviews_model = NMF(n_components=5, random_state=314)\n",
    "W_reviews_matrix = nmf_reviews_model.fit_transform(tfidf_reviews_vectors)\n",
    "H_reviews_matrix = nmf_reviews_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Topic 00\n",
      "  medium (3.35)\n",
      "  tannins (2.76)\n",
      "  bodied (2.66)\n",
      "  body (1.92)\n",
      "  drink (1.67)\n",
      "\n",
      "Topic 01\n",
      "  commentary (2.97)\n",
      "  tasting (2.93)\n",
      "  san (2.92)\n",
      "  francisco (2.90)\n",
      "  tasted (2.86)\n",
      "\n",
      "Topic 02\n",
      "  cabernet (2.03)\n",
      "  sauvignon (1.27)\n",
      "  merlot (0.90)\n",
      "  wine (0.84)\n",
      "  franc (0.83)\n",
      "\n",
      "Topic 03\n",
      "  white (1.33)\n",
      "  lemon (1.20)\n",
      "  acidity (1.03)\n",
      "  fresh (1.02)\n",
      "  apple (0.94)\n",
      "\n",
      "Topic 04\n",
      "  cherry (1.90)\n",
      "  red (1.55)\n",
      "  black (1.50)\n",
      "  flavors (1.35)\n",
      "  fruit (1.12)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "display_topics(nmf_reviews_model, tfidf_reviews_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting an LSA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the SVD model\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "svd_reviews_model = TruncatedSVD(n_components = 10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the matrix \n",
    "W_svd_reviews_matrix = svd_reviews_model.fit_transform(tfidf_reviews_vectors)\n",
    "H_svd_reviews_matrix = svd_reviews_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the matrix of the new topic \n",
    "wine_df[\"svd_topic_reviews\"] = np.argmax(W_svd_reviews_matrix, axis = 1).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 0 observes original label counts of:\n",
      "Red Wine                   11040\n",
      "White Wine                  4696\n",
      "Champagne & Sparkling       1481\n",
      "Dessert, Sherry, & Port      765\n",
      "Rosé Wine                    419\n",
      "Name: category, dtype: int64\n",
      "\n",
      "topic 1 observes original label counts of:\n",
      "Red Wine                   482\n",
      "White Wine                 242\n",
      "Champagne & Sparkling      172\n",
      "Rosé Wine                   55\n",
      "Dessert, Sherry, & Port     25\n",
      "Name: category, dtype: int64\n",
      "\n",
      "topic 4 observes original label counts of:\n",
      "Red Wine                   248\n",
      "Dessert, Sherry, & Port     12\n",
      "Champagne & Sparkling        4\n",
      "White Wine                   3\n",
      "Rosé Wine                    2\n",
      "Name: category, dtype: int64\n",
      "\n",
      "topic 2 observes original label counts of:\n",
      "Red Wine                   392\n",
      "Dessert, Sherry, & Port      4\n",
      "White Wine                   1\n",
      "Name: category, dtype: int64\n",
      "\n",
      "topic 8 observes original label counts of:\n",
      "Red Wine                 23\n",
      "Champagne & Sparkling     3\n",
      "Rosé Wine                 3\n",
      "Name: category, dtype: int64\n",
      "\n",
      "topic 6 observes original label counts of:\n",
      "Red Wine                   192\n",
      "White Wine                 156\n",
      "Dessert, Sherry, & Port      5\n",
      "Name: category, dtype: int64\n",
      "\n",
      "topic 5 observes original label counts of:\n",
      "Red Wine                   149\n",
      "White Wine                  11\n",
      "Champagne & Sparkling        4\n",
      "Dessert, Sherry, & Port      1\n",
      "Name: category, dtype: int64\n",
      "\n",
      "topic 7 observes original label counts of:\n",
      "Red Wine                   44\n",
      "White Wine                 13\n",
      "Champagne & Sparkling       7\n",
      "Dessert, Sherry, & Port     5\n",
      "Name: category, dtype: int64\n",
      "\n",
      "topic 9 observes original label counts of:\n",
      "Champagne & Sparkling    290\n",
      "Red Wine                  25\n",
      "White Wine                 6\n",
      "Name: category, dtype: int64\n",
      "\n",
      "topic 3 observes original label counts of:\n",
      "Red Wine    8\n",
      "Name: category, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#for loop that count the unique topic \n",
    "for label in wine_df['svd_topic_reviews'].unique():\n",
    "    print(f\"topic {label} observes original label counts of:\")\n",
    "    print(wine_df[wine_df['svd_topic_reviews'] == label]['category'].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting an LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit your LDA model here\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "lda_reviews_model = LatentDirichletAllocation(n_components = 10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the matrix \n",
    "W_lda_reviews_matrix = lda_reviews_model.fit_transform(count_reviews_vectors)\n",
    "H_lda_reviews_matrix = lda_reviews_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the matrix of the new topic \n",
    "wine_df[\"lda_topic_reviews\"] = np.argmax(W_lda_reviews_matrix, axis = 1).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "topic 6 observes original label counts of:\n",
      "Red Wine                   1303\n",
      "White Wine                  320\n",
      "Dessert, Sherry, & Port     176\n",
      "Champagne & Sparkling        70\n",
      "Rosé Wine                    16\n",
      "Name: category, dtype: int64\n",
      "\n",
      "topic 2 observes original label counts of:\n",
      "Red Wine                   485\n",
      "White Wine                 244\n",
      "Champagne & Sparkling      174\n",
      "Rosé Wine                   55\n",
      "Dessert, Sherry, & Port     28\n",
      "Name: category, dtype: int64\n",
      "\n",
      "topic 0 observes original label counts of:\n",
      "Red Wine                   5166\n",
      "White Wine                 2299\n",
      "Champagne & Sparkling       420\n",
      "Rosé Wine                   258\n",
      "Dessert, Sherry, & Port     205\n",
      "Name: category, dtype: int64\n",
      "\n",
      "topic 3 observes original label counts of:\n",
      "Red Wine                   2272\n",
      "Dessert, Sherry, & Port      63\n",
      "Champagne & Sparkling        39\n",
      "White Wine                   26\n",
      "Rosé Wine                    26\n",
      "Name: category, dtype: int64\n",
      "\n",
      "topic 8 observes original label counts of:\n",
      "Red Wine                   822\n",
      "Champagne & Sparkling       62\n",
      "Dessert, Sherry, & Port     34\n",
      "Rosé Wine                   10\n",
      "White Wine                   9\n",
      "Name: category, dtype: int64\n",
      "\n",
      "topic 9 observes original label counts of:\n",
      "Red Wine                   1912\n",
      "White Wine                  158\n",
      "Dessert, Sherry, & Port      15\n",
      "Champagne & Sparkling         4\n",
      "Rosé Wine                     3\n",
      "Name: category, dtype: int64\n",
      "\n",
      "topic 5 observes original label counts of:\n",
      "White Wine                 1496\n",
      "Champagne & Sparkling       483\n",
      "Dessert, Sherry, & Port     224\n",
      "Rosé Wine                    48\n",
      "Red Wine                     34\n",
      "Name: category, dtype: int64\n",
      "\n",
      "topic 1 observes original label counts of:\n",
      "Red Wine                   518\n",
      "White Wine                 377\n",
      "Champagne & Sparkling       34\n",
      "Rosé Wine                   17\n",
      "Dessert, Sherry, & Port     14\n",
      "Name: category, dtype: int64\n",
      "\n",
      "topic 4 observes original label counts of:\n",
      "Champagne & Sparkling      671\n",
      "White Wine                  69\n",
      "Dessert, Sherry, & Port     53\n",
      "Rosé Wine                   44\n",
      "Red Wine                    25\n",
      "Name: category, dtype: int64\n",
      "\n",
      "topic 7 observes original label counts of:\n",
      "White Wine                 130\n",
      "Red Wine                    66\n",
      "Dessert, Sherry, & Port      5\n",
      "Champagne & Sparkling        4\n",
      "Rosé Wine                    2\n",
      "Name: category, dtype: int64\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#for loop that count the unique topic \n",
    "for label in wine_df['lda_topic_reviews'].unique():\n",
    "    print(f\"topic {label} observes original label counts of:\")\n",
    "    print(wine_df[wine_df['lda_topic_reviews'] == label]['category'].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\utils\\deprecation.py:87: FutureWarning: Function get_feature_names is deprecated; get_feature_names is deprecated in 1.0 and will be removed in 1.2. Please use get_feature_names_out instead.\n",
      "  warnings.warn(msg, category=FutureWarning)\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\pyLDAvis\\_prepare.py:243: FutureWarning: In a future version of pandas all arguments of DataFrame.drop except for the argument 'labels' will be keyword-only.\n",
      "  default_term_info = default_term_info.sort_values(\n"
     ]
    }
   ],
   "source": [
    "lda_display_review = pyLDAvis.sklearn.prepare(lda_reviews_model, count_reviews_vectors, count_reviews_vectorizer, sort_topics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<link rel=\"stylesheet\" type=\"text/css\" href=\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v1.0.0.css\">\n",
       "\n",
       "\n",
       "<div id=\"ldavis_el190001870328165424406599288\" style=\"background-color:white;\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "\n",
       "var ldavis_el190001870328165424406599288_data = {\"mdsDat\": {\"x\": [-0.11744755924129195, -0.054765639606807144, 0.08736547990348888, -0.14604110212888738, 0.07024698332776128, -0.0967535475506907, 0.2114283270788195, 0.15513833489589723, -0.186759625393005, 0.07758834871471537], \"y\": [-0.04353773849976042, -0.08647402245242085, 0.30241187798849894, -0.03821774450922412, 0.044215593779202686, 0.055375260950132184, -0.11984636030126357, -0.03231856537509464, 0.011025499815326656, -0.09263380139539638], \"topics\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10], \"cluster\": [1, 1, 1, 1, 1, 1, 1, 1, 1, 1], \"Freq\": [11.663079163483872, 7.674997753051171, 6.980940652732741, 12.242562146696905, 6.588434030176028, 11.321923547089328, 15.102485874501312, 3.3113488973026697, 7.176094464695957, 17.938133470270017]}, \"tinfo\": {\"Term\": [\"tannins\", \"cherry\", \"wine\", \"flavors\", \"tasting\", \"tasted\", \"san\", \"commentary\", \"francisco\", \"notes\", \"bodied\", \"aromas\", \"medium\", \"pinot\", \"black\", \"sample\", \"cabernet\", \"lemon\", \"barrel\", \"white\", \"palate\", \"noir\", \"finish\", \"2022\", \"red\", \"plum\", \"nose\", \"drink\", \"elegant\", \"dark\", \"alder\", \"2035\", \"sanguine\", \"puree\", \"balsam\", \"ganache\", \"smoldering\", \"tug\", \"loganberry\", \"woodsy\", \"avola\", \"2042\", \"p\\u00e2te\", \"lined\", \"singed\", \"relax\", \"canaiolo\", \"chai\", \"extends\", \"2045\", \"firming\", \"winey\", \"potting\", \"lining\", \"inlaid\", \"tile\", \"a\\u00e7a\\u00ed\", \"embedded\", \"latent\", \"slavonian\", \"2038\", \"2040\", \"cast\", \"burly\", \"root\", \"vigna\", \"plum\", \"boysenberry\", \"brunello\", \"riserva\", \"montalcino\", \"currant\", \"tarry\", \"blackberry\", \"iron\", \"olive\", \"beam\", \"di\", \"steeped\", \"black\", \"cherry\", \"fill\", \"dense\", \"best\", \"mulberry\", \"tobacco\", \"dark\", \"tar\", \"accents\", \"elements\", \"flavors\", \"structure\", \"licorice\", \"tea\", \"spice\", \"fruit\", \"notes\", \"red\", \"cabernet\", \"finish\", \"cassis\", \"merlot\", \"tannins\", \"core\", \"wine\", \"shows\", \"rich\", \"savory\", \"sauvignon\", \"drink\", \"earth\", \"ripe\", \"long\", \"finale\", \"possess\", \"certification\", \"pastille\", \"suffused\", \"inflected\", \"appealingly\", \"exude\", \"overtly\", \"wisps\", \"repeating\", \"slew\", \"shimmering\", \"delineation\", \"trimmed\", \"imposing\", \"spicecake\", \"brim\", \"tautly\", \"florality\", \"culminate\", \"deepen\", \"spicier\", \"valmur\", \"treatment\", \"particular\", \"smacking\", \"demeter\", \"sufficiently\", \"f\\u00e8vre\", \"evokes\", \"lip\", \"biodynamically\", \"markedly\", \"frames\", \"youthfully\", \"discreet\", \"middle\", \"sneaky\", \"89\", \"attractively\", \"brimming\", \"austere\", \"91\", \"weight\", \"sample\", \"chablis\", \"barrel\", \"impressively\", \"excellent\", \"floral\", \"92\", \"patience\", \"mineral\", \"good\", \"93\", \"sappy\", \"long\", \"90\", \"nose\", \"finish\", \"flavors\", \"depth\", \"fruit\", \"succulent\", \"dry\", \"minerality\", \"palate\", \"touch\", \"wood\", \"aromas\", \"fine\", \"ripe\", \"red\", \"spice\", \"acidity\", \"lovely\", \"medium\", \"fresh\", \"notes\", \"cherry\", \"san\", \"commentary\", \"francisco\", \"pair\", \"tasting\", \"lamb\", \"pan\", \"chops\", \"fried\", \"stew\", \"ribeye\", \"oven\", \"pork\", \"braised\", \"ribs\", \"garlic\", \"active\", \"steamed\", \"sashimi\", \"clams\", \"marbled\", \"avocado\", \"crab\", \"serve\", \"pasta\", \"seared\", \"leg\", \"appetizers\", \"fils\", \"exhibits\", \"december\", \"excels\", \"chicken\", \"tasted\", \"enjoy\", \"shines\", \"oysters\", \"november\", \"august\", \"2022\", \"june\", \"grilled\", \"27\", \"roast\", \"february\", \"pleasing\", \"notes\", \"2021\", \"wine\", \"flavors\", \"aromas\", \"offers\", \"spices\", \"2020\", \"dried\", \"earth\", \"savory\", \"fruits\", \"palate\", \"fruit\", \"black\", \"red\", \"2019\", \"ripe\", \"bright\", \"bark\", \"cent\", \"onwards\", \"chocolatey\", \"fontalloro\", \"brawny\", \"relaxed\", \"brambleberries\", \"carm\\u00e9n\\u00e8re\", \"cacao\", \"tannined\", \"peppermint\", \"needle\", \"pomegranates\", \"needles\", \"stags\", \"cheval\", \"fudge\", \"cab\", \"charges\", \"broody\", \"brisson\", \"pineapples\", \"cloves\", \"beefy\", \"moss\", \"sensibility\", \"sarsaparilla\", \"bits\", \"laurel\", \"chewy\", \"fantastically\", \"walnuts\", \"olives\", \"soften\", \"flavorful\", \"drinkable\", \"undertones\", \"body\", \"blackberries\", \"chili\", \"plums\", \"follow\", \"walnut\", \"cherries\", \"try\", \"blackcurrants\", \"blueberries\", \"hold\", \"strawberries\", \"mushrooms\", \"tight\", \"chocolate\", \"tannins\", \"firm\", \"medium\", \"box\", \"bodied\", \"polished\", \"character\", \"juicy\", \"fine\", \"drink\", \"dark\", \"berries\", \"nose\", \"finish\", \"fresh\", \"lots\", \"aromas\", \"red\", \"fruit\", \"dried\", \"palate\", \"ripe\", \"black\", \"long\", \"hints\", \"creamy\", \"disgorged\", \"meunier\", \"dosage\", \"clairette\", \"solera\", \"pillowy\", \"nv\", \"quinta\", \"bourboulenc\", \"pinpoint\", \"laurent\", \"perrier\", \"current\", \"oger\", \"sel\", \"copper\", \"2008\", \"champagnes\", \"magnum\", \"drier\", \"disgorgement\", \"nonvintage\", \"rotie\", \"roederer\", \"7g\", \"mesnil\", \"cramant\", \"avize\", \"montagne\", \"a\\u00ff\", \"blancs\", \"noir\", \"champagne\", \"pinot\", \"vinous\", \"brut\", \"ros\\u00e9\", \"mousse\", \"noirs\", \"lacy\", \"non\", \"pink\", \"latest\", \"reserve\", \"chardonnay\", \"bread\", \"cuv\\u00e9e\", \"blanc\", \"pale\", \"acids\", \"grand\", \"orchard\", \"de\", \"release\", \"crisp\", \"blend\", \"based\", \"citrus\", \"flowers\", \"color\", \"fruit\", \"palate\", \"wine\", \"white\", \"finish\", \"fresh\", \"drink\", \"aromas\", \"bodied\", \"vintage\", \"medium\", \"fine\", \"notes\", \"ripe\", \"red\", \"pineapple\", \"lemons\", \"curd\", \"pears\", \"honeysuckle\", \"mango\", \"crust\", \"apricots\", \"semillon\", \"papaya\", \"s\\u00e9millon\", \"oily\", \"struck\", \"elderflower\", \"kiwi\", \"meringue\", \"mangoes\", \"viscous\", \"guava\", \"lanolin\", \"zingy\", \"lemongrass\", \"caramelized\", \"yuzu\", \"perlage\", \"cerequio\", \"flint\", \"br\\u00fbl\\u00e9e\", \"butterscotch\", \"limey\", \"lime\", \"beeswax\", \"honey\", \"lemon\", \"butter\", \"melon\", \"ginger\", \"tropical\", \"honeyed\", \"meyer\", \"peach\", \"apricot\", \"pear\", \"jasmine\", \"apple\", \"chamomile\", \"grapefruit\", \"white\", \"sliced\", \"blossom\", \"almonds\", \"green\", \"peaches\", \"almond\", \"salted\", \"apples\", \"yellow\", \"creamy\", \"zest\", \"acidity\", \"citrus\", \"chardonnay\", \"crisp\", \"fresh\", \"finish\", \"drink\", \"palate\", \"flavors\", \"bright\", \"nose\", \"aromas\", \"notes\", \"medium\", \"bodied\", \"texture\", \"fruit\", \"stone\", \"blanc\", \"mineral\", \"dried\", \"ripe\", \"long\", \"touriga\", \"port\", \"yeasts\", \"nacional\", \"planted\", \"field\", \"indigenous\", \"casks\", \"plot\", \"facing\", \"meters\", \"hectares\", \"certified\", \"altitude\", \"franca\", \"lower\", \"season\", \"acres\", \"bottlings\", \"american\", \"parcels\", \"oldest\", \"uco\", \"achieved\", \"douro\", \"residual\", \"roriz\", \"biodynamic\", \"helped\", \"auslese\", \"vines\", \"fermented\", \"500\", \"ages\", \"matured\", \"000\", \"ha\", \"growing\", \"produced\", \"different\", \"vats\", \"fermentation\", \"soils\", \"stainless\", \"clay\", \"months\", \"old\", \"organic\", \"steel\", \"barrels\", \"year\", \"vineyard\", \"aged\", \"comes\", \"single\", \"sugar\", \"ripeness\", \"wine\", \"vintage\", \"vineyards\", \"wines\", \"oak\", \"alcohol\", \"little\", \"bottled\", \"grapes\", \"new\", \"2017\", \"french\", \"fruit\", \"good\", \"blend\", \"2018\", \"age\", \"freshness\", \"bottle\", \"years\", \"2019\", \"time\", \"like\", \"tannins\", \"acidity\", \"stated\", \"mazuelo\", \"stimulating\", \"veltliner\", \"piquant\", \"sole\", \"kabinett\", \"gg\", \"sancerre\", \"petrale\", \"tondonia\", \"gr\\u00fcner\", \"creek\", \"tensioned\", \"certain\", \"vi\\u00f1a\", \"coolish\", \"sous\", \"vinification\", \"aglianico\", \"handrolls\", \"tender\", \"enormously\", \"ried\", \"trocken\", \"monopole\", \"priorat\", \"vital\", \"panko\", \"vergelesses\", \"extreme\", \"ripened\", \"riesling\", \"graciano\", \"bois\", \"clear\", \"crystalline\", \"natural\", \"highly\", \"reserva\", \"intertwined\", \"tempranillo\", \"elegant\", \"flinty\", \"intense\", \"lush\", \"pure\", \"terroir\", \"bouquet\", \"alcohol\", \"palate\", \"salty\", \"fruit\", \"fine\", \"complex\", \"refined\", \"fresh\", \"2021\", \"wine\", \"nose\", \"ripe\", \"2020\", \"round\", \"concentrated\", \"notes\", \"deep\", \"finish\", \"long\", \"mineral\", \"aromas\", \"great\", \"2019\", \"2018\", \"cranberry\", \"skinned\", \"imported\", \"earl\", \"hibiscus\", \"marasca\", \"forefront\", \"woodland\", \"nebbiolo\", \"cannubi\", \"highlighted\", \"spirits\", \"dusky\", \"scrub\", \"morra\", \"monforte\", \"nerello\", \"recalling\", \"botanical\", \"grimaldi\", \"scavino\", \"berardenga\", \"suede\", \"heavier\", \"sagebrush\", \"beach\", \"align\", \"gather\", \"muddled\", \"wears\", \"pomegranate\", \"petal\", \"serralunga\", \"blood\", \"forest\", \"floor\", \"grey\", \"alba\", \"hip\", \"barolo\", \"alongside\", \"rose\", \"pipe\", \"star\", \"orange\", \"strawberry\", \"cherry\", \"underbrush\", \"raspberry\", \"wild\", \"berry\", \"peel\", \"editors\", \"camphor\", \"grained\", \"anise\", \"flower\", \"red\", \"spice\", \"dried\", \"tannins\", \"aromas\", \"licorice\", \"palate\", \"flavors\", \"drink\", \"acidity\", \"bright\", \"finish\", \"fresh\", \"juicy\", \"savory\", \"fine\", \"offers\", \"black\", \"notes\", \"dark\", \"ch\\u00e2teauneuf\", \"pape\", \"checks\", \"emilion\", \"mingled\", \"canon\", \"ducru\", \"cos\", \"gallonibarrel\", \"muids\", \"lynch\", \"seeking\", \"peonies\", \"pavie\", \"chest\", \"embers\", \"flawless\", \"estournel\", \"burning\", \"crozes\", \"garni\", \"beaucaillou\", \"bages\", \"gravelly\", \"l\\u00e9oville\", \"unquestionably\", \"bilberry\", \"du\", \"pomerol\", \"define\", \"saint\", \"haut\", \"98\", \"longevity\", \"hermitage\", \"ch\\u00e2teau\", \"evolve\", \"97\", \"brought\", \"pencil\", \"purple\", \"96\", \"sexy\", \"notions\", \"shavings\", \"coming\", \"decades\", \"garnet\", \"cabernet\", \"95\", \"franc\", \"sauvignon\", \"petit\", \"incredible\", \"sample\", \"going\", \"merlot\", \"seamless\", \"verdot\", \"rating\", \"barrel\", \"years\", \"great\", \"2019\", \"deep\", \"bodied\", \"fruits\", \"medium\", \"new\", \"cassis\", \"2018\", \"black\", \"blend\", \"glass\", \"tannins\", \"2020\", \"de\", \"palate\", \"ripe\", \"oak\", \"wine\", \"notes\", \"finish\", \"fruit\"], \"Freq\": [5035.0, 2471.0, 6263.0, 4285.0, 1050.0, 1317.0, 1001.0, 947.0, 932.0, 4281.0, 4426.0, 4180.0, 3914.0, 1050.0, 3553.0, 1322.0, 2549.0, 1215.0, 1451.0, 1695.0, 4744.0, 700.0, 5708.0, 1184.0, 3595.0, 1368.0, 2475.0, 2935.0, 1344.0, 1848.0, 67.26799898744214, 84.65820813487228, 53.31505822430597, 45.53676570308497, 25.78104645784664, 24.793340120804384, 24.793338522665046, 23.80555055344055, 21.83000461717345, 21.829861962517082, 21.828920571547524, 22.755439725509447, 20.84220503478555, 18.866619703200104, 106.3435243918395, 16.891019542807488, 16.88394304773489, 15.90323534286197, 15.900644475493323, 36.437761212734095, 14.915475041575924, 14.915466162245789, 14.915436884036158, 14.863865240540415, 13.927735408388587, 13.927475918502914, 12.939952780248205, 12.939912307354398, 12.939911931520376, 12.939835033273786, 47.64439412160234, 75.93259180303922, 40.99823390724482, 23.7523628282698, 52.188193286565095, 22.73752119200511, 962.0452976471154, 119.70660964388738, 150.21068658778404, 116.43964461943759, 111.40807241384144, 430.26097148027077, 47.81371107707323, 695.1896113530715, 275.3407320649131, 113.7538943148781, 47.63736422881194, 162.81746974056904, 56.5441241833805, 1369.2805068016607, 1016.0563425958301, 59.2379503621826, 310.3059301842481, 651.0413169073904, 57.67395976639181, 420.0853835808398, 673.1860489877781, 154.3202815519942, 219.4599987216207, 119.09153743732551, 1043.2163349513023, 327.7641953196133, 349.20030217603534, 204.41886613147227, 507.81724988238, 1078.9771565452384, 826.9873448016856, 738.3810653362658, 587.6874120135825, 948.049068257097, 294.37514340148755, 321.57532503052295, 734.3447154231893, 261.0625369872519, 715.5019446129504, 305.4516371648681, 348.5788377499485, 301.3579124470064, 334.83382604752006, 362.713619705398, 260.81700617560597, 285.581828946808, 261.3055973631688, 157.0593343523476, 113.89500388192084, 49.1484362326456, 37.376357098456594, 31.490345938066074, 41.007424338227246, 30.509294517720104, 25.60429896384961, 24.623241590456253, 22.661265865065143, 22.661248366430655, 22.625612862911517, 19.718155521366235, 22.513450776951835, 16.7752263368904, 16.747738699440937, 14.813207776638704, 14.813195570516022, 14.813174866192982, 14.81317407532949, 14.813123466426307, 14.78016466587585, 13.83220208112064, 13.832182137136934, 13.832087970773419, 13.801834690870145, 18.399358861460875, 29.390570875057104, 12.851189026370234, 12.851121562889897, 28.452526856895574, 17.355052594686676, 62.27384498686991, 22.66127524926219, 25.919708282136803, 99.10202966150685, 56.16525658645559, 74.23112007830619, 31.128192646579492, 93.53726176323207, 42.5155897329777, 27.849171233487617, 146.92574421813708, 223.90418403100105, 222.77408791114757, 536.9221917648956, 52.80658871302725, 551.5701870628188, 67.30390369791617, 225.2501937348888, 353.7484768489405, 201.707925599021, 54.57013671316253, 346.34930424867434, 326.672286115226, 185.30672591173715, 81.02343567499673, 464.160198507974, 135.09036279510732, 471.0626820777295, 744.888805251084, 592.1515922179088, 189.9060048476137, 580.7069349961317, 121.89193593339415, 205.2732493417083, 149.7181788800447, 419.07223723839655, 162.29016277354194, 137.81826351433017, 362.6320422778102, 246.15949862692366, 256.8245987976467, 263.9246626771329, 201.84815764313834, 201.01442034927402, 151.3808830364133, 177.95859809937608, 171.15981040422426, 174.28783284304765, 163.4913071512138, 1000.8677105353129, 947.0951137564008, 931.1624853335971, 325.72257108045596, 1047.2768278891645, 129.55211110121212, 86.73268258175767, 60.842716307916135, 82.39494593630532, 56.85955940710664, 47.89745790452105, 47.897454065977456, 44.90987020391389, 43.91425841224253, 43.91338713093705, 40.92691923763182, 40.92667375511655, 39.93113781300164, 37.9395433939546, 36.94374162469054, 32.96061986200342, 29.97325137726899, 29.973211976850514, 148.63672117234023, 27.975003231729744, 24.99425348911527, 23.002728770809817, 23.002695648355985, 22.971896753200802, 279.28136733454784, 147.87605120754165, 113.26070402493667, 97.92113630139978, 1048.3054597245896, 513.3482500296618, 180.43415053250408, 43.236874545008035, 111.51185550506911, 114.27809475842902, 696.9648452045982, 106.01228408936691, 321.6115157110796, 79.43582226017566, 82.49697421719038, 91.63711795981706, 112.5710377273276, 1271.5235558893744, 419.1400329909367, 1351.6487526340823, 1001.2215907974978, 968.9376675357814, 307.9597905970749, 247.80500008705434, 269.6377720338156, 295.1031659013494, 227.8806910646776, 229.52930028036167, 252.33432531735144, 355.3764859873683, 354.89780516832326, 304.8841618164986, 298.52319907985225, 228.87135642090334, 231.0790515347938, 181.77092154754493, 167.7542851678509, 36.718165533586394, 22.262190490939368, 17.3978025307242, 13.588543939636377, 13.588199219536738, 12.624096922024489, 11.661108838804179, 11.661034854260008, 11.660996523579685, 9.73368188150387, 8.769896594888063, 8.769888505611137, 8.769856214245191, 18.337489688171345, 7.806031567913775, 7.8059171084329515, 7.805838882402582, 25.102740446466353, 6.842367586950832, 6.842274467105897, 6.84226998986008, 6.842256395431625, 115.48650311734374, 16.06521672242185, 13.514826786395238, 5.8787188628533045, 5.878679331198653, 5.878430459398693, 5.878391121690065, 395.69462552962835, 12.521675880773353, 100.40854086741226, 104.19538474226431, 115.80316692987404, 306.9952388058856, 121.3891320313024, 163.8071577389755, 632.5143544402636, 383.3066354624084, 30.89382089149028, 423.25423021340333, 131.2118048129295, 121.3811546800509, 628.6503405286377, 229.82295540602578, 167.4576621228562, 191.07872659926483, 477.3751836757742, 184.5534582820774, 76.7755767911042, 316.21496250423877, 594.1759101052289, 2196.2412807442884, 654.4947956155102, 1604.1438038936612, 98.8116691740492, 1702.5096512736138, 337.7070902595558, 463.55170878240904, 596.4971878611002, 774.8353799065799, 948.8267900550663, 667.2398818536261, 347.0202127881643, 771.7260406965121, 1299.002543963854, 809.1471667099104, 317.3825057158575, 954.8395193130699, 846.3804244034532, 1104.8400185678265, 548.5020879149807, 926.6458754925031, 643.2828919082491, 621.6970268702406, 463.62964414986425, 394.0206487323226, 360.93000696339203, 145.33218785776276, 196.29787972182476, 150.13496507275843, 61.372653660402584, 43.58349140422464, 43.570411704077735, 291.29747267983157, 37.63671609265672, 36.6654582134771, 34.688863228011314, 26.78258296572233, 24.80601745442425, 23.81740560661281, 21.84116485869992, 21.841141806917182, 21.83474549836486, 64.24731607562396, 18.876295440896968, 17.887905142448805, 17.887883426542, 16.89971168831103, 16.899706559864164, 16.89965075587117, 17.829706351180754, 16.870151367463098, 15.911432597001403, 15.911431750012035, 15.911427941259669, 14.923104302864616, 13.934852865338492, 83.22470359154956, 619.1627647694789, 390.5729313669491, 884.3071626172955, 81.32831194871453, 326.07624244594047, 284.2032416853968, 211.5020036000241, 35.93353498907653, 47.390465950831064, 33.77663856194269, 132.26378533047782, 56.40863894253213, 134.4738656341744, 351.5785516370742, 99.89366329033582, 129.78082731773497, 209.09403109067588, 107.447373632831, 135.27982638383378, 125.05762098938989, 95.17742390585012, 231.9649834817188, 101.80906253095084, 175.95450209651756, 261.5426228690189, 94.14723829371468, 177.86126458483784, 174.3331551182004, 132.06102469449007, 298.3915004443543, 275.1357025321676, 291.97298299355646, 190.04499417300602, 261.7038695109584, 205.45891732792438, 197.36172772313373, 199.65223476760286, 195.0600286203444, 160.8282874368911, 179.8310944899134, 168.0495799884446, 176.15880580856853, 166.84951128082236, 161.20224494666468, 246.5891492757064, 158.53670102789093, 135.71439452394065, 112.00367916187716, 176.65352539162794, 93.18785590373727, 69.42046053084397, 70.3896024705813, 40.70151208979919, 34.75973294656411, 25.846978971568394, 26.79074299578097, 24.856610667701627, 23.866352611658396, 23.866347038683674, 21.885750003407722, 21.88573673748873, 20.895428776127048, 46.47143373441356, 19.905134612097278, 18.9147495844404, 42.50575288418625, 41.203120966047194, 14.953625089464198, 14.953563173378486, 14.953333909156083, 104.61540177956977, 13.963291928547099, 48.21085945704609, 12.972947213669572, 349.9264707350954, 60.102508692694705, 219.35244219624659, 1084.0245705894126, 75.74977178479969, 216.57149458092908, 227.99425405280127, 121.87721270304864, 115.55317473051726, 54.979561795166596, 556.4317692132952, 272.9610780695276, 489.9426537617622, 98.58951455181982, 764.557603182227, 73.374756971347, 278.1484680766623, 1169.121416802074, 147.77497548033298, 212.11982256387287, 105.59348427685688, 451.40562082255394, 137.52305501711243, 215.01210978568994, 116.78971098923522, 237.60281439260467, 218.55497790928618, 473.0085328348305, 219.46217260015547, 940.093501897394, 437.22042872359634, 411.5230451608051, 344.8782907136283, 777.5554751961361, 1169.1363859451822, 692.8322334128195, 881.8847385757161, 740.6984956037971, 443.9012433429371, 525.837227431916, 648.1707895644963, 618.9482155156056, 580.0577917329281, 576.1148503753575, 370.8610408356019, 565.4853727729428, 297.1804745116596, 260.2162937047755, 296.0135007366202, 296.9484717100113, 306.1921086329104, 298.0736657305879, 112.83820960838469, 98.99306438912002, 94.0484002374025, 64.38010937673693, 205.82114974099062, 62.40219571046045, 69.22485574967922, 62.36246893411243, 59.4353176359817, 73.04498188513028, 51.52385291443033, 51.52384938493595, 50.534868770129656, 47.568057539671344, 42.623387968288995, 58.09723103961907, 55.189958787109276, 38.66759457021166, 37.67799709241606, 70.2095084264414, 77.87231901505844, 30.75603849278587, 34.550960932888785, 30.710149563472886, 27.78923469665157, 88.17264026792087, 26.80030855636959, 32.51179250406127, 24.82239708275209, 24.821765665442406, 434.5798493971222, 336.70329574056757, 90.47224367174117, 59.97402140126787, 257.7005600984993, 189.0829011446331, 45.43468068595847, 62.42480120756166, 283.29438807104185, 139.39291138569658, 89.305222157722, 81.30578503246328, 246.43838300550397, 167.9328357093018, 110.34686147145929, 497.11617664256113, 417.8368042607635, 108.48579044899081, 174.10523782371916, 316.0403879874797, 554.2149927220576, 457.93379611938894, 398.4681271673601, 389.2128373812414, 125.1480909691742, 137.09170997861608, 195.94667140945342, 2373.099241710391, 690.3379163254161, 211.77231517204848, 362.14603033810266, 875.6259289051036, 310.3039727950805, 309.99457575822674, 165.72208790197607, 325.28741435517617, 479.3928891051822, 344.97627101800157, 288.33379154587016, 1070.2859132465192, 412.5400283231924, 460.2146596201045, 414.3097184768973, 308.7658112040271, 363.75995502552206, 298.9281715620993, 370.8673269520185, 360.8655317112562, 309.4341753628149, 305.68208163497604, 360.95853432039814, 328.52752029884454, 48.85881172826044, 45.74656226951134, 41.59741732737875, 41.59740522445546, 34.336002139386935, 32.26122662591575, 30.18664231799315, 28.111956598220395, 24.999932449479715, 23.96250513780901, 21.88791050572014, 21.88789670237222, 21.88778621182574, 20.850553589078654, 20.84963715489534, 30.770162770180743, 19.813239457803704, 26.720636880371163, 18.77577989007163, 17.738514960887446, 17.738235758474485, 17.73715465493746, 16.70118771400192, 15.663870441719489, 15.663858794368855, 16.57952474420363, 14.626480453959932, 14.626470491836994, 14.626418084380456, 13.58910767949255, 22.545317855146493, 32.019637327369786, 203.66656459657014, 49.69071959603982, 27.400643634472562, 142.57836848585515, 55.24800012117906, 61.24482882548804, 55.18982510584455, 59.51516917546874, 23.662202740588434, 63.171198932798035, 278.83400833642963, 61.30262479812635, 165.97330596262591, 82.51630926649668, 152.18623363394377, 51.39770699477183, 131.0648160080903, 107.12429371412443, 270.2561001958231, 67.56004751511782, 241.83650163351805, 156.43279997874816, 91.83152870896356, 82.67175045554056, 142.80261365501767, 101.11141543710151, 181.43958130602016, 132.1250190951467, 136.76872918289922, 100.45039653428424, 79.03770930948916, 87.79334711633635, 117.70770536362666, 85.48626164746372, 115.70556088739923, 98.00610218027103, 84.96414930642273, 98.78160850569526, 78.98871112413327, 75.04520135026598, 68.19916966235478, 111.92670588661505, 47.32936106164426, 48.21167869608169, 25.222208867717878, 25.221757923425756, 24.217373047289517, 18.188139590419762, 30.614060657251223, 54.344639516586874, 15.173279324126007, 21.69716965463241, 13.163785277094211, 13.163783986464134, 13.163747003643634, 14.975986417614527, 17.782759478448863, 12.158902115209536, 10.149186137901243, 10.149177882743365, 10.148561891592031, 9.144303866747068, 9.144287680626414, 9.144240218443025, 9.14422964404172, 9.144211803315438, 9.144211564847572, 8.139413184981489, 8.139389707004183, 8.139390037234904, 8.139334456493621, 111.91551372912508, 107.242310094101, 23.14558823722401, 140.3506611375435, 203.82982873611033, 135.78105636872306, 24.812049162214777, 41.77943994910513, 16.66442158600173, 128.15783106698137, 140.632165208177, 308.43950285738816, 25.23249880140677, 104.37512845315483, 487.4088608792572, 274.89897876719147, 1006.4380599751657, 77.96960940100513, 392.7611050675341, 312.2136054871148, 363.96577737589064, 237.99172926227254, 74.20946204050634, 62.48773520324999, 184.19659476525848, 148.69998131933468, 102.84799600040603, 703.5804948802647, 419.67890166175033, 369.365416442194, 688.2706401506327, 606.5421997754577, 246.84748183452044, 600.4374890036563, 567.4762844577411, 395.94647070539503, 331.0786244782685, 262.40376050156766, 425.6857728767615, 260.9770453876778, 212.8787975497712, 200.9325641924453, 232.7192989478498, 202.31359287998725, 228.45881870131504, 224.60865091680947, 192.89816671983812, 140.30525618114558, 223.43868906455617, 71.18937143374961, 58.35354296352807, 67.04644906537835, 51.441924024487165, 46.50512642097374, 46.505094257911395, 45.51773159968181, 48.36726182466837, 40.58079380796113, 47.185064429913886, 34.65657450311103, 33.66930151400643, 31.69456954145766, 31.69456353397932, 31.69451603019908, 30.707194600613732, 30.707185014899924, 30.70717687911754, 29.719823616091087, 28.732464439066977, 28.73239407883141, 28.73216219304096, 27.745078463229714, 26.757707933942573, 26.757642399381368, 309.47633798611884, 25.770307550075046, 63.91439217354935, 215.55741851872145, 96.74945944681521, 115.2975275731904, 51.43826155173533, 66.4777243895745, 305.0655015820719, 130.5540884908356, 140.59376843719633, 122.99759488841256, 230.0179394830813, 506.33668624562074, 181.59379919001833, 69.52348344839147, 109.32262546086632, 100.22439281695362, 292.1086767431221, 146.53727964352652, 201.48415494919365, 1606.5843809854568, 261.9285101458711, 552.7034952771788, 1001.5316773356435, 387.08296741705635, 104.48674142990596, 774.26279490618, 247.8512170588529, 611.3667508498642, 253.58401439151007, 344.9788356207259, 269.5575926213964, 769.2802977309962, 774.2193258313445, 560.7709573234891, 741.9001138697396, 507.1127183823219, 1621.631827234908, 705.6273816559138, 1246.1155730256353, 507.3597385530595, 430.4017105221481, 524.6526110906888, 943.9314514304186, 540.3056256106381, 385.4520702476827, 908.2503784283949, 438.1843020676074, 377.9127286850393, 716.665223779894, 589.5427035758568, 508.4652311323527, 700.1214106487034, 579.3556828301964, 536.9422133559096, 398.51632445738665], \"Total\": [5035.0, 2471.0, 6263.0, 4285.0, 1050.0, 1317.0, 1001.0, 947.0, 932.0, 4281.0, 4426.0, 4180.0, 3914.0, 1050.0, 3553.0, 1322.0, 2549.0, 1215.0, 1451.0, 1695.0, 4744.0, 700.0, 5708.0, 1184.0, 3595.0, 1368.0, 2475.0, 2935.0, 1344.0, 1848.0, 68.16179759554856, 85.94036737387208, 54.33497784951398, 46.43056832619907, 26.67490745357308, 25.68712224177567, 25.687122094106705, 24.6993390419006, 22.72377262381848, 22.72377347790587, 22.723779400156264, 23.71162977150975, 21.73598950621208, 19.76042350337686, 111.69376480920826, 17.784857029704288, 17.784809821709153, 16.797074569314844, 16.797076629783394, 38.52046090193976, 15.809290674912392, 15.809290150804712, 15.809291540770438, 15.808936946255022, 14.821507595799511, 14.821502627287604, 13.833724323145985, 13.833724466445881, 13.833724441274729, 13.833725261931965, 51.418461135961365, 83.02091446539157, 44.44183796776613, 25.686687447370375, 57.40742022218494, 24.707996139421457, 1368.0848331490913, 146.01706912318159, 187.32306615704644, 144.61607734599156, 139.31384904652725, 609.9969816718701, 55.652322301308374, 1055.9640861801054, 385.62676896323677, 149.3748649115872, 56.32609800268593, 241.1951888875502, 71.36191554482673, 3553.004789066787, 2471.1533218169957, 76.95839522295759, 599.4210157814388, 1591.7155417752085, 74.9166862599012, 959.339656968106, 1848.7738365744858, 273.9132711185499, 461.10476042069075, 200.2191209110585, 4285.735302790172, 855.3669207851449, 955.3955891754367, 456.04109822971026, 1825.0047677750888, 5877.794877218924, 4281.088020957158, 3595.53138047052, 2549.0809002318015, 5708.006327309899, 842.0931094538637, 1057.164601368426, 5035.0633264505595, 782.1868695525422, 6263.399304222148, 1116.312074348415, 1593.9879030533448, 1167.95644333932, 1689.349754687385, 2935.573529329324, 1061.320926279396, 2994.385846304359, 2273.558032701072, 157.95379566239754, 114.78945102313975, 50.042934535233584, 38.270841445422704, 32.384793780561175, 42.19677254058251, 31.403786620477995, 26.498746716136633, 25.51773913605217, 23.555723362098114, 23.555723886041235, 23.555988547507354, 20.61270141458045, 23.5566833757348, 17.669676378694064, 17.669866794422337, 15.7076608203595, 15.707661003310204, 15.707661250676667, 15.70766108606787, 15.707661663447889, 15.707890270306779, 14.72665316426914, 14.72665311049984, 14.72665413423147, 14.726851733821686, 19.634425525844886, 31.412834046501345, 13.74564510110056, 13.745646958875831, 30.479725014377717, 18.65392975630215, 69.7156217713332, 24.560593454797566, 29.505683468743605, 132.02114818008087, 70.2974303533688, 103.16596291034618, 37.32621072362369, 137.63100303048793, 54.85260145973156, 33.400105957949116, 250.49831485338566, 423.12958808073336, 422.4405189741378, 1322.57860575191, 73.77440527880535, 1451.032695311428, 105.18175362311656, 537.6383288502605, 1011.6242018662034, 470.40860549284554, 82.60609256434952, 1239.6804419620025, 1194.7384448125463, 517.6166403466493, 154.51172709742627, 2273.558032701072, 340.87939414061805, 2475.279451608682, 5708.006327309899, 4285.735302790172, 726.2473851522501, 5877.794877218924, 340.8554439543388, 896.7917066733421, 551.6616140592706, 4744.657685582957, 676.7705645623575, 485.9286346171458, 4180.473018673696, 2128.456899014544, 2994.385846304359, 3595.53138047052, 1825.0047677750888, 2491.3714996311905, 753.0613893484182, 3914.2045932125175, 2816.5117469437173, 4281.088020957158, 2471.1533218169957, 1001.7606989890394, 947.9880816340342, 932.0554542703625, 326.61561439834105, 1050.595544385877, 130.44514247321305, 87.62622593071437, 61.73568509631476, 83.6578664214667, 57.75252824693041, 48.79042539981818, 48.79042540893741, 45.80306394282009, 44.80726836339342, 44.807306302810595, 41.819901072922725, 41.81989934178281, 40.82411165081459, 38.83253313338875, 37.83674392671534, 33.85358724063572, 30.86621967842859, 30.866219371196443, 153.3220369532959, 28.874701335190657, 25.887273347785605, 23.895695165612413, 23.895694930452088, 23.896981704222227, 290.64653574425574, 154.31200429104902, 118.45896061066108, 102.53990332326292, 1317.6035415050892, 627.5663404160229, 230.79222888253054, 46.77874248487906, 137.63443691431232, 142.7969040012172, 1184.4678366266955, 131.6990850841514, 497.06874911255665, 96.43068285138352, 101.74384454058432, 117.48618740978333, 157.08647364530782, 4281.088020957158, 1029.0574577788423, 6263.399304222148, 4285.735302790172, 4180.473018673696, 1287.8284067994666, 876.2243848068401, 1139.8032363246261, 1792.4860249232445, 1061.320926279396, 1167.95644333932, 1540.4868668157762, 4744.657685582957, 5877.794877218924, 3553.004789066787, 3595.53138047052, 1469.5369273223478, 2994.385846304359, 1574.2639429941196, 171.67030622826383, 37.61435582915746, 23.158377419991783, 18.34166878616588, 14.484791992406311, 14.484805529412279, 13.521091238048657, 12.557327677871571, 12.557329334602823, 12.557335761390789, 10.629862639469417, 9.666132707866797, 9.666132551679292, 9.666133086449888, 20.295683414414626, 8.702403304078528, 8.702410106746056, 8.702406790691755, 28.04283597075643, 7.738669976583543, 7.738676486491909, 7.738672194290166, 7.738676151267972, 131.45724478167313, 18.37577441112541, 15.474417755054855, 6.77493577499675, 6.7749370232417965, 6.7749456548860705, 6.774944483712601, 456.3881785782499, 14.512700116579053, 119.00771064884995, 125.8554980729679, 140.36861839348302, 381.78264300650903, 148.1153063018942, 202.47000331901668, 817.4139048177047, 510.2125260083695, 36.812306482751694, 599.8252544832706, 173.67683542455958, 162.14087436249739, 957.4025777646783, 337.82774112514795, 238.8231102091877, 277.76354960442245, 786.5556326481048, 271.3526951495292, 102.92286616714753, 506.27769017402716, 1112.385349100781, 5035.0633264505595, 1249.491259250919, 3914.2045932125175, 144.7029277346918, 4426.90768704306, 678.3170961110745, 1026.3630196540362, 1455.1636469274836, 2128.456899014544, 2935.573529329324, 1848.7738365744858, 734.4635561624749, 2475.279451608682, 5708.006327309899, 2816.5117469437173, 668.4263623328532, 4180.473018673696, 3595.53138047052, 5877.794877218924, 1792.4860249232445, 4744.657685582957, 2994.385846304359, 3553.004789066787, 2273.558032701072, 1292.766213033974, 957.9269390599923, 146.27103251820094, 197.66289865793823, 151.2127580024907, 62.26642527843546, 44.47723380498902, 44.477260401847495, 297.4825673628303, 38.54755353573835, 37.559214653470555, 35.58263854654458, 27.676330941787967, 25.699753815231333, 24.711469415191154, 22.73488843041293, 22.73488861508279, 22.734937040047676, 67.20923526875929, 19.770023225880816, 18.78173579051955, 18.781735851955023, 17.793446614047156, 17.793446225238586, 17.79344650780885, 18.78185408528074, 17.793507050772927, 16.805157943558978, 16.805157949274488, 16.80515797828141, 15.816869177262602, 14.828581169750146, 88.98688435238618, 700.4918158054038, 447.954709489661, 1050.0730290340207, 89.97242618783606, 391.855216476805, 372.2571941926848, 276.90982417236796, 40.552137762969146, 56.28311196738909, 39.54730435087328, 200.60259741378226, 78.09375567665042, 235.34492375204363, 849.4386663701811, 183.995504507538, 269.80465400502874, 539.6475707479251, 209.35733391555166, 296.3308800910374, 277.19929365321156, 182.64616312833647, 892.3492029944962, 236.2261350899203, 665.1653911561459, 1478.5855188251512, 213.4043019308222, 872.2630062899794, 956.3390134472693, 521.5460304966709, 5877.794877218924, 4744.657685582957, 6263.399304222148, 1695.557551733843, 5708.006327309899, 2816.5117469437173, 2935.573529329324, 4180.473018673696, 4426.90768704306, 1397.7691856551728, 3914.2045932125175, 2128.456899014544, 4281.088020957158, 2994.385846304359, 3595.53138047052, 247.57807622176887, 159.441383009735, 136.66448309518944, 112.89723105273795, 178.250872179196, 94.08139883899989, 70.31403129036322, 71.30445657433543, 41.59512896798625, 35.65328734416697, 26.740524514385434, 27.730394360619336, 25.750217629435532, 24.75991083754749, 24.75991048092772, 22.779296793268415, 22.779297438043095, 21.78898992029242, 48.524905665326486, 20.798683547402845, 19.808377609860592, 44.55509414723236, 43.571375217071626, 15.847147947717477, 15.847148869493061, 15.847151510006372, 110.96518319676315, 14.856840935366376, 51.51144674576528, 13.866535394784487, 377.1906622775867, 64.3653102380918, 238.62833200762356, 1215.5327448184028, 82.1455587911492, 241.62484025541107, 256.49777087974314, 135.0444367634264, 128.7165579736225, 59.439171877365986, 690.7051946384405, 327.0633570814134, 609.5128255156781, 110.8782239126798, 989.0656292822646, 81.19296768351253, 349.62671395602285, 1695.557551733843, 175.64970572994122, 270.1076845748449, 124.74354849047852, 678.4909800547837, 172.05381183898936, 299.4073714715377, 142.92901349839957, 360.03295808349503, 333.2703224524581, 957.9269390599923, 338.42013158300233, 2491.3714996311905, 872.2630062899794, 849.4386663701811, 665.1653911561459, 2816.5117469437173, 5708.006327309899, 2935.573529329324, 4744.657685582957, 4285.735302790172, 1574.2639429941196, 2475.279451608682, 4180.473018673696, 4281.088020957158, 3914.2045932125175, 4426.90768704306, 1409.9393860246705, 5877.794877218924, 709.3685369827962, 539.6475707479251, 1239.6804419620025, 1792.4860249232445, 2994.385846304359, 2273.558032701072, 113.7319758262852, 99.88678409794856, 94.94207278320413, 65.27379989785047, 208.73553313712247, 63.295914938068364, 70.21865042270035, 63.295905798315026, 60.32909057977926, 74.17390795094812, 52.41754930990337, 52.417549421466106, 51.428606186715264, 48.46178050119713, 43.51706686915053, 59.34031571315485, 56.37365765257808, 39.56129768572275, 38.57235502017148, 72.2038810894663, 80.10704895467019, 31.649758872242575, 35.60574956036052, 31.65200532522512, 28.682930989420413, 91.0802660919531, 27.69398833486689, 33.62738250413999, 25.7161039849392, 25.71613456471717, 456.40207996525635, 354.0436276447834, 93.993131113867, 62.313658182651686, 278.33950536194476, 202.78786391463882, 47.47101752095395, 66.27800980385636, 329.2922592941776, 155.4693449972556, 97.24889475685255, 88.01624085083526, 294.7217305233519, 196.94503709395553, 123.63690035998255, 673.4687875695668, 554.187115169672, 121.77047572777064, 208.81641800283512, 429.0262441256313, 832.926753402112, 688.6048606271551, 597.2376148149101, 586.9154771343789, 147.34025943954936, 165.42638176787474, 261.69524857169426, 6263.399304222148, 1397.7691856551728, 299.1509429158269, 619.73947753154, 2087.766507643434, 514.3334084347566, 521.2984904541034, 217.73652591836736, 599.3295315839156, 1080.0996468438593, 711.1572590110691, 535.8645447419964, 5877.794877218924, 1194.7384448125463, 1478.5855188251512, 1238.9529117892746, 696.3180840627908, 995.8149630389505, 692.2058251769893, 1505.8823335535526, 1469.5369273223478, 863.2644517424043, 885.4949030445689, 5035.0633264505595, 2491.3714996311905, 49.74764089534898, 46.6356000696509, 42.48624348207585, 42.48624292167882, 35.2248450203587, 33.15015634679578, 31.07547531064076, 29.00079013582418, 25.88876280690752, 24.851417019785167, 22.776735522315278, 22.776734979902482, 22.776730357271703, 21.739391956923864, 21.739348950670635, 32.09164361659995, 20.70205114141772, 27.94737414198827, 19.66470308396862, 18.627364102022863, 18.62735328057048, 18.627299930264275, 17.590022335789243, 16.552681087681535, 16.55268062965122, 17.584345950596916, 15.515335815170445, 15.515335883272142, 15.515334163797322, 14.47799225620109, 24.78529395498295, 36.024208129673276, 270.47253583336027, 59.61927987381623, 30.944585123723854, 222.10080624844304, 73.79547426749758, 83.93393783141258, 85.42936272639334, 112.658634026265, 28.795776235528475, 129.16057090373297, 1344.8391458531828, 128.53175901720812, 732.3315309453653, 239.17698014421148, 739.619633495807, 107.1753582054914, 596.0139332144148, 514.3334084347566, 4744.657685582957, 245.26017227807492, 5877.794877218924, 2128.456899014544, 658.2116228551544, 501.1115254420876, 2816.5117469437173, 1029.0574577788423, 6263.399304222148, 2475.279451608682, 2994.385846304359, 1139.8032363246261, 521.2290959492913, 1011.5801983126993, 4281.088020957158, 919.1779775807647, 5708.006327309899, 2273.558032701072, 1239.6804419620025, 4180.473018673696, 1008.0706304399775, 1469.5369273223478, 1238.9529117892746, 113.52095381186678, 48.22144804115609, 49.22340821135086, 26.114300676473228, 26.114283279012128, 25.109431046892286, 19.080209019856213, 32.134181232502655, 57.281745465083354, 16.06559040611766, 23.09159906489923, 14.055857408094214, 14.055857061485389, 14.055856546119896, 16.062239012858424, 19.07331584045105, 13.050986804357422, 11.041246455546368, 11.041246267396374, 11.041221720725638, 10.036375984712164, 10.036375647716875, 10.036373213043518, 10.036373651149448, 10.036374324603814, 10.036375070714834, 9.031505260585934, 9.031504892808593, 9.031505564156065, 9.031504796205297, 124.39554347452722, 123.27126156987828, 26.079002665327288, 169.32416925400787, 257.60427304578127, 174.3424857512983, 29.18623396240811, 51.13518183453822, 19.053669948226762, 179.0897003517701, 208.9273042294157, 509.5450807142889, 31.05279241182951, 161.0675827294709, 969.8640135668975, 498.6116350941321, 2471.1533218169957, 116.88385268371317, 838.5107670182048, 687.2285961924339, 888.6233985357248, 520.9850835239652, 119.90221606860207, 98.84364826170258, 423.86959212617893, 320.5553915892546, 197.2608475232119, 3595.53138047052, 1825.0047677750888, 1792.4860249232445, 5035.0633264505595, 4180.473018673696, 955.3955891754367, 4744.657685582957, 4285.735302790172, 2935.573529329324, 2491.3714996311905, 1574.2639429941196, 5708.006327309899, 2816.5117469437173, 1455.1636469274836, 1167.95644333932, 2128.456899014544, 1287.8284067994666, 3553.004789066787, 4281.088020957158, 1848.7738365744858, 141.19911297345348, 225.12631771023723, 72.08319820501404, 59.247385381040836, 68.1324723685367, 52.33579454055711, 47.39894294676357, 47.398942971292605, 46.41157308591619, 49.37386297304161, 41.47472238870386, 48.387227611951374, 35.55050123680568, 34.56313024245814, 32.58838986721944, 32.58838990540256, 32.588391013727616, 31.601019802910734, 31.601019659640492, 31.601019708344666, 30.61364991357571, 29.626279274247988, 29.626279705823197, 29.62627383226165, 28.638909292961305, 27.651539060414926, 27.651539752377253, 320.0457805856632, 26.66416827640983, 66.16023340251456, 225.23647852627838, 100.7431296878547, 120.47114273428724, 53.33161432931944, 69.1238734840988, 332.9773592911482, 139.0446867182728, 153.065983163861, 134.30453985067166, 267.9503032253334, 639.2558971023262, 212.39929911975162, 76.03208787769337, 125.42334094826919, 114.49812523234053, 373.33163089398124, 173.97293979829104, 247.99456383683392, 2549.0809002318015, 338.59375189828256, 821.3303049169135, 1689.349754687385, 566.1722002361055, 121.571952330756, 1322.57860575191, 346.1208764978683, 1057.164601368426, 357.9119153652116, 523.7104838552724, 388.98610317794123, 1451.032695311428, 1505.8823335535526, 1008.0706304399775, 1469.5369273223478, 919.1779775807647, 4426.90768704306, 1540.4868668157762, 3914.2045932125175, 1080.0996468438593, 842.0931094538637, 1238.9529117892746, 3553.004789066787, 1478.5855188251512, 840.8946833221651, 5035.0633264505595, 1139.8032363246261, 892.3492029944962, 4744.657685582957, 2994.385846304359, 2087.766507643434, 6263.399304222148, 4281.088020957158, 5708.006327309899, 5877.794877218924], \"Category\": [\"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Default\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic1\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic2\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic3\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic4\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic5\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic6\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic7\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic8\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic9\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\", \"Topic10\"], \"logprob\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, -6.678, -6.448, -6.9104, -7.0681, -7.637, -7.6761, -7.6761, -7.7167, -7.8034, -7.8034, -7.8034, -7.7619, -7.8497, -7.9493, -6.22, -8.0599, -8.0603, -8.1201, -8.1203, -7.2911, -8.1843, -8.1843, -8.1843, -8.1877, -8.2528, -8.2528, -8.3263, -8.3263, -8.3263, -8.3263, -7.0229, -6.5568, -7.1731, -7.719, -6.9318, -7.7626, -4.0176, -6.1016, -5.8746, -6.1293, -6.1735, -4.8223, -7.0193, -4.3425, -5.2686, -6.1526, -7.023, -5.794, -6.8516, -3.6646, -3.963, -6.8051, -5.1491, -4.4081, -6.8319, -4.8462, -4.3746, -5.8476, -5.4955, -6.1068, -3.9366, -5.0944, -5.031, -5.5665, -4.6565, -3.9029, -4.1689, -4.2822, -4.5105, -4.0323, -5.2018, -5.1134, -4.2877, -5.3219, -4.3137, -5.1649, -5.0328, -5.1784, -5.073, -4.993, -5.3228, -5.2321, -5.321, -5.4116, -5.7329, -6.5734, -6.8472, -7.0185, -6.7544, -7.0502, -7.2254, -7.2645, -7.3475, -7.3475, -7.3491, -7.4867, -7.3541, -7.6483, -7.6499, -7.7727, -7.7727, -7.7727, -7.7727, -7.7727, -7.7749, -7.8412, -7.8412, -7.8412, -7.8434, -7.5559, -7.0875, -7.9148, -7.9148, -7.12, -7.6143, -6.3367, -7.3475, -7.2132, -5.872, -6.4399, -6.161, -7.0301, -5.9298, -6.7183, -7.1414, -5.4783, -5.057, -5.062, -4.1823, -6.5016, -4.1554, -6.259, -5.051, -4.5996, -5.1614, -6.4687, -4.6207, -4.6792, -5.2462, -6.0735, -4.328, -5.5623, -4.3132, -3.855, -4.0844, -5.2217, -4.104, -5.6651, -5.1439, -5.4594, -4.4302, -5.3788, -5.5423, -4.5748, -4.9622, -4.9198, -4.8925, -5.1607, -5.1648, -5.4484, -5.2866, -5.3256, -5.3075, -5.3714, -3.4648, -3.52, -3.537, -4.5874, -3.4195, -5.5093, -5.9106, -6.2651, -5.9619, -6.3328, -6.5044, -6.5044, -6.5688, -6.5912, -6.5912, -6.6616, -6.6616, -6.6863, -6.7374, -6.764, -6.8781, -6.9731, -6.9731, -5.3719, -7.0421, -7.1548, -7.2378, -7.2378, -7.2391, -4.7412, -5.377, -5.6437, -5.7893, -3.4185, -4.1325, -5.178, -6.6067, -5.6593, -5.6348, -3.8267, -5.7099, -4.6001, -5.9985, -5.9607, -5.8556, -5.6498, -3.2254, -4.3352, -3.1643, -3.4644, -3.4972, -4.6434, -4.8608, -4.7763, -4.6861, -4.9446, -4.9374, -4.8427, -4.5002, -4.5016, -4.6535, -4.6746, -4.9403, -4.9307, -5.1707, -5.8126, -7.3319, -7.8323, -8.0788, -8.3259, -8.3259, -8.3995, -8.4789, -8.4789, -8.4789, -8.6596, -8.7638, -8.7638, -8.7638, -8.0262, -8.8803, -8.8803, -8.8803, -7.7122, -9.012, -9.012, -9.012, -9.012, -6.186, -8.1585, -8.3314, -9.1638, -9.1638, -9.1639, -9.1639, -4.9545, -8.4077, -6.3259, -6.2889, -6.1833, -5.2083, -6.1361, -5.8365, -4.4854, -4.9863, -7.5046, -4.8872, -6.0583, -6.1362, -4.4916, -5.4978, -5.8144, -5.6825, -4.7668, -5.7172, -6.5943, -5.1787, -4.548, -3.2406, -4.4513, -3.5548, -6.3419, -3.4953, -5.113, -4.7962, -4.5441, -4.2825, -4.0799, -4.432, -5.0858, -4.2865, -3.7658, -4.2392, -5.175, -4.0736, -4.1942, -3.9277, -4.628, -4.1036, -4.4686, -4.5027, -4.7961, -4.9587, -5.0465, -5.3365, -5.0359, -5.304, -6.1986, -6.5409, -6.5412, -4.6412, -6.6876, -6.7137, -6.7691, -7.0278, -7.1045, -7.1451, -7.2317, -7.2317, -7.232, -6.1528, -7.3776, -7.4314, -7.4314, -7.4882, -7.4882, -7.4883, -7.4347, -7.49, -7.5485, -7.5485, -7.5485, -7.6126, -7.6812, -5.894, -3.8872, -4.3479, -3.5307, -5.9171, -4.5284, -4.6659, -4.9613, -6.7339, -6.4571, -6.7958, -5.4307, -6.2829, -5.4142, -4.4531, -5.7114, -5.4497, -4.9728, -5.6385, -5.4082, -5.4868, -5.7598, -4.869, -5.6924, -5.1453, -4.7489, -5.7707, -5.1345, -5.1546, -5.4323, -4.6171, -4.6983, -4.6389, -5.0683, -4.7483, -4.9903, -5.0305, -5.019, -5.0422, -5.2352, -5.1235, -5.1913, -5.1442, -5.1985, -5.2329, -5.3492, -5.791, -5.9464, -6.1384, -5.6828, -6.3224, -6.6168, -6.6029, -7.1507, -7.3085, -7.6048, -7.5689, -7.6438, -7.6845, -7.6845, -7.7711, -7.7711, -7.8174, -7.0181, -7.866, -7.917, -7.1073, -7.1385, -8.152, -8.152, -8.152, -6.2067, -8.2205, -6.9814, -8.2941, -4.9992, -6.7609, -5.4663, -3.8685, -6.5295, -5.479, -5.4276, -6.054, -6.1072, -6.85, -4.5354, -5.2476, -4.6627, -6.266, -4.2177, -6.5614, -5.2288, -3.793, -5.8613, -5.4998, -6.1974, -4.7446, -5.9332, -5.4863, -6.0966, -5.3864, -5.4699, -4.6979, -5.4658, -4.011, -4.7765, -4.8371, -5.0138, -4.2008, -3.7929, -4.3162, -4.0749, -4.2494, -4.7614, -4.592, -4.3828, -4.4289, -4.4938, -4.5007, -4.9411, -4.5193, -5.1626, -5.2955, -5.1666, -5.1634, -5.1328, -5.1596, -6.4191, -6.55, -6.6013, -6.9803, -5.8181, -7.0115, -6.9077, -7.0121, -7.0602, -6.854, -7.203, -7.203, -7.2224, -7.2829, -7.3927, -7.083, -7.1343, -7.4901, -7.516, -6.8936, -6.79, -7.719, -7.6027, -7.7205, -7.8204, -6.6658, -7.8567, -7.6635, -7.9333, -7.9334, -5.0707, -5.3259, -6.64, -7.0512, -5.5933, -5.9029, -7.3288, -7.0111, -5.4986, -6.2078, -6.653, -6.7469, -5.638, -6.0215, -6.4415, -4.9363, -5.11, -6.4585, -5.9854, -5.3892, -4.8275, -5.0184, -5.1575, -5.181, -6.3156, -6.2244, -5.8672, -3.3731, -4.6079, -5.7896, -5.253, -4.3701, -5.4075, -5.4085, -6.0348, -5.3604, -4.9726, -5.3016, -5.481, -4.1694, -5.1228, -5.0134, -5.1185, -5.4125, -5.2486, -5.4449, -5.2292, -5.2566, -5.4103, -5.4225, -5.2563, -5.3505, -5.7386, -5.8045, -5.8995, -5.8995, -6.0914, -6.1537, -6.2202, -6.2914, -6.4087, -6.4511, -6.5417, -6.5417, -6.5417, -6.5902, -6.5902, -6.201, -6.6412, -6.3421, -6.695, -6.7518, -6.7519, -6.7519, -6.8121, -6.8762, -6.8762, -6.8194, -6.9448, -6.9448, -6.9448, -7.0183, -6.5121, -6.1612, -4.3111, -5.7218, -6.317, -4.6677, -5.6158, -5.5127, -5.6168, -5.5414, -6.4637, -5.4817, -3.997, -5.5118, -4.5158, -5.2146, -4.6025, -5.688, -4.7519, -4.9536, -4.0282, -5.4146, -4.1393, -4.575, -5.1076, -5.2127, -4.6661, -5.0114, -4.4267, -4.7438, -4.7093, -5.0179, -5.2577, -5.1526, -4.8594, -5.1792, -4.8765, -5.0426, -5.1854, -5.0347, -5.2583, -5.3095, -5.4052, -5.6831, -6.5439, -6.5254, -7.1733, -7.1733, -7.2139, -7.5002, -6.9795, -6.4056, -7.6814, -7.3238, -7.8235, -7.8235, -7.8235, -7.6945, -7.5228, -7.9029, -8.0836, -8.0836, -8.0837, -8.1879, -8.1879, -8.1879, -8.1879, -8.1879, -8.1879, -8.3043, -8.3043, -8.3043, -8.3043, -5.6832, -5.7259, -7.2592, -5.4568, -5.0837, -5.4899, -7.1897, -6.6686, -7.5877, -5.5477, -5.4548, -4.6695, -7.1729, -5.753, -4.2119, -4.7846, -3.4868, -6.0447, -4.4278, -4.6573, -4.5039, -4.9287, -6.0941, -6.266, -5.185, -5.3991, -5.7677, -3.8448, -4.3615, -4.4892, -3.8668, -3.9932, -4.8922, -4.0033, -4.0598, -4.4197, -4.5986, -4.8311, -4.3473, -4.8366, -5.0403, -5.098, -4.9512, -5.0912, -4.9696, -4.9866, -5.1388, -6.3733, -5.908, -7.0518, -7.2506, -7.1118, -7.3767, -7.4776, -7.4776, -7.4991, -7.4383, -7.6139, -7.4631, -7.7717, -7.8006, -7.861, -7.861, -7.861, -7.8927, -7.8927, -7.8927, -7.9253, -7.9591, -7.9591, -7.9591, -7.9941, -8.0303, -8.0303, -5.5823, -8.0679, -7.1596, -5.9439, -6.745, -6.5696, -7.3768, -7.1203, -5.5966, -6.4454, -6.3713, -6.505, -5.879, -5.09, -6.1154, -7.0755, -6.6229, -6.7097, -5.64, -6.3299, -6.0114, -3.9353, -5.7491, -5.0023, -4.4079, -5.3585, -6.6681, -4.6652, -5.8043, -4.9015, -5.7815, -5.4737, -5.7204, -4.6717, -4.6653, -4.9878, -4.7079, -5.0884, -3.926, -4.7581, -4.1894, -5.0879, -5.2524, -5.0544, -4.4671, -5.025, -5.3627, -4.5056, -5.2345, -5.3825, -4.7425, -4.9378, -5.0858, -4.7659, -4.9552, -5.0313, -5.3294], \"loglift\": [30.0, 29.0, 28.0, 27.0, 26.0, 25.0, 24.0, 23.0, 22.0, 21.0, 20.0, 19.0, 18.0, 17.0, 16.0, 15.0, 14.0, 13.0, 12.0, 11.0, 10.0, 9.0, 8.0, 7.0, 6.0, 5.0, 4.0, 3.0, 2.0, 1.0, 2.1355, 2.1337, 2.1298, 2.1293, 2.1147, 2.1133, 2.1133, 2.1119, 2.1086, 2.1086, 2.1086, 2.1076, 2.1068, 2.1025, 2.0997, 2.0972, 2.0968, 2.0941, 2.0939, 2.0932, 2.0905, 2.0905, 2.0905, 2.0871, 2.0865, 2.0865, 2.082, 2.0819, 2.0819, 2.0819, 2.0725, 2.0595, 2.0681, 2.0705, 2.0534, 2.0656, 1.7966, 1.9501, 1.9279, 1.932, 1.9252, 1.7997, 1.9969, 1.7307, 1.8119, 1.8763, 1.9812, 1.7558, 1.916, 1.1952, 1.26, 1.887, 1.4903, 1.2547, 1.8872, 1.323, 1.1385, 1.575, 1.4063, 1.6292, 0.7358, 1.1895, 1.1423, 1.3463, 0.8695, 0.4536, 0.5046, 0.5658, 0.6814, 0.3535, 1.0977, 0.9586, 0.2235, 1.0514, -0.0208, 0.8527, 0.6286, 0.794, 0.5303, 0.0577, 0.7453, -0.2012, -0.0147, 2.5615, 2.5594, 2.5492, 2.5436, 2.5392, 2.5386, 2.5383, 2.5329, 2.5315, 2.5285, 2.5285, 2.5269, 2.5228, 2.5219, 2.5153, 2.5136, 2.5086, 2.5086, 2.5086, 2.5086, 2.5086, 2.5063, 2.5045, 2.5045, 2.5045, 2.5023, 2.5022, 2.5007, 2.4999, 2.4999, 2.4984, 2.495, 2.4543, 2.4867, 2.4376, 2.2804, 2.3428, 2.238, 2.3856, 2.181, 2.3124, 2.3854, 2.0337, 1.9307, 1.9273, 1.6657, 2.2328, 1.5999, 2.1207, 1.6972, 1.5165, 1.7204, 2.1526, 1.292, 1.2705, 1.54, 1.9217, 0.9783, 1.6416, 0.9081, 0.5308, 0.5879, 1.2258, 0.2525, 1.5389, 1.0927, 1.263, 0.1405, 1.1393, 1.3071, 0.1224, 0.41, 0.1111, -0.0446, 0.3654, 0.05, 0.9629, -0.5236, -0.2335, -0.6341, -0.1485, 2.6611, 2.661, 2.661, 2.6592, 2.6588, 2.6551, 2.6517, 2.6474, 2.6468, 2.6464, 2.6435, 2.6435, 2.6423, 2.6419, 2.6418, 2.6404, 2.6404, 2.6399, 2.6387, 2.6381, 2.6353, 2.6326, 2.6326, 2.631, 2.6303, 2.6269, 2.6239, 2.6239, 2.6225, 2.6221, 2.6194, 2.6171, 2.6159, 2.4333, 2.4611, 2.4158, 2.5833, 2.4515, 2.4392, 2.1317, 2.445, 2.2266, 2.4681, 2.4523, 2.4135, 2.3288, 1.448, 1.7638, 1.1286, 1.2079, 1.2, 1.2312, 1.399, 1.2205, 0.858, 1.1235, 1.035, 0.8529, 0.0704, -0.1451, 0.2064, 0.1734, 0.8024, 0.1003, 0.5032, 2.0772, 2.0761, 2.0608, 2.0474, 2.0364, 2.0364, 2.0316, 2.0262, 2.0262, 2.0262, 2.0122, 2.0029, 2.0029, 2.0029, 1.9988, 1.9915, 1.9915, 1.9915, 1.9895, 1.9772, 1.9771, 1.9771, 1.9771, 1.9707, 1.9659, 1.9649, 1.9584, 1.9584, 1.9583, 1.9583, 1.9576, 1.9527, 1.9303, 1.9114, 1.9079, 1.8822, 1.9013, 1.8883, 1.8438, 1.8143, 1.925, 1.7516, 1.8199, 1.8107, 1.6796, 1.715, 1.7453, 1.7262, 1.6009, 1.7148, 1.8072, 1.6296, 1.4732, 1.2706, 1.4536, 1.2082, 1.7188, 1.1447, 1.4028, 1.3054, 1.2085, 1.0897, 0.9708, 1.0811, 1.3505, 0.9348, 0.62, 0.853, 1.3554, 0.6236, 0.6538, 0.4288, 0.9161, 0.467, 0.5623, 0.3572, 0.5102, 0.9121, 1.1242, 2.7134, 2.7129, 2.7127, 2.7054, 2.6996, 2.6993, 2.6988, 2.6959, 2.6958, 2.6944, 2.687, 2.6845, 2.683, 2.6798, 2.6797, 2.6795, 2.6748, 2.6736, 2.6711, 2.6711, 2.6683, 2.6683, 2.6683, 2.6678, 2.6666, 2.6652, 2.6652, 2.6652, 2.6617, 2.6577, 2.6529, 2.5964, 2.5828, 2.548, 2.6188, 2.5361, 2.45, 2.4504, 2.5989, 2.5479, 2.5621, 2.3033, 2.3946, 2.1602, 1.8377, 2.109, 1.988, 1.7717, 2.0528, 1.9357, 1.9239, 2.068, 1.3726, 1.8782, 1.39, 0.9876, 1.9015, 1.1298, 1.0177, 1.3463, -0.2607, -0.1277, -0.346, 0.5313, -0.3626, 0.1018, 0.0202, -0.3217, -0.4023, 0.5576, -0.3605, 0.181, -0.4707, -0.1675, -0.3849, 2.1744, 2.1727, 2.1715, 2.1705, 2.1694, 2.1689, 2.1656, 2.1655, 2.1567, 2.153, 2.1444, 2.144, 2.1431, 2.1417, 2.1417, 2.1384, 2.1384, 2.1366, 2.1352, 2.1345, 2.1323, 2.1313, 2.1225, 2.1204, 2.1204, 2.1204, 2.1195, 2.1164, 2.1122, 2.1118, 2.1034, 2.1099, 2.0942, 2.0639, 2.0974, 2.069, 2.0606, 2.0758, 2.0705, 2.1004, 1.9623, 1.9976, 1.9601, 2.061, 1.921, 2.0772, 1.9497, 1.8067, 2.0056, 1.9368, 2.0118, 1.7709, 1.9544, 1.8473, 1.9765, 1.7628, 1.7565, 1.4728, 1.7453, 1.2038, 1.4878, 1.4537, 1.5216, 0.8913, 0.5928, 0.7346, 0.4957, 0.423, 0.9125, 0.6293, 0.3144, 0.2445, 0.2692, 0.1393, 0.843, -0.1628, 1.3084, 1.449, 0.7462, 0.3806, -0.1019, 0.1467, 1.8824, 1.8813, 1.8809, 1.8765, 1.8763, 1.8761, 1.8761, 1.8755, 1.8754, 1.875, 1.8731, 1.8731, 1.8728, 1.8717, 1.8696, 1.8691, 1.8691, 1.8675, 1.8669, 1.8623, 1.862, 1.8617, 1.8602, 1.8601, 1.8587, 1.8579, 1.8575, 1.8566, 1.8549, 1.8549, 1.8413, 1.8401, 1.8521, 1.852, 1.8133, 1.8203, 1.8465, 1.8304, 1.7399, 1.7812, 1.8051, 1.811, 1.7114, 1.731, 1.7766, 1.5867, 1.6079, 1.7748, 1.7085, 1.5847, 1.4829, 1.4824, 1.4856, 1.4796, 1.7271, 1.7024, 1.601, 0.9198, 1.1849, 1.5449, 1.3531, 1.0214, 1.385, 1.3705, 1.6173, 1.2792, 1.078, 1.1669, 1.2705, 0.1871, 0.827, 0.7232, 0.7949, 1.0771, 0.8832, 1.0506, 0.489, 0.4861, 0.8643, 0.8267, -0.7451, -0.1357, 3.3898, 3.3886, 3.3867, 3.3867, 3.3823, 3.3806, 3.3788, 3.3767, 3.3729, 3.3714, 3.368, 3.368, 3.368, 3.3661, 3.366, 3.3658, 3.3639, 3.3629, 3.3616, 3.3589, 3.3589, 3.3588, 3.356, 3.3526, 3.3526, 3.349, 3.3488, 3.3488, 3.3488, 3.3445, 3.3131, 3.29, 3.1241, 3.2257, 3.2862, 2.9646, 3.1183, 3.0927, 2.9709, 2.7697, 3.2115, 2.6926, 1.8344, 2.6675, 1.9234, 2.3436, 1.8268, 2.6729, 1.8932, 1.8389, 0.5424, 2.1185, 0.2171, 0.7973, 1.4382, 1.6059, 0.426, 1.0876, -0.1337, 0.4775, 0.3216, 0.9789, 1.5215, 0.9635, -0.1859, 1.0327, -0.4908, 0.2637, 0.7274, -0.3375, 0.8613, 0.4332, 0.5082, 2.6203, 2.6157, 2.6136, 2.5997, 2.5996, 2.5982, 2.5865, 2.586, 2.5818, 2.5773, 2.5721, 2.5688, 2.5688, 2.5688, 2.5644, 2.5644, 2.5636, 2.5502, 2.5502, 2.5501, 2.5413, 2.5413, 2.5413, 2.5413, 2.5413, 2.5413, 2.5304, 2.5304, 2.5304, 2.5304, 2.5287, 2.4951, 2.5151, 2.4467, 2.4003, 2.3844, 2.472, 2.4323, 2.5004, 2.2998, 2.2386, 2.1324, 2.4269, 2.2006, 1.9464, 2.039, 1.7361, 2.2296, 1.876, 1.8454, 1.7418, 1.8509, 2.1546, 2.1758, 1.801, 1.8663, 1.9831, 1.0032, 1.1646, 1.0548, 0.6444, 0.704, 1.2811, 0.5673, 0.6126, 0.631, 0.6162, 0.8428, 0.0385, 0.2556, 0.7123, 0.8744, 0.4211, 0.7835, -0.1098, -0.3132, 0.3743, 1.7119, 1.7107, 1.7058, 1.703, 1.7022, 1.701, 1.6992, 1.6992, 1.6988, 1.6976, 1.6965, 1.6931, 1.6928, 1.692, 1.6904, 1.6904, 1.6904, 1.6895, 1.6895, 1.6895, 1.6886, 1.6876, 1.6876, 1.6876, 1.6865, 1.6854, 1.6854, 1.6847, 1.6841, 1.6837, 1.6743, 1.6778, 1.6743, 1.6821, 1.6792, 1.6307, 1.6552, 1.6332, 1.6303, 1.5656, 1.4851, 1.5615, 1.6288, 1.5809, 1.5851, 1.4729, 1.5466, 1.5105, 1.2566, 1.4615, 1.3221, 1.1954, 1.338, 1.5668, 1.1828, 1.3843, 1.1706, 1.3736, 1.3008, 1.3515, 1.0837, 1.053, 1.1318, 1.0348, 1.1235, 0.714, 0.9375, 0.5737, 0.9627, 1.0471, 0.859, 0.3927, 0.7115, 0.9382, 0.0056, 0.7623, 0.859, -0.1719, 0.0931, 0.3058, -0.473, -0.2818, -0.6455, -0.9729]}, \"token.table\": {\"Topic\": [1, 7, 9, 10, 5, 7, 1, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 5, 6, 7, 8, 9, 10, 1, 2, 3, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 3, 4, 5, 7, 8, 9, 10, 1, 1, 9, 1, 6, 9, 1, 1, 2, 3, 4, 10, 7, 9, 5, 2, 10, 2, 4, 7, 8, 10, 2, 7, 10, 2, 7, 8, 10, 2, 4, 7, 8, 10, 2, 7, 8, 10, 2, 7, 8, 10, 7, 10, 7, 10, 1, 2, 3, 5, 6, 9, 10, 7, 1, 2, 4, 5, 6, 7, 8, 9, 10, 2, 5, 6, 7, 10, 7, 3, 1, 2, 4, 5, 6, 7, 8, 10, 1, 3, 4, 5, 6, 7, 8, 10, 7, 10, 8, 1, 3, 9, 7, 8, 9, 10, 1, 9, 1, 5, 6, 9, 5, 6, 1, 5, 6, 7, 9, 10, 7, 3, 7, 1, 2, 3, 9, 10, 2, 3, 1, 2, 3, 5, 6, 3, 5, 6, 2, 5, 6, 8, 9, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 6, 3, 7, 8, 7, 1, 2, 7, 9, 10, 5, 3, 1, 1, 5, 10, 1, 3, 4, 1, 3, 7, 9, 2, 6, 7, 10, 7, 10, 5, 7, 10, 9, 1, 6, 10, 4, 7, 5, 6, 9, 2, 3, 4, 5, 7, 8, 9, 10, 1, 2, 4, 5, 7, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 7, 2, 7, 4, 1, 2, 3, 4, 7, 8, 9, 10, 3, 4, 10, 1, 2, 4, 7, 9, 10, 3, 4, 10, 3, 5, 6, 10, 3, 5, 1, 2, 3, 4, 5, 6, 7, 8, 10, 2, 5, 9, 2, 5, 6, 9, 10, 3, 4, 10, 1, 2, 4, 5, 6, 7, 8, 9, 10, 2, 4, 6, 8, 9, 10, 5, 8, 9, 1, 2, 4, 5, 6, 7, 8, 10, 5, 6, 7, 8, 10, 7, 1, 2, 5, 6, 7, 8, 9, 10, 5, 1, 4, 10, 1, 2, 3, 4, 4, 5, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 2, 5, 4, 4, 1, 5, 6, 10, 1, 4, 7, 9, 3, 5, 6, 1, 10, 10, 2, 6, 3, 6, 3, 4, 1, 3, 4, 7, 10, 4, 1, 9, 10, 1, 9, 10, 6, 10, 4, 7, 1, 2, 4, 9, 10, 1, 4, 5, 4, 6, 8, 2, 7, 2, 6, 7, 1, 5, 6, 3, 5, 6, 5, 1, 2, 4, 5, 6, 7, 8, 9, 10, 3, 5, 6, 4, 10, 3, 4, 5, 10, 1, 2, 4, 5, 7, 9, 10, 10, 4, 1, 2, 4, 9, 3, 5, 6, 3, 4, 1, 4, 7, 9, 10, 4, 3, 3, 10, 10, 2, 3, 4, 5, 6, 7, 9, 5, 3, 3, 7, 10, 2, 6, 7, 8, 4, 10, 1, 2, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 4, 5, 6, 7, 10, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 6, 7, 8, 9, 10, 8, 5, 1, 2, 3, 4, 5, 6, 8, 9, 10, 10, 3, 5, 2, 9, 1, 2, 4, 5, 6, 7, 9, 8, 2, 3, 4, 5, 6, 7, 9, 10, 6, 5, 6, 8, 2, 6, 1, 2, 4, 9, 10, 5, 3, 5, 6, 7, 10, 1, 2, 4, 7, 8, 9, 10, 1, 3, 5, 7, 8, 10, 3, 5, 7, 8, 10, 3, 5, 1, 2, 4, 5, 6, 7, 8, 9, 10, 2, 5, 10, 2, 2, 7, 1, 2, 4, 6, 7, 8, 9, 10, 1, 2, 4, 5, 6, 7, 8, 9, 10, 1, 3, 7, 1, 7, 8, 10, 2, 7, 8, 10, 5, 5, 5, 7, 1, 2, 3, 4, 5, 6, 7, 9, 10, 5, 1, 2, 4, 5, 6, 7, 9, 10, 2, 4, 6, 7, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 3, 5, 8, 10, 10, 9, 9, 1, 2, 3, 4, 9, 10, 6, 7, 9, 6, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 1, 10, 10, 1, 3, 4, 5, 6, 8, 9, 10, 8, 10, 2, 8, 4, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 3, 10, 2, 3, 5, 1, 7, 8, 2, 7, 4, 7, 3, 5, 7, 8, 5, 7, 5, 6, 7, 7, 1, 4, 5, 7, 10, 3, 2, 1, 2, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 6, 7, 8, 9, 10, 1, 3, 4, 5, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 2, 6, 9, 2, 5, 6, 7, 8, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 2, 4, 5, 6, 9, 10, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 4, 6, 7, 9, 4, 9, 1, 2, 3, 4, 7, 8, 9, 10, 2, 9, 1, 3, 4, 10, 7, 3, 7, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 2, 10, 1, 3, 1, 4, 9, 10, 10, 9, 8, 2, 3, 5, 6, 9, 1, 2, 4, 5, 6, 7, 8, 9, 10, 2, 4, 6, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 10, 4, 7, 8, 1, 2, 4, 7, 8, 9, 10, 2, 3, 5, 6, 7, 8, 10, 2, 3, 5, 6, 8, 2, 4, 5, 6, 7, 8, 9, 10, 1, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 9, 10, 5, 8, 9, 1, 2, 3, 4, 6, 9, 9, 1, 3, 7, 8, 5, 6, 7, 10, 8, 3, 10, 9, 7, 7, 7, 10, 9, 7, 9, 2, 5, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 10, 1, 2, 4, 5, 6, 7, 5, 6, 5, 6, 10, 2, 6, 9, 2, 2, 4, 7, 10, 1, 6, 7, 8, 10, 7, 2, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 6, 8, 10, 1, 4, 9, 10, 2, 3, 6, 7, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 5, 7, 8, 8, 6, 1, 2, 5, 3, 6, 1, 5, 6, 7, 4, 5, 3, 2, 3, 5, 6, 2, 6, 6, 1, 2, 3, 4, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 5, 6, 6, 1, 1, 2, 7, 1, 2, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 10, 1, 2, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 1, 2, 4, 5, 6, 7, 8, 9, 10, 10, 10, 5, 6, 6, 9, 3, 2, 9, 6, 7, 8, 10, 8, 2, 4, 5, 6, 7, 8, 9, 10, 2, 3, 5, 6, 6, 1, 3, 4, 10, 5, 7, 5, 6, 3, 6, 1, 2, 4, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 10, 9, 8, 5, 1, 7, 5, 6, 7, 8, 10, 9, 1, 4, 3, 5, 6, 9, 10, 1, 2, 4, 10, 3, 4, 6, 7, 7, 7, 8, 10, 3, 8, 9, 4, 4, 6, 9, 1, 3, 4, 5, 6, 7, 9, 10, 3, 5, 6, 9, 3, 5, 3, 5, 7, 5, 1, 2, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 6, 10, 3, 5, 7, 8, 5, 7, 1, 2, 3, 4, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 5, 6, 1, 3, 4, 5, 7, 8, 10, 7, 1, 2, 3, 4, 5, 3, 4, 6, 7, 10, 4, 2, 4, 5, 6, 9, 10, 2, 5, 6, 1, 3, 7, 8, 3, 2, 3, 5, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 5, 6, 7, 9, 3, 8, 6, 5, 10, 5, 7, 2, 3, 2, 1, 2, 7, 10, 10, 2, 3, 5, 6, 3, 4, 5, 6, 2, 5, 6, 6, 2, 3, 4, 5, 6, 9, 1, 3, 4, 10, 10, 4, 6, 5, 2, 5, 9, 10, 1, 4, 10, 8, 5, 6, 4, 2, 5, 6, 2, 3, 5, 6, 8, 9, 5, 1, 9, 10, 8, 7, 8, 10, 1, 2, 3, 6, 9, 7, 1, 2, 4, 5, 6, 7, 9, 10, 4, 5, 7, 10, 1, 2, 4, 6, 7, 8, 9, 10, 2, 3, 5, 9, 4, 10, 3, 7, 2, 1, 8, 3, 5, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 4, 7, 9, 10, 1, 5, 1, 2, 4, 5, 7, 9, 10, 7, 8, 10, 9, 1, 2, 3, 4, 5, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 4, 1, 5, 6, 7, 10, 2, 3, 7, 8, 1, 3, 5, 6, 7, 10, 7, 8, 3, 3, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 8, 2, 6, 7, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 4, 8, 1, 2, 4, 5, 6, 7, 8, 10, 1, 7, 9, 3, 5, 6, 8, 9, 10, 5, 1, 8, 9, 7, 1, 2, 3, 4, 5, 7, 9, 10, 3, 5, 5, 1, 2, 4, 5, 6, 7, 8, 10, 9, 3, 5, 8, 10, 1, 5, 6, 8, 9, 1, 2, 4, 5, 6, 7, 8, 9, 2, 7, 10, 3, 8, 1, 1, 2, 7, 10, 4, 3, 1, 3, 4, 6, 7, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 9, 9, 2, 4, 6, 7, 8, 9, 10, 3, 7, 10, 5, 6, 4, 1, 9, 3, 7, 2, 7, 10, 1, 4, 10, 2, 3, 8, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 6, 9, 6, 7, 10, 9, 1, 2, 3, 4, 6, 2, 1, 1, 2, 1, 4, 7, 7, 8, 10, 8, 5, 8, 1, 2, 3, 4, 5, 6, 7, 9, 10, 2, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 9, 4, 5, 6, 7, 8, 2, 5, 6, 8, 9, 10, 8, 3, 5, 6, 7, 8, 1, 9, 3, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 3, 4, 5, 7, 10, 1, 2, 3, 4, 5, 9, 6, 1, 2, 4, 5, 6, 7, 8, 9, 10, 1, 2, 4, 5, 6, 7, 9, 10, 9, 2, 2, 3, 5, 6, 7, 8, 9, 6, 4, 1, 2, 4, 7, 8, 9, 10, 1, 3, 4, 9, 1, 8, 3, 7, 8, 10, 3, 7, 8, 2, 1, 2, 3, 4, 5, 6, 8, 9, 10, 1, 3, 4, 7, 8, 8, 8, 1, 7, 8, 10, 1, 2, 4, 5, 6, 7, 8, 9, 10, 1, 4, 5, 6, 7, 8, 9, 10, 1, 1, 2, 4, 5, 6, 7, 8, 9, 10, 1, 4, 9, 10, 8, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 7, 2, 2, 8, 3, 6, 8, 3, 4, 10, 1, 7, 1, 9, 10, 4, 6, 9, 10, 2, 7, 8, 8, 1, 4, 10, 8, 1, 3, 3, 7, 8, 10, 3, 4, 6, 7, 9, 10, 3, 5, 7, 8, 10, 8, 5, 6, 7, 9, 1, 4, 5, 6, 7, 8, 9, 10, 6, 8, 7, 8, 4, 6, 9, 4, 5, 9, 1, 2, 3, 5, 6, 7, 10, 2, 3, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 2, 3, 5, 6, 7, 8, 10, 1, 2, 1, 2, 3, 4, 6, 7, 10, 6, 9, 1, 4, 5, 6, 7, 8, 10, 1, 2, 4, 5, 6, 7, 8, 9, 10, 7, 2, 5, 6, 8, 1, 2, 9, 10, 6, 2, 4, 5, 6, 9, 6], \"Freq\": [0.009862523138178904, 0.9320084365579063, 0.019725046276357808, 0.03451883098362616, 0.9522500850377771, 0.029757815157430534, 0.02249853994635378, 0.08999415978541513, 0.026717016186295114, 0.05062171487929601, 0.01828006370641245, 0.4851247675932534, 0.014061587466471114, 0.005624634986588445, 0.2868563843160107, 0.013721264011114748, 0.10573444620329601, 0.001614266354248794, 0.033092460262100276, 0.001614266354248794, 0.33415313532950036, 0.054885056044458994, 0.03147819390785148, 0.42374491799030845, 0.001360972945160495, 0.15583140222087669, 0.02994140479353089, 0.00816583767096297, 0.24565561660146937, 0.05103648544351856, 0.00272194589032099, 0.5049209626545437, 0.005264066471110761, 0.00965078853036973, 0.23688299119998427, 0.042989876180737886, 0.03421643206221995, 0.16055402736887822, 0.08773444118517935, 0.03948049853333071, 0.3842768523910856, 0.020407023768456253, 0.0019435260731863098, 0.4071687123325319, 0.0019435260731863098, 0.06607988648833453, 0.015548208585490478, 0.1098092231350265, 0.09814806669590864, 0.025265838951422027, 0.2546019155874066, 0.07851627298286064, 0.5884499168715469, 0.07682775098322922, 0.09118018798009622, 0.03208191799299682, 0.03968026699133817, 0.019418002995761234, 0.0742949679837821, 0.9890579083775481, 0.9335168524992954, 0.05834480328120596, 0.9154319786694431, 0.04818063045628648, 0.02409031522814324, 0.9699881544049412, 0.934568256897133, 0.025960229358253692, 0.8192413209574878, 0.01037014330325934, 0.15555214954889007, 0.957516777379939, 0.03191722591266463, 0.9554046850624393, 0.6829856495282329, 0.3124296056352555, 0.396034498771464, 0.0029335888797886226, 0.11440996631175628, 0.005867177759577245, 0.4781749874055455, 0.5293886466697778, 0.009453368690531748, 0.45848838149078974, 0.4294139130136985, 0.01700649160450291, 0.03188717175844296, 0.5208238053879016, 0.3574073659535076, 0.005795795123570393, 0.04057056586499275, 0.019319317078567978, 0.5757156489413257, 0.1506229802353854, 0.05906783538642565, 0.014766958846606412, 0.7737886435621759, 0.07532981542928319, 0.04708113464330199, 0.018832453857320797, 0.8568766505080962, 0.07839756261947305, 0.9211713607788082, 0.03320297217419489, 0.954585450008103, 0.4749463002728371, 0.002168704567455877, 0.013012227404735262, 0.002168704567455877, 0.18217118366629367, 0.32096827598346983, 0.004337409134911754, 0.9794008209424412, 0.06060918655541867, 0.08067845362674936, 0.11238789559945184, 0.0497717823369001, 0.3773022209410169, 0.13205577732935592, 0.025688661851303276, 0.13285854801220914, 0.028899744582716186, 0.016873030574687062, 0.45557182551655073, 0.013498424459749652, 0.02362224280456189, 0.48594328055098746, 0.9858119495932178, 0.9803945166132039, 0.0459560088017395, 0.07467851430282668, 0.06893401320260925, 0.03303088132625027, 0.011489002200434875, 0.44376270999179707, 0.005744501100217438, 0.31594756051195905, 0.001674375449895114, 0.0234412562985316, 0.0050231263496853425, 0.06195189164611922, 0.00837187724947557, 0.6664014290582554, 0.0050231263496853425, 0.22938943663563063, 0.9628707694247388, 0.01604784615707898, 0.9663202963883262, 0.05866802253108858, 0.11733604506217717, 0.8213523154352401, 0.6027218821802893, 0.2080362625590031, 0.021386905496719944, 0.16720671570162865, 0.982955297006069, 0.8857881127426799, 0.023379517897626094, 0.15363683189868574, 0.7180851925699443, 0.10353786497520127, 0.14429603949717618, 0.849743343705593, 0.12444519923279303, 0.004786353816645886, 0.12923155304943892, 0.05264989198310474, 0.6748758881470699, 0.009572707633291771, 0.9904712435981231, 0.013849671027529964, 0.9694769719270975, 0.44922033376532683, 0.037435027813777236, 0.018717513906888618, 0.46481826202106735, 0.028076270860332927, 0.9871421040603209, 0.9625164728182634, 0.06268542568301716, 0.03336482334741236, 0.06470753618892094, 0.06470753618892094, 0.7734572685081956, 0.29441749045490884, 0.04444037591772209, 0.6610505917761161, 0.015287557874468304, 0.11007041669617178, 0.8347006599459693, 0.021402581024255626, 0.018345069449361963, 0.9817058198462598, 0.031096959463511625, 0.086832279117344, 0.23179195169340588, 0.22844304836656618, 0.04784147609771019, 0.15500638255658103, 0.006697806653679427, 0.023681530668366545, 0.14519887995655043, 0.04377495062940483, 0.7839190641043197, 0.14584540727522224, 0.07292270363761112, 0.7983366361992572, 0.10504429423674437, 0.0910383883385118, 0.9721523247238053, 0.023952256938381978, 0.5868302949903584, 0.25149869785301077, 0.1277453703380372, 0.007984085646127325, 0.9520886397306126, 0.9719363210832728, 0.9681488106616944, 0.939732475241607, 0.9441226938528396, 0.9788606699173201, 0.9746987893117254, 0.017475357654520685, 0.9786200286531582, 0.1842652030528892, 0.06142173435096307, 0.03908655822334014, 0.7147256360839339, 0.3804187195668441, 0.0013783286940827683, 0.08821303642129717, 0.5299673828748245, 0.7365516779609083, 0.26105629092285354, 0.44047846809794555, 0.1171485287494536, 0.4357925269479674, 0.8967381087880149, 0.8521804581192737, 0.14203007635321227, 0.9788606841766874, 0.8707116033331894, 0.10883895041664868, 0.046608957354556196, 0.932179147091124, 0.8967380572335756, 0.05446151774908675, 0.09258458017344748, 0.47245366647332754, 0.061269207467722596, 0.0027230758874543376, 0.025869220930816207, 0.09258458017344748, 0.19878453978416663, 0.08327487225965165, 0.08552554448288548, 0.3173447834759698, 0.01350403333940297, 0.0157547055626368, 0.40962234462855673, 0.07539751947833324, 0.40899267671530853, 0.010680300313610207, 0.003769517757744779, 0.0841858965896334, 0.08921192026662643, 0.019475841748348023, 0.1243940860055777, 0.0358104186985754, 0.06596656076053363, 0.15643498694640834, 0.9764374874523493, 0.9813431062003488, 0.8893272185588419, 0.10040791177277247, 0.8856159599852758, 0.3853076709079174, 0.016887114868133705, 0.085842833913013, 0.17506309079965274, 0.006754845947253482, 0.0002814519144688951, 0.06417103649890808, 0.26569060725863697, 0.05879902681869453, 0.7506675757186669, 0.19011685338044565, 0.6581663231693097, 0.016099032365292466, 0.24337948928706846, 0.0018940038076814666, 0.035039070442107134, 0.0454560913843552, 0.01674879787176525, 0.6992623111461991, 0.2805423643520679, 0.12230204225422016, 0.3872898004716972, 0.4817959240317764, 0.005559183738828189, 0.0561880555363654, 0.9327217219036656, 0.05140047635552916, 0.007439542630405536, 0.01285011908888229, 0.060192663100553885, 0.17719637901511368, 0.006763220573095942, 0.31110814636241335, 0.00811586468771513, 0.36521391094718086, 0.011811662852452828, 0.15355161708188675, 0.826816399671698, 0.09996013272446715, 0.022213362827659365, 0.7848721532439642, 0.018511135689716136, 0.07034231562092132, 0.025201290846005767, 0.6876352216553002, 0.28441456811920796, 0.00045178263053772736, 0.02891408835441455, 0.38469290990287486, 0.04404880647742842, 0.13011339759486548, 0.002033021837419773, 0.009713326556561139, 0.033431914659791825, 0.3663957133660969, 0.0012233704297249686, 0.7743934820159051, 0.19329252789654502, 0.0012233704297249686, 0.004893481718899874, 0.025690779024224338, 0.09694749462645184, 0.8725274516380666, 0.9056948606905849, 0.023114512213053075, 0.021669855199737256, 0.039005739359527065, 0.0751221646924225, 0.008667942079894903, 0.4319524469814293, 0.007223285066579086, 0.39294670762190226, 0.02755624016086988, 0.009185413386956626, 0.7623893111173999, 0.11941037403043614, 0.08266872048260963, 0.9851615225497078, 0.06711252501140788, 0.08724628251483024, 0.05033439375855591, 0.018455944378137166, 0.03858970188155953, 0.2197935194123608, 0.04530095438270032, 0.47146548820514034, 0.9851111196378841, 0.01382142041843781, 0.6841603107126716, 0.2971605389964129, 0.821821727559582, 0.17121285990824625, 0.981983539883611, 0.9556173341837938, 0.9665300629388601, 0.5434915394680372, 0.4510979777584709, 0.019691742377736587, 0.06796827207799402, 0.11560958428219545, 0.18929481382469365, 0.07495566453461024, 0.2820365682488724, 0.02540869984224076, 0.0368426147712491, 0.16642698396667696, 0.02286782985801668, 0.9549480343915576, 0.8383206938101372, 0.14970012389466736, 0.9045479410750618, 0.904547439373995, 0.014891529372154712, 0.037228823430386776, 0.029783058744309424, 0.9158290563875148, 0.8007556307787754, 0.09075230482159455, 0.08541393394973605, 0.01601511261557551, 0.16587759270992766, 0.8319399265144064, 0.9423268419515292, 0.9343361244681223, 0.038930671852838425, 0.9809810042171491, 0.0730410759668053, 0.9251869622462006, 0.05823948247476913, 0.9318317195963061, 0.07131946291329579, 0.8914932864161973, 0.2306713764739793, 0.0482526858950671, 0.08905170486325392, 0.0019614912965474434, 0.6304233027103483, 0.9556167190253532, 0.0607019277972637, 0.6272532539050583, 0.3035096389863185, 0.9558719025068703, 0.9336725025859061, 0.9744764639137761, 0.9409847588178916, 0.022950847776046134, 0.9556172081059423, 0.9795262302992507, 0.3491300388275028, 0.014250205666428687, 0.08550123399857211, 0.039188065582678885, 0.5106323697136946, 0.922554103854514, 0.02250131960620766, 0.04500263921241532, 0.9836669852343655, 0.9465423480382923, 0.9659902901256007, 0.9791592050921936, 0.9916659964464295, 0.7184063334662539, 0.2575418931294118, 0.013554836480495358, 0.95254682200608, 0.08621436313654361, 0.8990926441382405, 0.05804158199301193, 0.8728560984333718, 0.06920342468397576, 0.9610509700933085, 0.015589026196007372, 0.10327729854854885, 0.4520817596842138, 0.03897256549001843, 0.17050497401883064, 0.15491594782282328, 0.014614712058756913, 0.008768827235254148, 0.04092119376451935, 0.10124333092522732, 0.4143913079730234, 0.4850261900138797, 0.9045482002955695, 0.9849729447085119, 0.03133478089232151, 0.656985906042341, 0.005222463482053585, 0.30603636004834006, 0.41114405610937693, 0.06596110348998861, 0.08821791755102772, 0.011735411050366074, 0.006879378891593905, 0.4070973626437334, 0.008498056277851294, 0.9819448009055735, 0.9192855659374692, 0.07230686847061267, 0.03944011007487964, 0.867682421647352, 0.01972005503743982, 0.9557254963567635, 0.00975230098323228, 0.029256902949696843, 0.13582414354674396, 0.8421096899898126, 0.17350102656063873, 0.5339876154249711, 0.00988865954490687, 0.06472577156666316, 0.21755050998795117, 0.9268513240639354, 0.988083308783777, 0.0810865941680791, 0.9159781933801528, 0.9915076451388266, 0.13069452582298471, 0.0733723653743072, 0.08712968388198981, 0.20406689119729193, 0.5009956823214414, 0.0022928864179471, 0.00114644320897355, 0.9796611854177847, 0.9778854140214601, 0.03235280072820943, 0.8897020200257593, 0.06470560145641886, 0.08104428031596209, 0.009004920035106898, 0.2656451410356535, 0.6438517825101432, 0.8748091456731376, 0.11410554073997448, 0.0019173763033872484, 0.0019173763033872484, 0.03451277346097047, 0.2530936720471168, 0.01150425782032349, 0.04601703128129396, 0.021091139337259732, 0.10353832038291141, 0.5234437308247188, 0.003407645696727954, 0.020445874180367724, 0.001703822848363977, 0.015334405635275793, 0.023853519877095677, 0.03407645696727954, 0.662787088013587, 0.001703822848363977, 0.2368313759225928, 0.005357167286390368, 0.04821450557751331, 0.061607423793489234, 0.09910759479822182, 0.7821464238129937, 0.9989577066914902, 0.09571389779889035, 0.14736901724591053, 0.025827559723510092, 0.1412919443697905, 0.03798170547575014, 0.18838925915972068, 0.07596341095150028, 0.1397726761507605, 0.03798170547575014, 0.1109065799891904, 0.13345456961808663, 0.13839733145579353, 0.0504161707446105, 0.017793942615744882, 0.067221560992814, 0.11664917936988313, 0.08699260834364166, 0.05239327547969327, 0.33610780496407, 0.9660878462418077, 0.9676736716379253, 0.3336798534464119, 0.02173393681451725, 0.007670801228653147, 0.15725142518738952, 0.12528975340133475, 0.12145435278700817, 0.006392334357210956, 0.04091093988615012, 0.18409922948767554, 0.9915832939242079, 0.9719363307576057, 0.9520886413739867, 0.008808946422853842, 0.9866019993596302, 0.009395288547612665, 0.002087841899469481, 0.3768554628542413, 0.06263525698408443, 0.49377460922453226, 0.017746656145490588, 0.03758115419045066, 0.9658980746978144, 0.07967943116805724, 0.04209479382463401, 0.05111510678705559, 0.26459584689769955, 0.5186679953392406, 0.009020312962421574, 0.034577866355949366, 0.98098100270524, 0.981311961976168, 0.06775483252368222, 0.17616256456157378, 0.7453031577605045, 0.9549479942584558, 0.9951378508875154, 0.7049215207941895, 0.07704956157517885, 0.11311531380185831, 0.060656037835779095, 0.042623161722439365, 0.9712089393294523, 0.04447662344540706, 0.48183008732524313, 0.00741277057423451, 0.04818300873252431, 0.4151151521571325, 0.36402505632975257, 0.07139867375264092, 0.3607796620682689, 0.03353574070199801, 0.001081798087161226, 0.10439351541105832, 0.06436698618609295, 0.030257213106029447, 0.12551140251389992, 0.25998790520736414, 0.15240670305259277, 0.007844462657118745, 0.4236009834844122, 0.028740101798573597, 0.028740101798573597, 0.08622030539572079, 0.017244061079144158, 0.8449589928780636, 0.9590958310725852, 0.03240188618488463, 0.052220572262113865, 0.052220572262113865, 0.12728764488890254, 0.010879285887940389, 0.0021758571775880777, 0.05548435802849598, 0.0924739300474933, 0.0533085008509079, 0.5515797945185776, 0.9549340962965007, 0.01511481971225925, 0.967348461584592, 0.9763683466447477, 0.9231895459375121, 0.03183412227370731, 0.5171657179818206, 0.04671174226932573, 0.16682765096187763, 0.01835104160580654, 0.05171657179818206, 0.08508210199055759, 0.006673106038475105, 0.10676969661560168, 0.10051671302705802, 0.2616188421252195, 0.11841694959352041, 0.06609318116847651, 0.033046590584238256, 0.16247907037250475, 0.015146354017775867, 0.002753882548686521, 0.23958778173572734, 0.6758012079419782, 0.049752236167507596, 0.27363729892129174, 0.025728544749903007, 0.8940669300591295, 0.03216068093737876, 0.03859281712485451, 0.7966151780868953, 0.014225271037265987, 0.17070325244719184, 0.014225271037265987, 0.9913104290280935, 0.955407930163414, 0.9919797904719738, 0.9761903346044967, 0.0853563140089483, 0.0011157688105744878, 0.16457589955973692, 0.3062785385026969, 0.027894220264362193, 0.16569166837031143, 0.0005578844052872439, 0.20585934555099297, 0.04295709920711777, 0.9583778699627675, 0.12365556385260525, 0.04224046809290097, 0.32327584048518565, 0.06710784043791525, 0.2360697128095191, 0.021120234046450484, 0.13489697874829665, 0.051778638307426995, 0.0405089801304571, 0.8169310992975516, 0.047260476818866616, 0.06751496688409517, 0.006751496688409517, 0.013502993376819034, 0.02007155046824773, 0.22859265811059915, 0.015611205919748234, 0.041258187073620334, 0.14719137010048336, 0.26204524222434533, 0.20294567695672705, 0.07136551277599193, 0.008920689096998991, 0.024996423903356935, 0.003124552987919617, 0.006249105975839234, 0.9654868732671616, 0.9915832944373539, 0.924881346127334, 0.9573298672525006, 0.24591996024705817, 0.07443554352305592, 0.21482663193995885, 0.14510219876646344, 0.11589331459918834, 0.20446218917092576, 0.3085847886150031, 0.06672103537621689, 0.6171695772300062, 0.9693088217266472, 0.029743334080763614, 0.10187091922661537, 0.004461500112114542, 0.0461021678251836, 0.07807625196200449, 0.08402491877815721, 0.10484525263469174, 0.2074597552133262, 0.09071716894632902, 0.2528183396864907, 0.5943488287158262, 0.25472092659249695, 0.004994527972401901, 0.004994527972401901, 0.024972639862009503, 0.09989055944803801, 0.014983583917205702, 0.9397324655071666, 0.9819447997550498, 0.9789461530999476, 0.04461679697709471, 0.817443458901771, 0.004780371104688719, 0.011154199244273678, 0.0031869140697924794, 0.0031869140697924794, 0.015934570348962396, 0.10038779319846311, 0.9664569876873457, 0.9809809997696538, 0.9186434584561378, 0.03280869494486206, 0.050343527431459076, 0.9421431562173056, 0.0018599864376085313, 0.41849694846191954, 0.27527799276606263, 0.13577900994542277, 0.009299932188042656, 0.037199728752170626, 0.06509952531629859, 0.05393960669064741, 0.0018599864376085313, 0.9539168621561434, 0.03376696857189888, 0.02064363156655509, 0.9599288678448117, 0.017203026305462575, 0.9525467051587969, 0.04034650554543677, 0.9279696275450456, 0.9811784790624487, 0.984173572845529, 0.895767148468053, 0.06890516526677332, 0.7830707764744348, 0.08511638874722117, 0.10213966649666541, 0.03404655549888847, 0.06816923719985322, 0.9202847021980185, 0.03389412790685717, 0.01412255329452382, 0.9518600920509054, 0.9795260888583988, 0.7666480028471229, 0.05197613578624562, 0.038982101839684216, 0.038982101839684216, 0.07796420367936843, 0.9624646444758442, 0.9939615527540969, 0.03993503464380896, 0.11557668849855299, 0.3641135511641405, 0.07893042141364594, 0.04087468251778093, 0.10383109007390329, 0.07329253416981409, 0.10946897731773514, 0.07423218204378607, 0.16608250685783293, 0.13051842574798053, 0.013489823869254363, 0.22757508059949894, 0.04590043965902134, 0.2048000532877708, 0.022775027311728147, 0.020322332062772806, 0.0746320125753553, 0.0940783820492155, 0.15606351669631063, 0.09123713283784314, 0.026410748979375644, 0.5234130252276264, 0.012004885899716202, 0.014405863079659444, 0.008803582993125215, 0.11364625318398006, 0.05442214941204678, 0.9488091723054566, 0.03405078842145886, 0.8041224650298362, 0.010477165668141188, 0.14668031935397663, 0.002619291417035297, 0.24336547320618904, 0.13813265593294718, 0.23356552126500021, 0.03943313995383121, 0.014233263533631382, 0.1728991521052599, 0.01469992791178323, 0.0011666609453796215, 0.13229935120604908, 0.010033284130264745, 0.9819447663592915, 0.009011835705500551, 0.9462427490775579, 0.036047342822002205, 0.007780178281588115, 0.007780178281588115, 0.49015123174005126, 0.01556035656317623, 0.47459087517687504, 0.022943346154339274, 0.7800737692475354, 0.18928260577329903, 0.0632645995241468, 0.349932316117937, 0.021747206086425462, 0.0790807494051835, 0.051402487113369276, 0.09983944612404418, 0.13344876462124716, 0.019770187351295876, 0.11960963347534005, 0.06227609015658201, 0.9549480293603012, 0.010138859409314145, 0.050694297046570726, 0.030416578227942435, 0.3751377981446234, 0.5221512595796785, 0.010138859409314145, 0.044963134824961254, 0.02195874026335317, 0.26245922886198314, 0.18194384789635484, 0.155802490439982, 0.00731958008778439, 0.010456542982549128, 0.04810009771972599, 0.26768750035325767, 0.011515640500420936, 0.7542744527775713, 0.16697678725610357, 0.04030474175147328, 0.023031281000841872, 0.966530966225786, 0.9433858916989813, 0.0038819231846448474, 0.0038819231846448474, 0.0038819231846448474, 0.01164576955393454, 0.007763846369289695, 0.03105538547715878, 0.7919123296675489, 0.1475130810165042, 0.881186162914772, 0.10167532649016602, 0.2569002978909258, 0.0060876847841451615, 0.06331192175510968, 0.6732979371264548, 0.9881180670860635, 0.9988676057143094, 0.5374492543421843, 0.026126005419411735, 0.4329452326645373, 0.055387661766112053, 0.06071339847439206, 0.05893815290496539, 0.28723473313323494, 0.0727850683464934, 0.2762282106027896, 0.03834530429961604, 0.050772023285602715, 0.09266781872407209, 0.00674593316382134, 0.0411723076291999, 0.07631939950778518, 0.004016810500409747, 0.09841185726003879, 0.05021013125512183, 0.08134041263329736, 0.36552975553728695, 0.031130281378175535, 0.0020084052502048733, 0.25105065627560913, 0.9801827790692821, 0.1835722447855357, 0.09884659334605768, 0.06039679972091304, 0.1879956723707293, 0.050699285399527005, 0.0961244840628616, 0.1820410583137379, 0.04117190290834072, 0.031304256756754925, 0.06788260024970226, 0.07335343288811916, 0.025965816951546605, 0.1635846467947436, 0.027264107799123936, 0.07140599661675316, 0.024667526103969276, 0.12982908475773303, 0.020772653561237284, 0.005842308814097986, 0.45829666919479756, 0.9192859162314657, 0.9457539567903458, 0.9911321022204033, 0.9732503222701147, 0.980394476029653, 0.012097039360805611, 0.056452850350426194, 0.11693804715445426, 0.810501637173976, 0.9799550228310547, 0.8857881488133902, 0.9654909355525483, 0.015594677436301647, 0.02339201615445247, 0.06237870974520659, 0.8888966138691938, 0.011696008077226235, 0.03329786780120786, 0.04043312518718097, 0.0035676286929865564, 0.08800150776033505, 0.09989360340362358, 0.12129937556154292, 0.005946047821644261, 0.14984040510543536, 0.45784568226660805, 0.06067244545455591, 0.052004953246762214, 0.03755913290043938, 0.1329015471861701, 0.7165126891776127, 0.08035231511702325, 0.2737000733673604, 0.0033480131298759684, 0.06277524618517441, 0.030969121451352707, 0.034317134581228675, 0.34568235565969374, 0.036828144428635655, 0.13308352191256975, 0.033546195194456986, 0.11741168318059944, 0.8386548798614246, 0.07313570158335825, 0.021232945620974976, 0.38219302117754955, 0.021232945620974976, 0.004718432360216661, 0.43409577713993286, 0.0613396206828166, 0.025252589599875773, 0.018037563999911264, 0.45093909999778164, 0.003607512799982253, 0.07215025599964506, 0.04329015359978704, 0.38600386959810107, 0.005720386687189945, 0.01144077337437989, 0.17447179395929333, 0.7951337495194023, 0.01144077337437989, 0.10678599439420247, 0.23526289389972732, 0.01001118697445648, 0.09510627625733657, 0.5422726277830594, 0.00500559348722824, 0.00500559348722824, 0.9788608639814952, 0.016863897713774542, 0.020831873646427378, 0.06447960890560855, 0.061503626956118926, 0.1993907906158049, 0.07836752466989347, 0.0029759819494896253, 0.55650862455456, 0.011790871559344905, 0.07369294724590565, 0.04716348623737962, 0.04716348623737962, 0.07074522935606943, 0.664710384158069, 0.02358174311868981, 0.04863734518229773, 0.011790871559344905, 0.034262728150812494, 0.10278818445243748, 0.8565682037703123, 0.08047176587024256, 0.006035382440268192, 0.6477977152554526, 0.09254253075077895, 0.09857791319104714, 0.07443638342997437, 0.905696874217176, 0.015087960591445149, 0.030175921182890298, 0.9354535566695992, 0.9658978786648811, 0.020607974117393305, 0.947966809400092, 0.9479468178691719, 0.02106548484153715, 0.9663208577664736, 0.029778705598042098, 0.9628448143366946, 0.8967382356245023, 0.9920341674482189, 0.9721534807388168, 0.02893356374856624, 0.9548076037026859, 0.9573305050302617, 0.04330579260403263, 0.9527274372887179, 0.31605058422914317, 0.011705577193671969, 0.023411154387343938, 0.6438067456519583, 0.12376561081720905, 0.04177089365080806, 0.05182684952970629, 0.30477281663737726, 0.05646805993535163, 0.17095124994127, 0.009282420811290678, 0.0030941402704302264, 0.081994717166401, 0.157027618724334, 0.8922165675270403, 0.1049666550031812, 0.0012713658875384184, 0.18816215135568592, 0.6064415283558255, 0.0012713658875384184, 0.16273483360491756, 0.04068370840122939, 0.07543111016434104, 0.9177451736661493, 0.09322809892460862, 0.9012049562712167, 0.007769008243717385, 0.005610070726580781, 0.9929825186047982, 0.9751458044900527, 0.9620898786495784, 0.6369926122364528, 0.019014704842879187, 0.08556617179295634, 0.256698515378869, 0.024676744450381274, 0.01645116296692085, 0.08225581483460424, 0.01645116296692085, 0.8554604742798841, 0.9826449181896213, 0.971638292017914, 0.9445732770104756, 0.08329560782567319, 0.1747842262571503, 0.0054620070705359465, 0.2362318058006797, 0.00819301060580392, 0.13791567853103265, 0.10924014141071894, 0.2266732934272418, 0.019117024746875814, 0.10418194583338143, 0.8334555666670515, 0.03472731527779381, 0.7131247676071387, 0.03111817167740242, 0.10891360087090846, 0.14262495352142773, 0.04509451742244412, 0.018037806968977648, 0.8928714449643936, 0.036075613937955296, 0.10995326906209706, 0.05428942659941042, 0.0006872079316381066, 0.40957592725631153, 0.022677861744057518, 0.14019041805417373, 0.06803358523217255, 0.03504760451354343, 0.1463752894389167, 0.013056950701124025, 0.8048651206063389, 0.04555840305318899, 0.075930671755315, 0.0683376045797835, 0.96539150890244, 0.969308835687711, 0.017767318917607265, 0.12437123242325085, 0.8350639891275414, 0.9965875120776961, 0.9615993221117797, 0.9397324672170567, 0.7170867826087108, 0.012805121118012694, 0.2561024223602539, 0.8856161130802448, 0.9755628394814868, 0.9625164633460264, 0.04524764983457261, 0.012340268136701622, 0.05018375708925327, 0.8917900440123039, 0.022444122701110198, 0.9650972761477385, 0.9972316910365231, 0.3652937107457318, 0.006280121101645819, 0.08896838227331577, 0.15072290643949965, 0.25853165201775286, 0.12978916943401358, 0.04291385514399441, 0.12648294147703615, 0.001129311977473537, 0.05985353480609746, 0.07792252644567405, 0.11744844565724785, 0.34556946510690234, 0.021456927571997204, 0.003387935932420611, 0.2044054679227102, 0.023860611886984125, 0.04772122377396825, 0.9279126844938271, 0.937508875136149, 0.9615178539444303, 0.9488304021323426, 0.9113361217765186, 0.05360800716332462, 0.007673147099496869, 0.10742405939295617, 0.15921780231456004, 0.013428007424119521, 0.034529161947735906, 0.5946689002110074, 0.028774301623113258, 0.009591433874371086, 0.04220230904723278, 0.9681490993683047, 0.11479803736961235, 0.20408539976819973, 0.01231549826187412, 0.20408539976819973, 0.05761893829662535, 0.13107208864423173, 0.07345315034760637, 0.043104243916559426, 0.04442376158747451, 0.11479803736961235, 0.018750604356827856, 0.9562808221982206, 0.05834599321290585, 0.001496051108023227, 0.474248201243363, 0.004488153324069681, 0.12118013974988139, 0.053857839888836176, 0.011968408864185816, 0.013464459972209044, 0.26031289279604153, 0.07436312735201264, 0.2005148612527484, 0.10357721309744619, 0.21512190412546514, 0.06241191045615347, 0.1912194703337468, 0.059756084479295875, 0.011951216895859176, 0.07834686631729904, 0.0026558259768575945, 0.9774130673717039, 0.476634498567813, 0.008362008746803736, 0.004181004373401868, 0.008362008746803736, 0.07525807872123362, 0.029267030613813076, 0.34702336299235503, 0.029267030613813076, 0.02508602624041121, 0.9885539345084765, 0.9776908650247259, 0.9583778730976427, 0.9885057104555761, 0.9657892241776689, 0.9558161614725398, 0.9747859145747744, 0.9364594565815458, 0.040715628547023736, 0.003592734702533974, 0.9269255532537652, 0.035927347025339736, 0.035927347025339736, 0.9863709254581988, 0.04547539500328201, 0.40978951452395695, 0.04598635449770091, 0.1481782533814807, 0.0012773987360472474, 0.002043837977675596, 0.02886921143466779, 0.31832776502297405, 0.012415941990189546, 0.028970531310442277, 0.062079709950947734, 0.8980864706237106, 0.9657892515146163, 0.30458832955926957, 0.017026676807661033, 0.0993222813780227, 0.5779610849711606, 0.9520886416977964, 0.9920341695596119, 0.9915871988662074, 0.005059118361562283, 0.06729568857811041, 0.9253157179490182, 0.04846559716934088, 0.717290838106245, 0.04846559716934088, 0.08723807490481358, 0.0775449554709454, 0.009693119433868176, 0.14439204970976427, 0.2791041854725052, 0.08711922552320972, 0.024199784867558256, 0.03226637982341101, 0.23877121069324145, 0.019359827894046604, 0.06856605712474839, 0.06614607863799257, 0.041139634274849034, 0.09063527119838356, 0.2719058135951507, 0.05981927899093315, 0.009063527119838356, 0.05619386814299781, 0.20483571290834685, 0.07432092238267453, 0.0036254108479353425, 0.0018127054239676713, 0.23021358884389426, 0.9833783755503394, 0.9437268354684958, 0.9667689687044015, 0.9483545594195794, 0.7967621364257106, 0.1938070061576053, 0.008909098848742449, 0.0014848498081237414, 0.7379703546374995, 0.008909098848742449, 0.24203051872416986, 0.9338673137656549, 0.0646227868362505, 0.9047190157075071, 0.03611283936887448, 0.7655921946201389, 0.19862061652880963, 0.8857880829692594, 0.9721742863467713, 0.7741933459094309, 0.040044483409108496, 0.08008896681821699, 0.106785289090956, 0.116592166997292, 0.748133071565957, 0.087444125247969, 0.038864055665764, 0.9804852804671417, 0.14296958191218043, 0.7267620413869171, 0.11914131826015036, 0.01745756858281422, 0.01745756858281422, 0.9427087034719678, 0.9310859283050525, 0.8868880949934329, 0.0492715608329685, 0.9194707020923106, 0.0018516810053999793, 0.007406724021599917, 0.0037033620107999586, 0.010184245529699886, 0.002777521508099969, 0.44347760079329507, 0.06017963267549933, 0.46940113486889473, 0.08565410565291738, 0.8836648566525976, 0.001427568427548623, 0.028551368550972463, 0.09863844967632424, 0.8877460470869182, 0.02528617351837074, 0.8597298996246052, 0.10114469407348296, 0.9554079510402461, 0.0048479374691214, 0.19028154566301494, 0.3118839771801434, 0.0500953538475878, 0.2125012590631547, 0.03110759876019565, 0.0533273121603354, 0.0670631349895127, 0.0791829786623162, 0.19317519190252502, 0.04064387350790732, 0.2971207304715983, 0.056994857103042446, 0.04111104446776832, 0.14458941207698064, 0.011212103036664088, 0.027563086631799218, 0.05255673298436291, 0.13524599287976055, 0.08770297391884135, 0.03189199051594231, 0.869056741559428, 0.8137498326071427, 0.10171872907589284, 0.021796870516262754, 0.06539061154878825, 0.9782085806899612, 0.01680770757199246, 0.11543436448361684, 0.019159230619687443, 0.07663692247874977, 0.04885603808020298, 0.04358724965978893, 0.419587150571155, 0.013890442199273396, 0.019638211385179627, 0.24332222887003052, 0.16228870158207676, 0.033389541473824405, 0.23916229706832365, 0.004659005787045266, 0.033389541473824405, 0.10871013503105621, 0.0015530019290150886, 0.0473665588349602, 0.15685319483052396, 0.21120826234605206, 0.9676757406282296, 0.9736608736565034, 0.028871115119847888, 0.10465779230944859, 0.003608889389980986, 0.03608889389980986, 0.7542578825060261, 0.005413334084971479, 0.06676445371464824, 0.9794703373613243, 0.7631806065061544, 0.013389133447476394, 0.006694566723738197, 0.2075315684358841, 0.006694566723738197, 0.03178248118871135, 0.8263445109064951, 0.015891240594355677, 0.03178248118871135, 0.08740182326895621, 0.9499801994334975, 0.0360875334174731, 0.07939257351844083, 0.12579083076947767, 0.2371466481719661, 0.5021322506945544, 0.01855930290041474, 0.21352761718074867, 0.520131375183875, 0.26280322114553684, 0.008212171251064125, 0.09033388376170538, 0.8869144951149256, 0.008212171251064125, 0.9837995794807598, 0.9797106188251336, 0.9192209477178547, 0.06413169402682707, 0.9981151715619136, 0.015596488704518189, 0.08830984820531244, 0.07482099310951293, 0.1953776355282211, 0.05795992423976354, 0.18589328428898705, 0.047421756196170166, 0.0569061074354042, 0.12645801652312044, 0.15111732974512893, 0.21494350906355936, 0.5110878993289077, 0.2006139417926554, 0.04776522423634652, 0.02388261211817326, 0.9928534417172147, 0.9667854937343356, 0.9816766589329988, 0.004441950679827278, 0.9905550016014829, 0.012483295952717787, 0.9736970843119873, 0.9506444590494247, 0.9697069997352794, 0.9667934804298718, 0.14526773543551996, 0.6658104540794665, 0.08473951233738665, 0.10895080157663997, 0.9837071978577223, 0.06949419285188131, 0.01882134389738452, 0.10568908496223615, 0.8049744005342918, 0.07555775638476178, 0.06393348617172151, 0.05812135106520137, 0.8020746446997788, 0.06070421892877507, 0.1345336743826907, 0.803920737164859, 0.9920526744157363, 0.013436085257281655, 0.02687217051456331, 0.0902137152988911, 0.08829427454785088, 0.3224660461747597, 0.45682689874757626, 0.03358831802638951, 0.007464070672531001, 0.09703291874290301, 0.8583681273410652, 0.9845149514731527, 0.9310859132603606, 0.9465425057548437, 0.9727719642661864, 0.06489752678863493, 0.04056095424289684, 0.8680044207979923, 0.016224381697158733, 0.2684695573830945, 0.04592242428921354, 0.683537623074063, 0.9657396993053828, 0.9892695638729657, 0.9976650750721116, 0.9045474785571765, 0.08474466541893354, 0.6580174020764251, 0.2492490159380398, 0.0066662030225073145, 0.07523286268258256, 0.8418462102709238, 0.027617126807530305, 0.002856944152503135, 0.04571110644005016, 0.9836257632838991, 0.1288128921531774, 0.8050805759573587, 0.0644064460765887, 0.965227809529019, 0.9868947414174785, 0.009581502343859015, 0.004790751171929507, 0.0763910457821805, 0.019097761445545126, 0.7193490144488663, 0.152782091564361, 0.0254636819273935, 0.9779693251297784, 0.7031727687424505, 0.04678072473962249, 0.1089113747844336, 0.015349925305188628, 0.0007309488240566014, 0.0007309488240566014, 0.09429239830330158, 0.029237952962264054, 0.7052053857992364, 0.0050014566368740175, 0.0016671522122913391, 0.28675018051411033, 0.10909352045563435, 0.0014742367629139777, 0.49829202586492444, 0.01916507791788171, 0.03685591907284944, 0.029484735258279552, 0.19017654241590312, 0.11499046750729026, 0.024116619584642254, 0.024116619584642254, 0.040194365974403755, 0.9003537978266442, 0.9310858767935151, 0.9750913559528714, 0.9824670257032887, 0.9911221078348164, 0.9931226169643358, 0.9488091203401896, 0.9667853908346241, 0.01214726397934106, 0.04251542392769371, 0.85941892653838, 0.08503084785538742, 0.08382687153254362, 0.06489822312196925, 0.005408185260164104, 0.05813799154676412, 0.04056138945123078, 0.11492393677848721, 0.04596957471139489, 0.20551103988623595, 0.050025713656517964, 0.32989930087001035, 0.990726619515529, 0.012514550176639877, 0.03597933175783965, 0.07665161983191925, 0.012514550176639877, 0.0703943447435993, 0.7915452986724723, 0.9661395904703701, 0.9857953751791095, 0.2504440112877414, 0.1431108635929951, 0.0035777715898248777, 0.0787109749761473, 0.0011925905299416258, 0.46868807826705894, 0.05485916437731479, 0.20823366011856378, 0.09768986524080771, 0.6941122003952126, 0.9056948452569576, 0.20525477930981748, 0.07342447389944691, 0.08315877915126751, 0.23529206408686393, 0.04477780415837482, 0.047002788215933816, 0.008899936230235989, 0.19579859706519173, 0.10624298874844211, 0.10376931553135776, 0.19356968474118658, 0.019955637602184183, 0.19955637602184184, 0.05188465776567888, 0.01197338256131051, 0.017960073841965764, 0.16563179209812873, 0.16563179209812873, 0.06984473160764465, 0.9558693652474451, 0.9614608592698277, 0.012699695564413478, 0.4317896491900583, 0.004233231854804493, 0.37675763507759985, 0.1735625060469842, 0.9764081168241852, 0.14202195986389993, 0.3195494096937749, 0.5325823494896248, 0.033992660102703964, 0.0977288977952739, 0.5693770567202914, 0.0042490825128379955, 0.050988990154055946, 0.24219770323176576, 0.9661807521637747, 0.021958653458267606, 0.9837995796646379, 0.9819827084146775, 0.21894770928403984, 0.07653759464943513, 0.007528287998305095, 0.16060347729717536, 0.03011315199322038, 0.14052804263502844, 0.14554690130056516, 0.028231079993644104, 0.03513201065875711, 0.1574666906312149, 0.9666107813740918, 0.0776418941586669, 0.1515865552621592, 0.018486165275873073, 0.7542355432556214, 0.09551207315282309, 0.08582728251844592, 0.07714436677728019, 0.21473518544498338, 0.05577103572210299, 0.10219123910756596, 0.06612374295195445, 0.04575228678998868, 0.060446451890423004, 0.19703539566491476, 0.08327733365300231, 0.888291558965358, 0.057318579843800975, 0.03821238656253398, 0.01910619328126699, 0.011463715968760195, 0.03439114790628058, 0.748962776625666, 0.06878229581256116, 0.026748670593773786, 0.8021238172742851, 0.027659441974975347, 0.15904179135610824, 0.8059455623115485, 0.04914302209216759, 0.009828604418433518, 0.04914302209216759, 0.05897162651060111, 0.019657208836867036, 0.9583718368947677, 0.905806249414161, 0.017419350950272326, 0.05225805285081698, 0.9749408309675225, 0.05495097697881634, 0.04121323273411225, 0.0117752093526035, 0.03140055827360934, 0.13345237266283969, 0.029438023381508752, 0.6044607467669797, 0.09027660503662684, 0.23370938522404422, 0.7629133954440064, 0.9554079358678131, 0.04796355421116432, 0.0844158554116492, 0.4566130360902843, 0.030696674695145164, 0.10935690360145464, 0.036452301200484885, 0.15156483130727924, 0.0844158554116492, 0.8967381754521472, 0.026638669008048708, 0.008879556336016236, 0.004439778168008118, 0.9589920842897535, 0.020989440328247923, 0.020989440328247923, 0.8185881728016691, 0.01399296021883195, 0.11894016186007157, 0.02038651426180612, 0.11824178271847549, 0.02446381711416734, 0.04485033137597346, 0.43627140520265095, 0.06115954278541836, 0.27725659396056324, 0.02038651426180612, 0.4060250163314155, 0.008317085995615588, 0.5852204146005877, 0.9992406380188332, 0.9656699389794563, 0.9754305991766237, 0.10355201058565162, 0.5242320535898614, 0.03236000330801613, 0.3365440344033678, 0.885617088308964, 0.9785609367658551, 0.19830115052876773, 0.09885460339792303, 0.0668896418201515, 0.03610856770822338, 0.0065113810621386424, 0.5931276203875382, 0.25771509007596793, 0.001712392625089488, 0.1969251518852911, 0.2217548449490887, 0.0214049078136186, 0.05993374187813208, 0.000856196312544744, 0.043666011939781944, 0.17209545882149355, 0.024829693063797575, 0.8967380271234541, 0.9248813800386029, 0.0949954403314753, 0.0949954403314753, 0.02235186831328831, 0.027939835391610386, 0.01676390123496623, 0.030733818930771425, 0.7096718189469038, 0.9657255000993953, 0.9756329869343636, 0.9713307068742095, 0.9676757327680395, 0.9856923398784437, 0.885617251479093, 0.07669004929621223, 0.8819355669064407, 0.9718107257170575, 0.026088878542739797, 0.013152341700896318, 0.0657617085044816, 0.9206639190627423, 0.10480520947962688, 0.017467534913271146, 0.8733767456635573, 0.9702755401993524, 0.7799222741230903, 0.017331606091624232, 0.19931347005367867, 0.2732210884469979, 0.02597839857364898, 0.12093392439457283, 0.08958068473672062, 0.006270647931570443, 0.08062261626304856, 0.19170266533658212, 0.030457432810485012, 0.0877890710419862, 0.09316391212618945, 0.9490234318904536, 0.008953051244249562, 0.035812204976998246, 0.040722067565393934, 0.8483764076123735, 0.10180516891348483, 0.9746700256675493, 0.9397324114693651, 0.9763971464671904, 0.017079447913294285, 0.13094243400192287, 0.8425860970558514, 0.9167571506640985, 0.973250327865092, 0.13395412775815224, 0.8305155921005439, 0.03562049735350347, 0.8263955386012805, 0.1353578899433132, 0.8346856526770716, 0.07464668438575438, 0.08482577771108452, 0.9653046478947617, 0.9892701554444358, 0.9661014971504986, 0.27835543718568806, 0.11068464234549015, 0.019178032089565124, 0.1271229555651174, 0.007123269061838474, 0.06520530910452142, 0.048219052110906595, 0.23013638507478149, 0.11452024876340317, 0.9549480455140549, 0.018260162895976845, 0.03309654524895803, 0.28303252488764113, 0.3503668755665557, 0.021683943438972505, 0.01597764253397974, 0.0034237805429956584, 0.006847561085991317, 0.04793292760193922, 0.21912195475172214, 0.9506572772398689, 0.9248813233203271, 0.9192862845428762, 0.11678385167445225, 0.010155117536908892, 0.8530298731003468, 0.015232676305363338, 0.0062085739604076625, 0.01862572188122299, 0.13658862712896858, 0.031042869802038315, 0.645691691882397, 0.16142292297059924, 0.9849713296571843, 0.9798131149095527, 0.12451128243971217, 0.02394447739225234, 0.8332678132503815, 0.014366686435351404, 0.7987453751041038, 0.19618307458697284, 0.986969778297621, 0.9885552724311579, 0.07330463262604657, 0.0761240415732022, 0.018326158156511643, 0.14097044735778186, 0.08881138183540258, 0.4186822286526121, 0.007048522367889093, 0.0380620207866011, 0.06766581473173529, 0.07048522367889093, 0.1584653506990406, 0.6817695320772678, 0.11055722141793531, 0.003685240713931177, 0.040537647853242945, 0.002005568923018035, 0.13838425568824442, 0.00802227569207214, 0.12434527322711816, 0.17448449630256904, 0.5515314538299596, 0.9708655810124903, 0.383461169738628, 0.029227223303249086, 0.11223253748447648, 0.04910173514945846, 0.023381778642599268, 0.22212689710469305, 0.0701453359277978, 0.026889045438989158, 0.08417440311335736, 0.06160969517275239, 0.35792299100360914, 0.28164432078972523, 0.017602770049357827, 0.11441800532082587, 0.011735180032905218, 0.04107313011516826, 0.11441800532082587, 0.8967382747687559, 0.9457540846125251, 0.9572393824723877, 0.0060449850218158895, 0.05440486519634301, 0.03626991013089534, 0.8281629479887769, 0.01813495506544767, 0.048359880174527116, 0.9723070310761085, 0.9407459286321638, 0.14577770971500953, 0.02204540296780117, 0.4361414857413637, 0.07169721145383984, 0.006951253188045414, 0.13664177695357843, 0.18033536842129247, 0.5622217549778691, 0.0365079061673941, 0.3139679930395892, 0.08396818418500643, 0.8624977002778468, 0.12578091462385266, 0.7953834116162719, 0.07134164188161218, 0.03642977457784451, 0.09562815826684186, 0.9965776131403843, 0.0009518410822735285, 0.0009518410822735285, 0.9549480193529013, 0.447328104400898, 0.059205190288354144, 0.01096392412747299, 0.12937430470418126, 0.0021927848254945977, 0.06578354476483794, 0.0043855696509891955, 0.2762908880123193, 0.0043855696509891955, 0.007742300866301747, 0.023226902598905242, 0.1161345129945262, 0.3638881407161821, 0.48776495457701, 0.9663236253985966, 0.9659883791419304, 0.03732201195288424, 0.21460156872908437, 0.475855652399274, 0.27058458665841073, 0.10567830183119153, 0.04822902365450352, 0.20355484983591926, 0.08723779278682253, 0.2631318790561883, 0.12979281365844328, 0.007092503478603458, 0.019859009740089684, 0.1361760667891864, 0.03357841028735919, 0.6241633912238531, 0.013826404235971429, 0.10863603328263266, 0.1362888417545755, 0.04147921270791429, 0.03357841028735919, 0.007900802420555103, 0.9445735936533756, 0.1633334949856988, 0.016217510140423993, 0.18302618587049935, 0.028959839536471417, 0.022009478047718276, 0.3579436166707867, 0.01042554223312971, 0.01390072297750628, 0.20387727033675876, 0.4378011447242436, 0.14801848226391093, 0.1157045882485501, 0.2981217318836516, 0.9658978556626661, 0.07831309867070406, 0.2393721129180011, 0.03398492961181497, 0.03546253524711127, 0.08274591557659297, 0.1551485917061118, 0.16992464805907484, 0.02511929580003715, 0.041372957788296486, 0.13593971844725988, 0.9935640278737159, 0.9506572146253918, 0.962100246527347, 0.9666108081212423, 0.03702484989262536, 0.9034063373800587, 0.0518347898496755, 0.2930487581341804, 0.6808203471804192, 0.02368070772801458, 0.9716859207967378, 0.9829873105372033, 0.06844401357686199, 0.6673291323744043, 0.25666505091323244, 0.8099965294197067, 0.18274311944225088, 0.004939003228168943, 0.9764375118870816, 0.9506572807108664, 0.9151774960787274, 0.07198025250057406, 0.988555285470283, 0.2883272431142106, 0.05155520241115024, 0.6587609196980309, 0.9669849073170792, 0.9308727373201925, 0.04047272770957359, 0.021910504879296893, 0.9531069622494148, 0.019719454391367203, 0.004382100975859378, 0.03485307957039646, 0.0014522116487665191, 0.03485307957039646, 0.6651129351350658, 0.03485307957039646, 0.22799722885634352, 0.18719646829178443, 0.023399558536473054, 0.7086723442474696, 0.010028382229917024, 0.07019867560941916, 0.9661981632201448, 0.9002758226269872, 0.022229032657456474, 0.04445806531491295, 0.022229032657456474, 0.07869682715064288, 0.008585108416433769, 0.11518353792048641, 0.003577128506847404, 0.49364373394494176, 0.030763305158887675, 0.020747345339714944, 0.24825271837520982, 0.9637895137324552, 0.9667853865910985, 0.031160759852223115, 0.9659835554189166, 0.7462646323806114, 0.1973592416213187, 0.043172334104663465, 0.8402816880921687, 0.15125070385659037, 0.8857881582880078, 0.11362546404536211, 0.5278849683774115, 0.023671971676117107, 0.023671971676117107, 0.021304774508505396, 0.2438213082640062, 0.0449767461846225, 0.09023580464346095, 0.0047182120075012265, 0.11205753517815413, 0.6894487295961167, 0.02830927204500736, 0.01592396552531664, 0.03715591955907216, 0.021821730534693173, 0.06693551498709657, 0.055294555858905865, 0.05965991553197739, 0.13387102997419315, 0.02037167847433374, 0.007275599455119193, 0.04801895640378667, 0.017461438692286065, 0.45399740599943766, 0.13678126975624083, 0.114314921534277, 0.024267972169287853, 0.21585722613734987, 0.02778044182536899, 0.046620051798895085, 0.040393401044933074, 0.3788677497218426, 0.028898045806849352, 0.011335697526443669, 0.11176039814803616, 0.019362974983934748, 0.053248181205820554, 0.14360873113084938, 0.001613581248661229, 0.5841164120153649, 0.025817299978579664, 0.17103961235809026, 0.9488092037602639, 0.9764081385421477, 0.27370274259467803, 0.28399231938395164, 0.014405407504983054, 0.10906951396630025, 0.004115830715709444, 0.2922239808153705, 0.022637068936401942, 0.03111951080267554, 0.9647048348829418, 0.9681490629798088, 0.10805271848021755, 0.03601757282673919, 0.0024011715217826125, 0.6651245115337836, 0.046822844674760944, 0.14166911978517413, 0.009960937628243028, 0.03718750047877397, 0.08101562604304328, 0.08964843865418724, 0.003984375051297211, 0.24636719067187754, 0.007304687594044887, 0.010625000136792562, 0.5139843816173402, 0.9900773939773223, 0.17403289789859358, 0.15602949466770458, 0.6571242179274481, 0.01200226882059266, 0.03029817612662994, 0.749879859134091, 0.1514908806331497, 0.06059635225325988, 0.9465425608120548, 0.07978248774298424, 0.044323604301657916, 0.12410609204464217, 0.6471246228042056, 0.10046683641709128, 0.9591901151228972], \"Term\": [\"000\", \"000\", \"000\", \"000\", \"2008\", \"2008\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2017\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2018\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2019\", \"2020\", \"2020\", \"2020\", \"2020\", \"2020\", \"2020\", \"2020\", \"2020\", \"2020\", \"2021\", \"2021\", \"2021\", \"2021\", \"2021\", \"2021\", \"2021\", \"2021\", \"2021\", \"2021\", \"2022\", \"2022\", \"2022\", \"2022\", \"2022\", \"2022\", \"2022\", \"2022\", \"2035\", \"2038\", \"2038\", \"2040\", \"2040\", \"2040\", \"2042\", \"2045\", \"2045\", \"27\", \"27\", \"27\", \"500\", \"500\", \"7g\", \"89\", \"89\", \"90\", \"90\", \"90\", \"90\", \"90\", \"91\", \"91\", \"91\", \"92\", \"92\", \"92\", \"92\", \"93\", \"93\", \"93\", \"93\", \"93\", \"95\", \"95\", \"95\", \"95\", \"96\", \"96\", \"96\", \"96\", \"97\", \"97\", \"98\", \"98\", \"accents\", \"accents\", \"accents\", \"accents\", \"accents\", \"accents\", \"accents\", \"achieved\", \"acidity\", \"acidity\", \"acidity\", \"acidity\", \"acidity\", \"acidity\", \"acidity\", \"acidity\", \"acidity\", \"acids\", \"acids\", \"acids\", \"acids\", \"acids\", \"acres\", \"active\", \"age\", \"age\", \"age\", \"age\", \"age\", \"age\", \"age\", \"age\", \"aged\", \"aged\", \"aged\", \"aged\", \"aged\", \"aged\", \"aged\", \"aged\", \"ages\", \"ages\", \"aglianico\", \"alba\", \"alba\", \"alba\", \"alcohol\", \"alcohol\", \"alcohol\", \"alcohol\", \"alder\", \"align\", \"almond\", \"almond\", \"almond\", \"almond\", \"almonds\", \"almonds\", \"alongside\", \"alongside\", \"alongside\", \"alongside\", \"alongside\", \"alongside\", \"altitude\", \"american\", \"american\", \"anise\", \"anise\", \"anise\", \"anise\", \"anise\", \"appealingly\", \"appetizers\", \"apple\", \"apple\", \"apple\", \"apple\", \"apple\", \"apples\", \"apples\", \"apples\", \"apricot\", \"apricot\", \"apricot\", \"apricot\", \"apricot\", \"apricots\", \"aromas\", \"aromas\", \"aromas\", \"aromas\", \"aromas\", \"aromas\", \"aromas\", \"aromas\", \"aromas\", \"aromas\", \"attractively\", \"attractively\", \"attractively\", \"august\", \"august\", \"august\", \"auslese\", \"austere\", \"austere\", \"austere\", \"austere\", \"austere\", \"avize\", \"avocado\", \"avola\", \"a\\u00e7a\\u00ed\", \"a\\u00ff\", \"bages\", \"balsam\", \"bark\", \"bark\", \"barolo\", \"barolo\", \"barolo\", \"barolo\", \"barrel\", \"barrel\", \"barrel\", \"barrel\", \"barrels\", \"barrels\", \"based\", \"based\", \"based\", \"beach\", \"beam\", \"beam\", \"beaucaillou\", \"beefy\", \"beefy\", \"beeswax\", \"beeswax\", \"berardenga\", \"berries\", \"berries\", \"berries\", \"berries\", \"berries\", \"berries\", \"berries\", \"berries\", \"berry\", \"berry\", \"berry\", \"berry\", \"berry\", \"berry\", \"berry\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"best\", \"bilberry\", \"biodynamic\", \"biodynamically\", \"biodynamically\", \"bits\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"black\", \"blackberries\", \"blackberries\", \"blackberries\", \"blackberry\", \"blackberry\", \"blackberry\", \"blackberry\", \"blackberry\", \"blackberry\", \"blackcurrants\", \"blackcurrants\", \"blackcurrants\", \"blanc\", \"blanc\", \"blanc\", \"blanc\", \"blancs\", \"blancs\", \"blend\", \"blend\", \"blend\", \"blend\", \"blend\", \"blend\", \"blend\", \"blend\", \"blend\", \"blood\", \"blood\", \"blood\", \"blossom\", \"blossom\", \"blossom\", \"blossom\", \"blossom\", \"blueberries\", \"blueberries\", \"blueberries\", \"bodied\", \"bodied\", \"bodied\", \"bodied\", \"bodied\", \"bodied\", \"bodied\", \"bodied\", \"bodied\", \"body\", \"body\", \"body\", \"body\", \"body\", \"body\", \"bois\", \"bois\", \"botanical\", \"bottle\", \"bottle\", \"bottle\", \"bottle\", \"bottle\", \"bottle\", \"bottle\", \"bottle\", \"bottled\", \"bottled\", \"bottled\", \"bottled\", \"bottled\", \"bottlings\", \"bouquet\", \"bouquet\", \"bouquet\", \"bouquet\", \"bouquet\", \"bouquet\", \"bouquet\", \"bouquet\", \"bourboulenc\", \"box\", \"box\", \"box\", \"boysenberry\", \"boysenberry\", \"braised\", \"brambleberries\", \"brawny\", \"bread\", \"bread\", \"bright\", \"bright\", \"bright\", \"bright\", \"bright\", \"bright\", \"bright\", \"bright\", \"bright\", \"bright\", \"brim\", \"brimming\", \"brimming\", \"brisson\", \"broody\", \"brought\", \"brought\", \"brought\", \"brought\", \"brunello\", \"brunello\", \"brunello\", \"brunello\", \"brut\", \"brut\", \"br\\u00fbl\\u00e9e\", \"burly\", \"burly\", \"burning\", \"butter\", \"butter\", \"butterscotch\", \"butterscotch\", \"cab\", \"cab\", \"cabernet\", \"cabernet\", \"cabernet\", \"cabernet\", \"cabernet\", \"cacao\", \"camphor\", \"camphor\", \"camphor\", \"canaiolo\", \"cannubi\", \"canon\", \"caramelized\", \"caramelized\", \"carm\\u00e9n\\u00e8re\", \"casks\", \"cassis\", \"cassis\", \"cassis\", \"cassis\", \"cassis\", \"cast\", \"cast\", \"cast\", \"cent\", \"cerequio\", \"certain\", \"certification\", \"certified\", \"chablis\", \"chablis\", \"chablis\", \"chai\", \"chamomile\", \"chamomile\", \"champagne\", \"champagne\", \"champagne\", \"champagnes\", \"character\", \"character\", \"character\", \"character\", \"character\", \"character\", \"character\", \"character\", \"character\", \"chardonnay\", \"chardonnay\", \"chardonnay\", \"charges\", \"checks\", \"cherries\", \"cherries\", \"cherries\", \"cherries\", \"cherry\", \"cherry\", \"cherry\", \"cherry\", \"cherry\", \"cherry\", \"cherry\", \"chest\", \"cheval\", \"chewy\", \"chewy\", \"chewy\", \"chewy\", \"chicken\", \"chicken\", \"chicken\", \"chili\", \"chili\", \"chocolate\", \"chocolate\", \"chocolate\", \"chocolate\", \"chocolate\", \"chocolatey\", \"chops\", \"ch\\u00e2teau\", \"ch\\u00e2teau\", \"ch\\u00e2teauneuf\", \"citrus\", \"citrus\", \"citrus\", \"citrus\", \"citrus\", \"citrus\", \"citrus\", \"clairette\", \"clams\", \"clay\", \"clay\", \"clay\", \"clear\", \"clear\", \"clear\", \"clear\", \"cloves\", \"cloves\", \"color\", \"color\", \"color\", \"color\", \"color\", \"color\", \"color\", \"color\", \"color\", \"comes\", \"comes\", \"comes\", \"comes\", \"comes\", \"comes\", \"comes\", \"comes\", \"comes\", \"coming\", \"coming\", \"coming\", \"coming\", \"coming\", \"commentary\", \"complex\", \"complex\", \"complex\", \"complex\", \"complex\", \"complex\", \"complex\", \"complex\", \"complex\", \"complex\", \"concentrated\", \"concentrated\", \"concentrated\", \"concentrated\", \"concentrated\", \"concentrated\", \"concentrated\", \"concentrated\", \"concentrated\", \"coolish\", \"copper\", \"core\", \"core\", \"core\", \"core\", \"core\", \"core\", \"core\", \"core\", \"core\", \"cos\", \"crab\", \"cramant\", \"cranberry\", \"cranberry\", \"creamy\", \"creamy\", \"creamy\", \"creamy\", \"creamy\", \"creamy\", \"creamy\", \"creek\", \"crisp\", \"crisp\", \"crisp\", \"crisp\", \"crisp\", \"crisp\", \"crisp\", \"crozes\", \"crust\", \"crystalline\", \"crystalline\", \"crystalline\", \"culminate\", \"curd\", \"currant\", \"currant\", \"currant\", \"currant\", \"currant\", \"current\", \"cuv\\u00e9e\", \"cuv\\u00e9e\", \"cuv\\u00e9e\", \"cuv\\u00e9e\", \"cuv\\u00e9e\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"dark\", \"de\", \"de\", \"de\", \"de\", \"de\", \"de\", \"decades\", \"decades\", \"decades\", \"decades\", \"decades\", \"december\", \"december\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deep\", \"deepen\", \"define\", \"define\", \"delineation\", \"demeter\", \"demeter\", \"dense\", \"dense\", \"dense\", \"dense\", \"dense\", \"dense\", \"dense\", \"dense\", \"depth\", \"depth\", \"depth\", \"depth\", \"depth\", \"depth\", \"depth\", \"depth\", \"depth\", \"di\", \"di\", \"di\", \"different\", \"different\", \"different\", \"different\", \"discreet\", \"discreet\", \"discreet\", \"discreet\", \"disgorged\", \"disgorgement\", \"dosage\", \"douro\", \"dried\", \"dried\", \"dried\", \"dried\", \"dried\", \"dried\", \"dried\", \"dried\", \"dried\", \"drier\", \"drink\", \"drink\", \"drink\", \"drink\", \"drink\", \"drink\", \"drink\", \"drink\", \"drinkable\", \"drinkable\", \"drinkable\", \"drinkable\", \"drinkable\", \"drinkable\", \"dry\", \"dry\", \"dry\", \"dry\", \"dry\", \"dry\", \"dry\", \"dry\", \"dry\", \"du\", \"du\", \"du\", \"du\", \"ducru\", \"dusky\", \"earl\", \"earth\", \"earth\", \"earth\", \"earth\", \"earth\", \"earth\", \"editors\", \"editors\", \"editors\", \"elderflower\", \"elegant\", \"elegant\", \"elegant\", \"elegant\", \"elegant\", \"elegant\", \"elegant\", \"elegant\", \"elegant\", \"elegant\", \"elements\", \"elements\", \"elements\", \"elements\", \"elements\", \"elements\", \"elements\", \"embedded\", \"embers\", \"emilion\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"enjoy\", \"enormously\", \"estournel\", \"evokes\", \"evokes\", \"evolve\", \"evolve\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excellent\", \"excels\", \"excels\", \"exhibits\", \"exhibits\", \"exhibits\", \"extends\", \"extreme\", \"extreme\", \"exude\", \"facing\", \"fantastically\", \"fantastically\", \"february\", \"february\", \"february\", \"february\", \"fermentation\", \"fermentation\", \"fermented\", \"fermented\", \"fermented\", \"field\", \"fill\", \"fill\", \"fill\", \"fill\", \"fill\", \"fils\", \"finale\", \"fine\", \"fine\", \"fine\", \"fine\", \"fine\", \"fine\", \"fine\", \"fine\", \"fine\", \"finish\", \"finish\", \"finish\", \"finish\", \"finish\", \"finish\", \"finish\", \"finish\", \"finish\", \"finish\", \"firm\", \"firm\", \"firm\", \"firm\", \"firm\", \"firm\", \"firm\", \"firm\", \"firm\", \"firming\", \"flavorful\", \"flavorful\", \"flavorful\", \"flavorful\", \"flavorful\", \"flavors\", \"flavors\", \"flavors\", \"flavors\", \"flavors\", \"flavors\", \"flavors\", \"flavors\", \"flavors\", \"flavors\", \"flawless\", \"flint\", \"flint\", \"flint\", \"flinty\", \"flinty\", \"flinty\", \"flinty\", \"flinty\", \"floor\", \"floor\", \"floor\", \"floral\", \"floral\", \"floral\", \"floral\", \"floral\", \"floral\", \"floral\", \"floral\", \"floral\", \"floral\", \"florality\", \"flower\", \"flower\", \"flower\", \"flower\", \"flower\", \"flower\", \"flowers\", \"flowers\", \"flowers\", \"flowers\", \"flowers\", \"flowers\", \"flowers\", \"flowers\", \"flowers\", \"follow\", \"follow\", \"follow\", \"follow\", \"follow\", \"fontalloro\", \"forefront\", \"forest\", \"forest\", \"forest\", \"forest\", \"forest\", \"forest\", \"forest\", \"forest\", \"frames\", \"frames\", \"franc\", \"franc\", \"franc\", \"franc\", \"franca\", \"francisco\", \"french\", \"french\", \"french\", \"fresh\", \"fresh\", \"fresh\", \"fresh\", \"fresh\", \"fresh\", \"fresh\", \"fresh\", \"fresh\", \"fresh\", \"freshness\", \"freshness\", \"freshness\", \"freshness\", \"freshness\", \"freshness\", \"freshness\", \"freshness\", \"freshness\", \"freshness\", \"fried\", \"fruit\", \"fruit\", \"fruit\", \"fruit\", \"fruit\", \"fruit\", \"fruit\", \"fruit\", \"fruit\", \"fruit\", \"fruits\", \"fruits\", \"fruits\", \"fruits\", \"fruits\", \"fruits\", \"fruits\", \"fruits\", \"fruits\", \"fruits\", \"fudge\", \"f\\u00e8vre\", \"gallonibarrel\", \"ganache\", \"garlic\", \"garnet\", \"garnet\", \"garnet\", \"garnet\", \"garni\", \"gather\", \"gg\", \"ginger\", \"ginger\", \"ginger\", \"ginger\", \"ginger\", \"glass\", \"glass\", \"glass\", \"glass\", \"glass\", \"glass\", \"glass\", \"glass\", \"glass\", \"going\", \"going\", \"going\", \"going\", \"going\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"good\", \"graciano\", \"graciano\", \"graciano\", \"grained\", \"grained\", \"grained\", \"grained\", \"grained\", \"grained\", \"grained\", \"grand\", \"grand\", \"grand\", \"grand\", \"grand\", \"grand\", \"grand\", \"grapefruit\", \"grapefruit\", \"grapefruit\", \"grapefruit\", \"grapefruit\", \"grapes\", \"grapes\", \"grapes\", \"grapes\", \"grapes\", \"grapes\", \"grapes\", \"gravelly\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"great\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"green\", \"grey\", \"grey\", \"grey\", \"grilled\", \"grilled\", \"grilled\", \"grilled\", \"grilled\", \"grilled\", \"grimaldi\", \"growing\", \"growing\", \"growing\", \"gr\\u00fcner\", \"guava\", \"guava\", \"ha\", \"ha\", \"handrolls\", \"haut\", \"haut\", \"heavier\", \"hectares\", \"helped\", \"hermitage\", \"hermitage\", \"hibiscus\", \"highlighted\", \"highlighted\", \"highly\", \"highly\", \"highly\", \"highly\", \"hints\", \"hints\", \"hints\", \"hints\", \"hints\", \"hints\", \"hints\", \"hints\", \"hints\", \"hints\", \"hip\", \"hip\", \"hold\", \"hold\", \"hold\", \"hold\", \"hold\", \"hold\", \"honey\", \"honey\", \"honeyed\", \"honeyed\", \"honeyed\", \"honeysuckle\", \"honeysuckle\", \"imported\", \"imposing\", \"impressively\", \"impressively\", \"impressively\", \"impressively\", \"incredible\", \"incredible\", \"incredible\", \"incredible\", \"incredible\", \"indigenous\", \"inflected\", \"inlaid\", \"intense\", \"intense\", \"intense\", \"intense\", \"intense\", \"intense\", \"intense\", \"intense\", \"intense\", \"intertwined\", \"intertwined\", \"intertwined\", \"iron\", \"iron\", \"iron\", \"iron\", \"jasmine\", \"jasmine\", \"jasmine\", \"jasmine\", \"juicy\", \"juicy\", \"juicy\", \"juicy\", \"juicy\", \"juicy\", \"juicy\", \"juicy\", \"juicy\", \"juicy\", \"june\", \"june\", \"june\", \"june\", \"kabinett\", \"kiwi\", \"lacy\", \"lacy\", \"lacy\", \"lamb\", \"lanolin\", \"latent\", \"latest\", \"latest\", \"latest\", \"laurel\", \"laurent\", \"leg\", \"lemon\", \"lemon\", \"lemon\", \"lemon\", \"lemongrass\", \"lemongrass\", \"lemons\", \"licorice\", \"licorice\", \"licorice\", \"licorice\", \"licorice\", \"licorice\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"like\", \"lime\", \"lime\", \"lime\", \"limey\", \"lined\", \"lining\", \"lip\", \"lip\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"little\", \"loganberry\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"long\", \"longevity\", \"longevity\", \"lots\", \"lots\", \"lots\", \"lots\", \"lots\", \"lots\", \"lots\", \"lots\", \"lots\", \"lovely\", \"lovely\", \"lovely\", \"lovely\", \"lovely\", \"lovely\", \"lovely\", \"lovely\", \"lovely\", \"lovely\", \"lower\", \"lush\", \"lush\", \"lush\", \"lush\", \"lush\", \"lush\", \"lush\", \"lush\", \"lush\", \"lynch\", \"l\\u00e9oville\", \"magnum\", \"mango\", \"mangoes\", \"marasca\", \"marbled\", \"markedly\", \"markedly\", \"matured\", \"matured\", \"matured\", \"matured\", \"mazuelo\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"medium\", \"melon\", \"melon\", \"melon\", \"melon\", \"meringue\", \"merlot\", \"merlot\", \"merlot\", \"merlot\", \"mesnil\", \"meters\", \"meunier\", \"meunier\", \"meyer\", \"meyer\", \"middle\", \"middle\", \"middle\", \"middle\", \"middle\", \"middle\", \"mineral\", \"mineral\", \"mineral\", \"mineral\", \"mineral\", \"mineral\", \"mineral\", \"mineral\", \"mineral\", \"mineral\", \"minerality\", \"minerality\", \"minerality\", \"minerality\", \"minerality\", \"minerality\", \"minerality\", \"minerality\", \"minerality\", \"minerality\", \"mingled\", \"monforte\", \"monopole\", \"montagne\", \"montalcino\", \"montalcino\", \"months\", \"months\", \"months\", \"months\", \"months\", \"morra\", \"moss\", \"moss\", \"mousse\", \"mousse\", \"mousse\", \"muddled\", \"muids\", \"mulberry\", \"mulberry\", \"mulberry\", \"mulberry\", \"mushrooms\", \"mushrooms\", \"mushrooms\", \"mushrooms\", \"nacional\", \"natural\", \"natural\", \"natural\", \"nebbiolo\", \"nebbiolo\", \"nebbiolo\", \"needle\", \"needles\", \"needles\", \"nerello\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"new\", \"noir\", \"noir\", \"noir\", \"noir\", \"noirs\", \"noirs\", \"non\", \"non\", \"non\", \"nonvintage\", \"nose\", \"nose\", \"nose\", \"nose\", \"nose\", \"nose\", \"nose\", \"nose\", \"nose\", \"notes\", \"notes\", \"notes\", \"notes\", \"notes\", \"notes\", \"notes\", \"notes\", \"notes\", \"notes\", \"notions\", \"notions\", \"notions\", \"november\", \"november\", \"november\", \"november\", \"nv\", \"nv\", \"oak\", \"oak\", \"oak\", \"oak\", \"oak\", \"oak\", \"oak\", \"oak\", \"oak\", \"offers\", \"offers\", \"offers\", \"offers\", \"offers\", \"offers\", \"offers\", \"offers\", \"offers\", \"offers\", \"oger\", \"oily\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"old\", \"oldest\", \"olive\", \"olive\", \"olive\", \"olive\", \"olive\", \"olives\", \"olives\", \"olives\", \"olives\", \"olives\", \"onwards\", \"orange\", \"orange\", \"orange\", \"orange\", \"orange\", \"orange\", \"orchard\", \"orchard\", \"orchard\", \"organic\", \"organic\", \"organic\", \"organic\", \"oven\", \"overtly\", \"oysters\", \"oysters\", \"pair\", \"palate\", \"palate\", \"palate\", \"palate\", \"palate\", \"palate\", \"palate\", \"palate\", \"palate\", \"palate\", \"pale\", \"pale\", \"pale\", \"pale\", \"pale\", \"pan\", \"panko\", \"papaya\", \"pape\", \"pape\", \"parcels\", \"parcels\", \"particular\", \"pasta\", \"pastille\", \"patience\", \"patience\", \"patience\", \"patience\", \"pavie\", \"peach\", \"peach\", \"peach\", \"peach\", \"peaches\", \"peaches\", \"peaches\", \"peaches\", \"pear\", \"pear\", \"pear\", \"pears\", \"peel\", \"peel\", \"peel\", \"peel\", \"peel\", \"peel\", \"pencil\", \"pencil\", \"pencil\", \"pencil\", \"peonies\", \"peppermint\", \"perlage\", \"perrier\", \"petal\", \"petal\", \"petal\", \"petal\", \"petit\", \"petit\", \"petit\", \"petrale\", \"pillowy\", \"pineapple\", \"pineapples\", \"pink\", \"pink\", \"pink\", \"pinot\", \"pinot\", \"pinot\", \"pinot\", \"pinot\", \"pinot\", \"pinpoint\", \"pipe\", \"pipe\", \"pipe\", \"piquant\", \"planted\", \"planted\", \"planted\", \"pleasing\", \"pleasing\", \"pleasing\", \"pleasing\", \"pleasing\", \"plot\", \"plum\", \"plum\", \"plum\", \"plum\", \"plum\", \"plum\", \"plum\", \"plum\", \"plums\", \"plums\", \"plums\", \"plums\", \"polished\", \"polished\", \"polished\", \"polished\", \"polished\", \"polished\", \"polished\", \"polished\", \"pomegranate\", \"pomegranate\", \"pomegranate\", \"pomegranate\", \"pomegranates\", \"pomerol\", \"pork\", \"port\", \"possess\", \"potting\", \"priorat\", \"produced\", \"produced\", \"produced\", \"produced\", \"pure\", \"pure\", \"pure\", \"pure\", \"pure\", \"pure\", \"pure\", \"pure\", \"pure\", \"pure\", \"puree\", \"purple\", \"purple\", \"purple\", \"purple\", \"purple\", \"purple\", \"p\\u00e2te\", \"quinta\", \"raspberry\", \"raspberry\", \"raspberry\", \"raspberry\", \"raspberry\", \"raspberry\", \"raspberry\", \"rating\", \"rating\", \"rating\", \"recalling\", \"red\", \"red\", \"red\", \"red\", \"red\", \"red\", \"red\", \"red\", \"red\", \"refined\", \"refined\", \"refined\", \"refined\", \"refined\", \"refined\", \"refined\", \"refined\", \"refined\", \"refined\", \"relax\", \"relaxed\", \"release\", \"release\", \"release\", \"release\", \"release\", \"repeating\", \"reserva\", \"reserva\", \"reserva\", \"reserve\", \"reserve\", \"reserve\", \"reserve\", \"reserve\", \"reserve\", \"residual\", \"residual\", \"ribeye\", \"ribs\", \"rich\", \"rich\", \"rich\", \"rich\", \"rich\", \"rich\", \"rich\", \"rich\", \"rich\", \"rich\", \"ried\", \"riesling\", \"riesling\", \"riesling\", \"riesling\", \"ripe\", \"ripe\", \"ripe\", \"ripe\", \"ripe\", \"ripe\", \"ripe\", \"ripe\", \"ripe\", \"ripe\", \"ripened\", \"ripened\", \"ripeness\", \"ripeness\", \"ripeness\", \"ripeness\", \"ripeness\", \"ripeness\", \"ripeness\", \"ripeness\", \"riserva\", \"riserva\", \"riserva\", \"roast\", \"roast\", \"roast\", \"roast\", \"roast\", \"roast\", \"roederer\", \"root\", \"root\", \"root\", \"roriz\", \"rose\", \"rose\", \"rose\", \"rose\", \"rose\", \"rose\", \"rose\", \"rose\", \"ros\\u00e9\", \"ros\\u00e9\", \"rotie\", \"round\", \"round\", \"round\", \"round\", \"round\", \"round\", \"round\", \"round\", \"sagebrush\", \"saint\", \"saint\", \"saint\", \"saint\", \"salted\", \"salted\", \"salted\", \"salted\", \"salted\", \"salty\", \"salty\", \"salty\", \"salty\", \"salty\", \"salty\", \"salty\", \"salty\", \"sample\", \"sample\", \"sample\", \"san\", \"sancerre\", \"sanguine\", \"sappy\", \"sappy\", \"sappy\", \"sappy\", \"sarsaparilla\", \"sashimi\", \"sauvignon\", \"sauvignon\", \"sauvignon\", \"sauvignon\", \"sauvignon\", \"sauvignon\", \"savory\", \"savory\", \"savory\", \"savory\", \"savory\", \"savory\", \"savory\", \"savory\", \"savory\", \"savory\", \"scavino\", \"scrub\", \"seamless\", \"seamless\", \"seamless\", \"seamless\", \"seamless\", \"seamless\", \"seamless\", \"seared\", \"season\", \"seeking\", \"sel\", \"semillon\", \"sensibility\", \"serralunga\", \"serralunga\", \"serve\", \"serve\", \"sexy\", \"sexy\", \"sexy\", \"shavings\", \"shavings\", \"shavings\", \"shimmering\", \"shines\", \"shines\", \"shines\", \"shows\", \"shows\", \"shows\", \"shows\", \"shows\", \"shows\", \"shows\", \"shows\", \"shows\", \"shows\", \"singed\", \"singed\", \"singed\", \"single\", \"single\", \"single\", \"skinned\", \"slavonian\", \"slew\", \"sliced\", \"sliced\", \"sliced\", \"smacking\", \"smoldering\", \"sneaky\", \"sneaky\", \"soften\", \"soften\", \"soften\", \"soils\", \"soils\", \"soils\", \"sole\", \"solera\", \"sous\", \"spice\", \"spice\", \"spice\", \"spice\", \"spice\", \"spice\", \"spice\", \"spice\", \"spice\", \"spicecake\", \"spices\", \"spices\", \"spices\", \"spices\", \"spices\", \"spices\", \"spices\", \"spices\", \"spices\", \"spices\", \"spicier\", \"spirits\", \"stags\", \"stainless\", \"stainless\", \"stainless\", \"stainless\", \"star\", \"star\", \"star\", \"star\", \"star\", \"star\", \"stated\", \"steamed\", \"steel\", \"steel\", \"steel\", \"steel\", \"steeped\", \"steeped\", \"stew\", \"stimulating\", \"stone\", \"stone\", \"stone\", \"stone\", \"stone\", \"stone\", \"stone\", \"stone\", \"stone\", \"stone\", \"strawberries\", \"strawberries\", \"strawberries\", \"strawberries\", \"strawberries\", \"strawberry\", \"strawberry\", \"strawberry\", \"strawberry\", \"strawberry\", \"strawberry\", \"struck\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"structure\", \"succulent\", \"succulent\", \"succulent\", \"succulent\", \"succulent\", \"succulent\", \"succulent\", \"succulent\", \"suede\", \"sufficiently\", \"suffused\", \"sugar\", \"sugar\", \"sugar\", \"sugar\", \"sugar\", \"sugar\", \"s\\u00e9millon\", \"tannined\", \"tannins\", \"tannins\", \"tannins\", \"tannins\", \"tannins\", \"tannins\", \"tannins\", \"tar\", \"tar\", \"tar\", \"tar\", \"tarry\", \"tarry\", \"tasted\", \"tasted\", \"tasted\", \"tasted\", \"tasting\", \"tasting\", \"tasting\", \"tautly\", \"tea\", \"tea\", \"tea\", \"tea\", \"tea\", \"tea\", \"tea\", \"tea\", \"tea\", \"tempranillo\", \"tempranillo\", \"tempranillo\", \"tempranillo\", \"tempranillo\", \"tender\", \"tensioned\", \"terroir\", \"terroir\", \"terroir\", \"terroir\", \"texture\", \"texture\", \"texture\", \"texture\", \"texture\", \"texture\", \"texture\", \"texture\", \"texture\", \"tight\", \"tight\", \"tight\", \"tight\", \"tight\", \"tight\", \"tight\", \"tight\", \"tile\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"time\", \"tobacco\", \"tobacco\", \"tobacco\", \"tobacco\", \"tondonia\", \"touch\", \"touch\", \"touch\", \"touch\", \"touch\", \"touch\", \"touch\", \"touch\", \"touch\", \"touch\", \"touriga\", \"treatment\", \"trimmed\", \"trocken\", \"tropical\", \"tropical\", \"tropical\", \"try\", \"try\", \"try\", \"tug\", \"uco\", \"underbrush\", \"underbrush\", \"underbrush\", \"undertones\", \"undertones\", \"undertones\", \"unquestionably\", \"valmur\", \"vats\", \"vats\", \"veltliner\", \"verdot\", \"verdot\", \"verdot\", \"vergelesses\", \"vigna\", \"vigna\", \"vines\", \"vines\", \"vines\", \"vines\", \"vineyard\", \"vineyard\", \"vineyard\", \"vineyard\", \"vineyard\", \"vineyard\", \"vineyards\", \"vineyards\", \"vineyards\", \"vineyards\", \"vineyards\", \"vinification\", \"vinous\", \"vinous\", \"vinous\", \"vinous\", \"vintage\", \"vintage\", \"vintage\", \"vintage\", \"vintage\", \"vintage\", \"vintage\", \"vintage\", \"viscous\", \"vital\", \"vi\\u00f1a\", \"vi\\u00f1a\", \"walnut\", \"walnut\", \"walnut\", \"walnuts\", \"walnuts\", \"wears\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"weight\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"white\", \"wild\", \"wild\", \"wild\", \"wild\", \"wild\", \"wild\", \"wild\", \"wild\", \"wild\", \"wild\", \"wine\", \"wine\", \"wine\", \"wine\", \"wine\", \"wine\", \"wine\", \"wine\", \"wine\", \"wine\", \"wines\", \"wines\", \"wines\", \"wines\", \"wines\", \"wines\", \"wines\", \"winey\", \"wisps\", \"wood\", \"wood\", \"wood\", \"wood\", \"wood\", \"wood\", \"wood\", \"woodland\", \"woodland\", \"woodsy\", \"year\", \"year\", \"year\", \"year\", \"year\", \"year\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"years\", \"yeasts\", \"yellow\", \"yellow\", \"yellow\", \"yellow\", \"youthfully\", \"youthfully\", \"youthfully\", \"youthfully\", \"yuzu\", \"zest\", \"zest\", \"zest\", \"zest\", \"zest\", \"zingy\"]}, \"R\": 30, \"lambda.step\": 0.01, \"plot.opts\": {\"xlab\": \"PC1\", \"ylab\": \"PC2\"}, \"topic.order\": [1, 2, 3, 4, 5, 6, 7, 8, 9, 10]};\n",
       "\n",
       "function LDAvis_load_lib(url, callback){\n",
       "  var s = document.createElement('script');\n",
       "  s.src = url;\n",
       "  s.async = true;\n",
       "  s.onreadystatechange = s.onload = callback;\n",
       "  s.onerror = function(){console.warn(\"failed to load library \" + url);};\n",
       "  document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "}\n",
       "\n",
       "if(typeof(LDAvis) !== \"undefined\"){\n",
       "   // already loaded: just create the visualization\n",
       "   !function(LDAvis){\n",
       "       new LDAvis(\"#\" + \"ldavis_el190001870328165424406599288\", ldavis_el190001870328165424406599288_data);\n",
       "   }(LDAvis);\n",
       "}else if(typeof define === \"function\" && define.amd){\n",
       "   // require.js is available: use it to load d3/LDAvis\n",
       "   require.config({paths: {d3: \"https://d3js.org/d3.v5\"}});\n",
       "   require([\"d3\"], function(d3){\n",
       "      window.d3 = d3;\n",
       "      LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "        new LDAvis(\"#\" + \"ldavis_el190001870328165424406599288\", ldavis_el190001870328165424406599288_data);\n",
       "      });\n",
       "    });\n",
       "}else{\n",
       "    // require.js not available: dynamically load d3 & LDAvis\n",
       "    LDAvis_load_lib(\"https://d3js.org/d3.v5.js\", function(){\n",
       "         LDAvis_load_lib(\"https://cdn.jsdelivr.net/gh/bmabey/pyLDAvis@3.4.0/pyLDAvis/js/ldavis.v3.0.0.js\", function(){\n",
       "                 new LDAvis(\"#\" + \"ldavis_el190001870328165424406599288\", ldavis_el190001870328165424406599288_data);\n",
       "            })\n",
       "         });\n",
       "}\n",
       "</script>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pyLDAvis.display(lda_display_review)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter tuning "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fitting an LSA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:770: UserWarning: Scoring failed. The score on this train-test partition for these parameters will be set to nan. Details: \n",
      "Traceback (most recent call last):\n",
      "  File \"C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 759, in _score\n",
      "    scores = scorer(estimator, X_test)\n",
      "TypeError: __call__() missing 1 required positional argument: 'y_true'\n",
      "\n",
      "  warnings.warn(\n",
      "C:\\Users\\garyb\\anaconda3\\lib\\site-packages\\sklearn\\feature_extraction\\text.py:396: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ll', 've'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_19000\\1564239691.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;31m# Perform grid search\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m \u001b[0mgrid_search\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 20\u001b[1;33m \u001b[0mgrid_search\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwine_df\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"reviews\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[1;31m# Print the best hyperparameters\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    889\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    890\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 891\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    892\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    893\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1390\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1391\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1392\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1394\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    836\u001b[0m                     )\n\u001b[0;32m    837\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 838\u001b[1;33m                 out = parallel(\n\u001b[0m\u001b[0;32m    839\u001b[0m                     delayed(_fit_and_score)(\n\u001b[0;32m    840\u001b[0m                         \u001b[0mclone\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_estimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1049\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_original_iterator\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1050\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1051\u001b[1;33m             \u001b[1;32mwhile\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatch_one_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1052\u001b[0m                 \u001b[1;32mpass\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mdispatch_one_batch\u001b[1;34m(self, iterator)\u001b[0m\n\u001b[0;32m    862\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    863\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 864\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_dispatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtasks\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    865\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    866\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m_dispatch\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    780\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    781\u001b[0m             \u001b[0mjob_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 782\u001b[1;33m             \u001b[0mjob\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    783\u001b[0m             \u001b[1;31m# A job can complete so quickly than its callback is\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    784\u001b[0m             \u001b[1;31m# called before we get here, causing self._jobs to\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mapply_async\u001b[1;34m(self, func, callback)\u001b[0m\n\u001b[0;32m    206\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mapply_async\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    207\u001b[0m         \u001b[1;34m\"\"\"Schedule a func to be run\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 208\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mImmediateResult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    209\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcallback\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    210\u001b[0m             \u001b[0mcallback\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, batch)\u001b[0m\n\u001b[0;32m    570\u001b[0m         \u001b[1;31m# Don't delay the application, to avoid keeping the input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    571\u001b[0m         \u001b[1;31m# arguments in memory\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 572\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresults\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbatch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    573\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    574\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    264\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    261\u001b[0m         \u001b[1;31m# change the default number of processes to -1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    262\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mparallel_backend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_n_jobs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 263\u001b[1;33m             return [func(*args, **kwargs)\n\u001b[0m\u001b[0;32m    264\u001b[0m                     for func, args, kwargs in self.items]\n\u001b[0;32m    265\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\fixes.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mconfig_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 216\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36m_fit_and_score\u001b[1;34m(estimator, X, y, scorer, train, test, verbose, parameters, fit_params, return_train_score, return_parameters, return_n_test_samples, return_times, return_estimator, split_progress, candidate_progress, error_score)\u001b[0m\n\u001b[0;32m    671\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    672\u001b[0m     \u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 673\u001b[1;33m     \u001b[0mX_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_split\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    674\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    675\u001b[0m     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m{\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\u001b[0m in \u001b[0;36m_safe_split\u001b[1;34m(estimator, X, y, indices, train_indices)\u001b[0m\n\u001b[0;32m    286\u001b[0m             \u001b[0mX_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mix_\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtrain_indices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    287\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 288\u001b[1;33m         \u001b[0mX_subset\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_safe_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    289\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    290\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0my\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_safe_indexing\u001b[1;34m(X, indices, axis)\u001b[0m\n\u001b[0;32m    374\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    375\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"iloc\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 376\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0m_pandas_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    377\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"shape\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    378\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_array_indexing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices_dtype\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\sklearn\\utils\\__init__.py\u001b[0m in \u001b[0;36m_pandas_indexing\u001b[1;34m(X, key, key_dtype, axis)\u001b[0m\n\u001b[0;32m    216\u001b[0m         \u001b[1;31m# using take() instead of iloc[] ensures the return value is a \"proper\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    217\u001b[0m         \u001b[1;31m# copy that will not raise SettingWithCopyWarning\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 218\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    219\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    220\u001b[0m         \u001b[1;31m# check whether we should index with loc or iloc\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\series.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indices, axis, is_copy, **kwargs)\u001b[0m\n\u001b[0;32m    905\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    906\u001b[0m         \u001b[0mindices\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mensure_platform_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 907\u001b[1;33m         \u001b[0mnew_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    908\u001b[0m         \u001b[0mnew_values\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\indexes\\base.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(self, indices, axis, allow_fill, fill_value, **kwargs)\u001b[0m\n\u001b[0;32m   1119\u001b[0m         \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1120\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1121\u001b[1;33m             taken = algos.take(\n\u001b[0m\u001b[0;32m   1122\u001b[0m                 \u001b[0mvalues\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mallow_fill\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mallow_fill\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfill_value\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_na_value\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1123\u001b[0m             )\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\pandas\\core\\algorithms.py\u001b[0m in \u001b[0;36mtake\u001b[1;34m(arr, indices, axis, allow_fill, fill_value)\u001b[0m\n\u001b[0;32m   1447\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1448\u001b[0m         \u001b[1;31m# NumPy style\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1449\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0marr\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mindices\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1450\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1451\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('tfidf', TfidfVectorizer(stop_words=stopwords, min_df=5, max_df=0.7)),\n",
    "    ('svd', TruncatedSVD(random_state=42))\n",
    "])\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'svd__n_components': [5, 10, 15],\n",
    "    'svd__algorithm': ['randomized', 'arpack', 'full'],\n",
    "    'svd__tol': [0.0001, 0.001, 0.01]\n",
    "}\n",
    "\n",
    "# Perform grid search\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, scoring='accuracy', cv=5)\n",
    "grid_search.fit(wine_df[\"reviews\"])\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best parameters: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyp_svd_reviews_model = TruncatedSVD(n_components=10, random_state=42, algorithm='randomized', tol=0.001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the matrix \n",
    "W_h_svd_reviews_matrix = hyp_svd_reviews_model.fit_transform(tfidf_reviews_vectors)\n",
    "H_h_svd_reviews_matrix = hyp_svd_reviews_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df[\"svd_topic_reviews_H\"] = np.argmax(W_h_svd_reviews_matrix, axis = 1).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for loop that count the unique topic \n",
    "for label in wine_df['svd_topic_reviews_H'].unique():\n",
    "    print(f\"topic {label} observes original label counts of:\")\n",
    "    print(wine_df[wine_df['svd_topic_reviews_H'] == label]['category'].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting an LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import make_scorer\n",
    "import numpy as np\n",
    "\n",
    "# Define a custom scorer based on the log-likelihood metric for LDA\n",
    "def log_likelihood_scorer(model, X):\n",
    "    return np.sum(model.score(X))\n",
    "\n",
    "# Create the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', CountVectorizer()),\n",
    "    ('lda', LatentDirichletAllocation())\n",
    "])\n",
    "\n",
    "# Define the hyperparameter grid\n",
    "param_grid = {\n",
    "    'vectorizer__max_df': [0.5, 0.75],\n",
    "    'vectorizer__min_df': [1, 2],\n",
    "    'lda__n_components': [5, 10],\n",
    "    'lda__learning_method': ['batch', 'online']\n",
    "}\n",
    "\n",
    "# Perform grid search with the custom scorer\n",
    "grid_search = GridSearchCV(pipeline, param_grid=param_grid, scoring=make_scorer(log_likelihood_scorer), refit=True)\n",
    "grid_search.fit(wine_df[\"reviews\"])\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best parameters: \", grid_search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_reviews_model_H = LatentDirichletAllocation(learning_method='batch', n_components = 10, random_state=42,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the matrix \n",
    "W_lda_H_reviews_matrix = lda_reviews_model_H.fit_transform(count_reviews_vectors)\n",
    "H_lda_H_reviews_matrix = lda_reviews_model_H.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the matrix of the new topic \n",
    "wine_df[\"lda_topic_reviews_H\"] = np.argmax(W_lda_H_reviews_matrix, axis = 1).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for loop that count the unique topic \n",
    "for label in wine_df['lda_topic_reviews_H'].unique():\n",
    "    print(f\"topic {label} observes original label counts of:\")\n",
    "    print(wine_df[wine_df['lda_topic_reviews_H'] == label]['category'].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_display_review_H = pyLDAvis.sklearn.prepare(lda_reviews_model_H, count_reviews_vectors, count_reviews_vectorizer, sort_topics=False)\n",
    "pyLDAvis.display(lda_display_review_H)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Description "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df['text'] = wine_df['text'].fillna('')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_text_vectorizer = CountVectorizer(stop_words=stopwords, min_df=5, max_df=0.7)\n",
    "count_text_vectors = count_text_vectorizer.fit_transform(wine_df[\"text\"])\n",
    "count_text_vectors.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_text_vectorizer = TfidfVectorizer(stop_words=stopwords, min_df=5, max_df=0.7)\n",
    "tfidf_text_vectors = tfidf_text_vectorizer.fit_transform(wine_df[\"text\"])\n",
    "tfidf_text_vectors.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting a Non-Negative Matrix Factorization Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nmf_text_model = NMF(n_components=5, random_state=314)\n",
    "W_text_matrix = nmf_text_model.fit_transform(tfidf_text_vectors)\n",
    "H_text_matrix = nmf_text_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "display_topics(nmf_text_model, tfidf_text_vectorizer.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting an LSA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svd_text_model = TruncatedSVD(n_components = 10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_svd_text_matrix = svd_text_model.fit_transform(tfidf_text_vectors)\n",
    "H_svd_text_matrix = svd_text_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_df[\"svd_topic_text\"] = np.argmax(W_svd_reviews_matrix, axis = 1).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for loop that count the unique topic \n",
    "for label in wine_df['svd_topic_reviews'].unique():\n",
    "    print(f\"topic {label} observes original label counts of:\")\n",
    "    print(wine_df[wine_df['svd_topic_reviews'] == label]['category'].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fitting an LDA Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_text_model = LatentDirichletAllocation(n_components = 10, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the matrix \n",
    "W_lda_text_matrix = lda_text_model.fit_transform(count_text_vectors)\n",
    "H_lda_text_matrix = lda_text_model.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#saving the matrix of the new topic \n",
    "wine_df[\"lda_topic\"] = np.argmax(W_lda_text_matrix, axis = 1).astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for loop that count the unique topic \n",
    "for label in wine_df['lda_topic'].unique():\n",
    "    print(f\"topic {label} observes original label counts of:\")\n",
    "    print(wine_df[wine_df['lda_topic'] == label]['category'].value_counts())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda_display = pyLDAvis.sklearn.prepare(lda_text_model, count_text_vectors, count_text_vectorizer, sort_topics=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pyLDAvis.display(lda_display)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display both visualizations\n",
    "pyLDAvis.display(lda_display)\n",
    "pyLDAvis.display(lda_display_review)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
