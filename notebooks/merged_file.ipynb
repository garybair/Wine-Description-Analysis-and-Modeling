{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "685b7ac9",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a33b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries needed for web scraping\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc6d34d",
   "metadata": {},
   "source": [
    "### Using the BeautifulSuop scarper api to scrape the data from wine.com, we will scrape the landing page and iterate through the win varieties and scrape results page and product page an placing it into a dictionary for futher analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0ced6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the scraper \n",
    "class Scraper:\n",
    "#initializing the web scaper for extracting data from wine.com\n",
    "    def __init__(self, pullQuantity = 10000):\n",
    "        self.session = requests.Session()\n",
    "        self.startTime = time.time()\n",
    "        self.pullQuantity = pullQuantity\n",
    "        self.staticURL = 'https://www.wine.com'\n",
    "        self.siteMapDirectory = '../../data/wine-com/'\n",
    "        self.filePath = '../../data/wine-com/raw/{}.txt'.format(self.startTime)\n",
    "        self.headerData = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'}\n",
    "\n",
    "       #determine if sitemap exists, initialize if not\n",
    "        try:\n",
    "            with open(self.siteMapDirectory + '/site-map.json', 'r') as fileObj:\n",
    "                self.siteMap = json.load(fileObj)\n",
    "        except:\n",
    "            self.siteMap = {}\n",
    "        \n",
    "        #generate session file - validated\n",
    "        with open(self.filePath,'w') as fileObj:\n",
    "            fileObj.write('product_url|product_name|product_variety|product_origin|product_family|user_avg_rating|user_rating_count|product_description|product_reviews\\n')  \n",
    "        fileObj.close()\n",
    "            \n",
    "    def scrape(self):\n",
    "        #determine landing page map exists\n",
    "        if len(self.siteMap) == 0:\n",
    "            self.scrapeLandingPage()\n",
    "            self.parseLandingPage()\n",
    "        #iterate through wine varieties\n",
    "        sleep(2)\n",
    "        for key, value in self.siteMap['landingPageLinks'].items():\n",
    "            self.searchVarietal = key\n",
    "            self.searchURL = value\n",
    "            self.scrapeParseResultsPage()\n",
    "            #write sitemap to JSON file\n",
    "            with open(self.siteMapDirectory + '/site-map.json', 'w') as fileObj:\n",
    "                json.dump(self.siteMap, fileObj)\n",
    "            fileObj.close()\n",
    "\n",
    "    # validated\n",
    "    def scrapeLandingPage(self):\n",
    "        landingPageResponse = self.session.get(self.staticURL, headers = self.headerData)\n",
    "        self.landingPageSoup = BeautifulSoup(landingPageResponse.content, \"html.parser\")\n",
    "    \n",
    "    #validated\n",
    "    def parseLandingPage(self):\n",
    "        self.siteMap['landingPageLinks'] = dict()\n",
    "        landingPageVarietalsLevel0 = self.landingPageSoup.find('section', attrs = {'class': 'mainNav_section mainNav_section-varietal mainNavList_item-level0'})\n",
    "        for row in landingPageVarietalsLevel0.find_all('li', attrs = {'class': 'mainNavList_item mainNavList_item-level2'}):\n",
    "            a_tag = row.find('a', attrs = {'class': 'mainNavList_itemLink'})\n",
    "            self.varietalName = a_tag.text\n",
    "            self.varietalLink = a_tag.get('href')\n",
    "            self.siteMap['landingPageLinks'][self.varietalName] = self.staticURL + self.varietalLink\n",
    "        #for some reason boxed sets and glassware links are present in varietals tab\n",
    "        del self.siteMap['landingPageLinks']['Wine Tasting Sets']\n",
    "        del self.siteMap['landingPageLinks']['Glassware & Accessories']\n",
    "    \n",
    "    def scrapeParseResultsPage(self):\n",
    "        if self.searchVarietal not in self.siteMap:\n",
    "            self.siteMap[self.searchVarietal] = dict()\n",
    "        \n",
    "        self.pullCount = 0\n",
    "        while self.pullCount < self.pullQuantity and self.searchURL is not None:\n",
    "            print(self.searchURL)\n",
    "            resultsPageResponse = self.session.get(self.searchURL, headers = self.headerData)\n",
    "            self.resultsPageSoup = BeautifulSoup(resultsPageResponse.content, \"html.parser\")\n",
    "            resultsContainer = self.resultsPageSoup.find('ul', attrs = {'class':'listGridLayout_list'})\n",
    "            for row in resultsContainer.find_all('div', attrs = {'class': 'listGridItemInfo'}):\n",
    "                productShortLink = row.a['href']\n",
    "                self.productURL = self.staticURL + productShortLink\n",
    "                if self.productURL not in self.siteMap[self.searchVarietal]:\n",
    "                    self.scrapeProductPage()\n",
    "                    self.parseProductPage()\n",
    "                else:\n",
    "                    pass\n",
    "            if self.pullCount < self.pullQuantity:\n",
    "                try:\n",
    "                    paginationContainer = self.resultsPageSoup.find('div', attrs = {'class':'nextPagePagination'})\n",
    "                    self.searchURL = paginationContainer.a['href']\n",
    "                except Exception:\n",
    "                    self.searchURL = None\n",
    "            \n",
    "    #validated\n",
    "    def scrapeProductPage(self):\n",
    "        if self.productURL not in self.siteMap[self.searchVarietal]:\n",
    "            self.siteMap[self.searchVarietal][self.productURL] = dict()\n",
    "        \n",
    "        try:\n",
    "            productPageResponse = self.session.get(self.productURL, headers = self.headerData)\n",
    "            self.productPageSoup = BeautifulSoup(productPageResponse.content, \"html.parser\")\n",
    "            \n",
    "            self.siteMap[self.searchVarietal][self.productURL]['scrapeStatus'] = 'success'\n",
    "        except Exception:\n",
    "            self.siteMap[self.searchVarietal][self.productURL]['scrapeStatus'] = 'fail'\n",
    "            \n",
    "    #validated\n",
    "    def parseProductPage(self):\n",
    "        try:\n",
    "            #generate dictionary for parsed fields\n",
    "            self.prodData = dict()\n",
    "            \n",
    "            #product info\n",
    "            prodInfo = self.productPageSoup.find('div', attrs = {'class':'pipInfo'})\n",
    "            self.prodData['Product_Name'] = prodInfo.find('h1', attrs = {'class':'pipName'}).text\n",
    "            self.prodData['Product_Variety'] = prodInfo.find('span', attrs = {'class':'prodItemInfo_varietal'}).text\n",
    "            self.prodData['Product_Origin'] = prodInfo.find('span', attrs = {'class':'prodItemInfo_originText'}).text\n",
    "            \n",
    "            #product attributes\n",
    "            self.prodData['Product_Family'] = self.searchVarietal\n",
    "            \n",
    "            try:\n",
    "                #average user product ratings\n",
    "                self.prodData['User_Avg_Rating'] = self.productPageSoup.find('span', attrs = {'class':'averageRating_average'}).text\n",
    "            except Exception:\n",
    "                self.prodData['User_Avg_Rating'] = 'nan'\n",
    "                \n",
    "            try:\n",
    "                #product ratings count\n",
    "                self.prodData['User_Rating_Count'] = self.productPageSoup.find('span', attrs = {'class':'averageRating_number'}).text\n",
    "            except Exception:\n",
    "                self.prodData['User_Rating_Count'] = 'nan'\n",
    "                \n",
    "            try:\n",
    "                #winemaker notes\n",
    "                prodNotes = self.productPageSoup.find('div', attrs = {'class':'pipWineNotes_copy viewMoreModule js-expanded'})\n",
    "                wineMakerNotes = prodNotes.find('div', attrs = {'class': 'viewMoreModule_text'}).text\n",
    "                wineMakerNotes = wineMakerNotes.replace('\\n', ' ')\n",
    "                wineMakerNotes = unicodedata.normalize('NFKD', wineMakerNotes)\n",
    "                self.prodData['Winemaker_Description'] = wineMakerNotes\n",
    "            except Exception:\n",
    "                self.prodData['Winemaker_Description'] = 'nan'\n",
    "            \n",
    "            try:\n",
    "                #professional reviews\n",
    "                prodProfessionalReviews = self.productPageSoup.find('div', attrs = {'class': 'viewMoreModule_text viewMoreModule-reviews'})\n",
    "                self.prodData['Critical_Reviews'] = []\n",
    "                for row in prodProfessionalReviews.find_all('div', attrs = {'class': 'pipProfessionalReviews_list'}):\n",
    "                    reviewer_name = row.find('div', attrs = {'class': 'pipProfessionalReviews_authorName'}).text\n",
    "                    reviewer_rating = row.find('span', attrs = {'class': 'wineRatings_rating'}).text\n",
    "                    reviewer_text = row.find('div', attrs = {'class': 'pipSecContent_copy'}).text\n",
    "                    reviewer_text = reviewer_text.replace('\\n', ' ')\n",
    "                    reviewer_text = unicodedata.normalize('NFKD', reviewer_text)                             \n",
    "                    self.prodData['Critical_Reviews'].append(f'({reviewer_name}; {reviewer_rating}; {reviewer_text})')\n",
    "            except Exception:\n",
    "                print('review parse failed')\n",
    "                self.prodData['Critical_Reviews'] = 'nan'\n",
    "\n",
    "            #write data to disk\n",
    "            self.writeProductData()\n",
    "            self.siteMap[self.searchVarietal][self.productURL]['parseStatus'] = 'success'\n",
    "        except Exception:\n",
    "            self.siteMap[self.searchVarietal][self.productURL]['parseStatus'] = 'fail'\n",
    "            print('{} parse failed'.format(self.productURL))\n",
    "\n",
    "    #validated\n",
    "    def writeProductData(self):\n",
    "        try:\n",
    "            with open(self.filePath, 'a') as file:\n",
    "                file.write('{}|{}|{}|{}|{}|{}|{}|{}|{}\\n'.format(self.productURL,\n",
    "                                                                 self.prodData['Product_Name'],\n",
    "                                                                 self.prodData['Product_Variety'],\n",
    "                                                                 self.prodData['Product_Origin'],\n",
    "                                                                 self.prodData['Product_Family'],\n",
    "                                                                 self.prodData['User_Avg_Rating'],\n",
    "                                                                 self.prodData['User_Rating_Count'],\n",
    "                                                                 self.prodData['Winemaker_Description'],\n",
    "                                                                 self.prodData['Critical_Reviews']))\n",
    "            file.close()\n",
    "            self.pullCount += 1\n",
    "            self.siteMap[self.searchVarietal][self.productURL]['writeStatus'] = 'success'\n",
    "        except Exception:\n",
    "            self.siteMap[self.searchVarietal][self.productURL]['writeStatus'] = 'fail'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8265d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_com_scraper = Scraper()\n",
    "wine_com_scraper.scrape()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "629ae7c3",
   "metadata": {},
   "source": [
    "# Preprocessing - Wine.com\n",
    "\n",
    "### Summary\n",
    "The code contained within this file aims to reconcile and process data scraped from Wine.com.\n",
    "\n",
    "This implementation observes the following considerations:\n",
    "1. Given that scrapes can and will fail, this script assumes that all files present in the raw folder are to be processed and combined into a master record. Redundant records and processing are prevented through the use of a site map that tracks successfully parsed and written pages.\n",
    "2. Ideally, this data would best be stored in a database with a mininum of two tables, product info and critical reviews, however as this analysis is predominantly interested in review data, a SQL style left join has been deemed adequate for this analysis despite the effect on the overall file size. This is a case of not letting perfect get in the way of good enough or having an \"agile\" mindset.\n",
    "3. The preprocess_text.py file contains the preprocess_text Class to be used in pipeline when addressing novel data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7cb060e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "from sklearn.base import TransformerMixin\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8708adaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#set directory locations\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "raw_folder = parent_directory + '/data/wine-com/raw/'\n",
    "processed_folder = parent_directory + '/data/wine-com/processed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5e6897",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b02838a",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1677386048.5362737.txt loading\n",
      "1677417882.7459548.txt loading\n"
     ]
    }
   ],
   "source": [
    "#instantiate data structure to hold initial raw data/critical critical reviews\n",
    "data = dict()\n",
    "review_data = dict()\n",
    "#iterate through wine.com raw data folder\n",
    "for filename in os.listdir(raw_folder):\n",
    "    #determine if file has already been processed - if true pass\n",
    "    if filename not in os.listdir(processed_folder):\n",
    "        print(f'{filename} loading')\n",
    "        data[filename] = dict()\n",
    "        review_data[filename] = dict()\n",
    "        #open file readlines to raw dictionary\n",
    "        with open(raw_folder + filename, newline='\\r\\n') as file:\n",
    "            header = next(file)\n",
    "            data[filename]['lines'] = file.readlines()\n",
    "        #create list to hold parsed records\n",
    "        data[filename]['data'] = []\n",
    "        review_data[filename]['data'] = []\n",
    "        #iterate through records to parse raw data\n",
    "        for line in data[filename]['lines']:\n",
    "            values = line.strip().split('|')\n",
    "            #while redundancy is built into the scraper to ensure that the correct format is achieved,\n",
    "            #this prevents rogue html from causing a bad data load\n",
    "            if len(values) == 9:\n",
    "                row = {\n",
    "                    'product_url': values[0],\n",
    "                    'product_name': values[1],\n",
    "                    'product_variety': values[2],\n",
    "                    'product_origin': values[3],\n",
    "                    'product_family': values[4],\n",
    "                    'user_avg_rating': values[5],\n",
    "                    'user_rating_count': values[6],\n",
    "                    'winemaker_description': values[7]\n",
    "                    #'critical_reviews' is parsed loaded below\n",
    "                }\n",
    "                #write to list to be loaded to dataframe\n",
    "                data[filename]['data'].append(row)\n",
    "                \n",
    "                #parse critical reviews - should be loaded as list\n",
    "                reviews = values[8].split(';')\n",
    "                for review in reviews:\n",
    "                    try:\n",
    "                        reviewer_name, reviewer_rating, reviewer_text = review.split(',')\n",
    "                        review = {\n",
    "                        'product_url': values[0],\n",
    "                        'reviewer_name': reviewer_name,\n",
    "                        'reviewer_rating': reviewer_rating,\n",
    "                        'reviewer_text': reviewer_text\n",
    "                        }\n",
    "                        \n",
    "                        review_data[filename]['data'].append(review)\n",
    "                    except Exception:\n",
    "                        pass      \n",
    "                    \n",
    "        # Convert the list of dictionaries to a Pandas DataFrame\n",
    "        data[filename]['unmerged_df'] = pd.DataFrame(data[filename]['data'])\n",
    "        review_data[filename]['df'] = pd.DataFrame(review_data[filename]['data'])\n",
    "        \n",
    "        #perform sql-style left join of review data onto main df\n",
    "        data[filename]['merged_df'] = pd.merge(data[filename]['unmerged_df'], \n",
    "                                               review_data[filename]['df'], \n",
    "                                               on='product_url', \n",
    "                                               how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ad138e2",
   "metadata": {},
   "source": [
    "We created two .txt file one for raw data and another for ciritcal review  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10d47afe",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3947addd",
   "metadata": {},
   "source": [
    "Using the transformerMixin to preprocess text data by removing stopwords, punctuation, and converting text to lowercase."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b881b2dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# a text preprocessor class is created to manage preprocessing of text fields\n",
    "# inheriting from TransformerMixin of Sklearn, this should allow for tying into large sklearn pipeline\n",
    "class preprocess_text(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if isinstance(X, pd.Series):\n",
    "            #standardize type\n",
    "            X = X.astype(str)\n",
    "            # lower text\n",
    "            X = X.str.lower()\n",
    "            # remove punctuation\n",
    "            X = X.str.replace('\\W+', ' ', regex = True)\n",
    "            # tokenize text into individual words\n",
    "            X = X.str.split()\n",
    "            # remove stopwords\n",
    "            X = X.apply(lambda x: [word for word in x if word not in (self.stop_words)])\n",
    "            # join words\n",
    "            X = X.apply(lambda x: ' '.join(x))\n",
    "            return X\n",
    "        elif isinstance(X, pd.DataFrame):\n",
    "            colnames = X.columns\n",
    "            for col in colnames:\n",
    "                #standardize type\n",
    "                X[col] = X[col].astype(str)\n",
    "                # convert text to lowercase\n",
    "                X[col] = X[col].str.lower()\n",
    "                # remove punctuation\n",
    "                X[col] = X[col].str.replace('\\W+', ' ', regex = True)\n",
    "                # tokenize text into individual words\n",
    "                X[col] = X[col].str.split()\n",
    "                # remove stopwords\n",
    "                X[col] = X[col].apply(lambda x: [word for word in x if word not in (self.stop_words)])\n",
    "                # join words\n",
    "                X[col] = X[col].apply(lambda x: ' '.join(x))\n",
    "            return X\n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "536bb781",
   "metadata": {},
   "source": [
    "### Clean & Write Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e89f23e4",
   "metadata": {},
   "source": [
    "applying the processor to the winemaker_description and reviewer_text and saving it to a .txt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bdb3fa2",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "processor = preprocess_text()\n",
    "\n",
    "output_path = processed_folder + f'{time.time()}.txt'\n",
    "\n",
    "for filename in data:\n",
    "    data[filename]['merged_df']['winemaker_description'] = processor.transform(data[filename]['merged_df']['winemaker_description'])\n",
    "    data[filename]['merged_df']['reviewer_text'] = processor.transform(data[filename]['merged_df']['reviewer_text'])\n",
    "    data[filename]['merged_df'].to_csv(output_path,\n",
    "                                       mode='a',\n",
    "                                       header = not os.path.exists(output_path),\n",
    "                                       sep = '|',\n",
    "                                       line_terminator = '\\r\\n',\n",
    "                                       index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
