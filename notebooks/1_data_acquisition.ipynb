{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "685b7ac9",
   "metadata": {},
   "source": [
    "# Data Acquisition\n",
    "\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2a33b22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import the libraries needed for web scraping\n",
    "from time import sleep\n",
    "from bs4 import BeautifulSoup\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import unicodedata"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cc6d34d",
   "metadata": {},
   "source": [
    "### Using the BeautifulSuop scarper api to scrape the data from wine.com, we will scrape the landing page and iterate through the win varieties and scrape results page and product page an placing it into a dictionary for futher analysis "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad0ced6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the scraper \n",
    "class Scraper:\n",
    "#initializing the web scaper for extracting data from wine.com\n",
    "    def __init__(self, pullQuantity = 10000):\n",
    "        self.session = requests.Session()\n",
    "        self.startTime = time.time()\n",
    "        self.pullQuantity = pullQuantity\n",
    "        self.staticURL = 'https://www.wine.com'\n",
    "        self.siteMapDirectory = '../../data/wine-com/'\n",
    "        self.filePath = '../../data/wine-com/raw/{}.txt'.format(self.startTime)\n",
    "        self.headerData = {'user-agent':'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/89.0.4389.82 Safari/537.36'}\n",
    "\n",
    "       #determine if sitemap exists, initialize if not\n",
    "        try:\n",
    "            with open(self.siteMapDirectory + '/site-map.json', 'r') as fileObj:\n",
    "                self.siteMap = json.load(fileObj)\n",
    "        except:\n",
    "            self.siteMap = {}\n",
    "        \n",
    "        #generate session file - validated\n",
    "        with open(self.filePath,'w') as fileObj:\n",
    "            fileObj.write('product_url|product_name|product_variety|product_origin|product_family|user_avg_rating|user_rating_count|product_description|product_reviews\\n')  \n",
    "        fileObj.close()\n",
    "            \n",
    "    def scrape(self):\n",
    "        #determine landing page map exists\n",
    "        if len(self.siteMap) == 0:\n",
    "            self.scrapeLandingPage()\n",
    "            self.parseLandingPage()\n",
    "        #iterate through wine varieties\n",
    "        sleep(2)\n",
    "        for key, value in self.siteMap['landingPageLinks'].items():\n",
    "            self.searchVarietal = key\n",
    "            self.searchURL = value\n",
    "            self.scrapeParseResultsPage()\n",
    "            #write sitemap to JSON file\n",
    "            with open(self.siteMapDirectory + '/site-map.json', 'w') as fileObj:\n",
    "                json.dump(self.siteMap, fileObj)\n",
    "            fileObj.close()\n",
    "\n",
    "    # validated\n",
    "    def scrapeLandingPage(self):\n",
    "        landingPageResponse = self.session.get(self.staticURL, headers = self.headerData)\n",
    "        self.landingPageSoup = BeautifulSoup(landingPageResponse.content, \"html.parser\")\n",
    "    \n",
    "    #validated\n",
    "    def parseLandingPage(self):\n",
    "        self.siteMap['landingPageLinks'] = dict()\n",
    "        landingPageVarietalsLevel0 = self.landingPageSoup.find('section', attrs = {'class': 'mainNav_section mainNav_section-varietal mainNavList_item-level0'})\n",
    "        for row in landingPageVarietalsLevel0.find_all('li', attrs = {'class': 'mainNavList_item mainNavList_item-level2'}):\n",
    "            a_tag = row.find('a', attrs = {'class': 'mainNavList_itemLink'})\n",
    "            self.varietalName = a_tag.text\n",
    "            self.varietalLink = a_tag.get('href')\n",
    "            self.siteMap['landingPageLinks'][self.varietalName] = self.staticURL + self.varietalLink\n",
    "        #for some reason boxed sets and glassware links are present in varietals tab\n",
    "        del self.siteMap['landingPageLinks']['Wine Tasting Sets']\n",
    "        del self.siteMap['landingPageLinks']['Glassware & Accessories']\n",
    "    \n",
    "    def scrapeParseResultsPage(self):\n",
    "        if self.searchVarietal not in self.siteMap:\n",
    "            self.siteMap[self.searchVarietal] = dict()\n",
    "        \n",
    "        self.pullCount = 0\n",
    "        while self.pullCount < self.pullQuantity and self.searchURL is not None:\n",
    "            print(self.searchURL)\n",
    "            resultsPageResponse = self.session.get(self.searchURL, headers = self.headerData)\n",
    "            self.resultsPageSoup = BeautifulSoup(resultsPageResponse.content, \"html.parser\")\n",
    "            resultsContainer = self.resultsPageSoup.find('ul', attrs = {'class':'listGridLayout_list'})\n",
    "            for row in resultsContainer.find_all('div', attrs = {'class': 'listGridItemInfo'}):\n",
    "                productShortLink = row.a['href']\n",
    "                self.productURL = self.staticURL + productShortLink\n",
    "                if self.productURL not in self.siteMap[self.searchVarietal]:\n",
    "                    self.scrapeProductPage()\n",
    "                    self.parseProductPage()\n",
    "                else:\n",
    "                    pass\n",
    "            if self.pullCount < self.pullQuantity:\n",
    "                try:\n",
    "                    paginationContainer = self.resultsPageSoup.find('div', attrs = {'class':'nextPagePagination'})\n",
    "                    self.searchURL = paginationContainer.a['href']\n",
    "                except Exception:\n",
    "                    self.searchURL = None\n",
    "            \n",
    "    #validated\n",
    "    def scrapeProductPage(self):\n",
    "        if self.productURL not in self.siteMap[self.searchVarietal]:\n",
    "            self.siteMap[self.searchVarietal][self.productURL] = dict()\n",
    "        \n",
    "        try:\n",
    "            productPageResponse = self.session.get(self.productURL, headers = self.headerData)\n",
    "            self.productPageSoup = BeautifulSoup(productPageResponse.content, \"html.parser\")\n",
    "            \n",
    "            self.siteMap[self.searchVarietal][self.productURL]['scrapeStatus'] = 'success'\n",
    "        except Exception:\n",
    "            self.siteMap[self.searchVarietal][self.productURL]['scrapeStatus'] = 'fail'\n",
    "            \n",
    "    #validated\n",
    "    def parseProductPage(self):\n",
    "        try:\n",
    "            #generate dictionary for parsed fields\n",
    "            self.prodData = dict()\n",
    "            \n",
    "            #product info\n",
    "            prodInfo = self.productPageSoup.find('div', attrs = {'class':'pipInfo'})\n",
    "            self.prodData['Product_Name'] = prodInfo.find('h1', attrs = {'class':'pipName'}).text\n",
    "            self.prodData['Product_Variety'] = prodInfo.find('span', attrs = {'class':'prodItemInfo_varietal'}).text\n",
    "            self.prodData['Product_Origin'] = prodInfo.find('span', attrs = {'class':'prodItemInfo_originText'}).text\n",
    "            \n",
    "            #product attributes\n",
    "            self.prodData['Product_Family'] = self.searchVarietal\n",
    "            \n",
    "            try:\n",
    "                #average user product ratings\n",
    "                self.prodData['User_Avg_Rating'] = self.productPageSoup.find('span', attrs = {'class':'averageRating_average'}).text\n",
    "            except Exception:\n",
    "                self.prodData['User_Avg_Rating'] = 'nan'\n",
    "                \n",
    "            try:\n",
    "                #product ratings count\n",
    "                self.prodData['User_Rating_Count'] = self.productPageSoup.find('span', attrs = {'class':'averageRating_number'}).text\n",
    "            except Exception:\n",
    "                self.prodData['User_Rating_Count'] = 'nan'\n",
    "                \n",
    "            try:\n",
    "                #winemaker notes\n",
    "                prodNotes = self.productPageSoup.find('div', attrs = {'class':'pipWineNotes_copy viewMoreModule js-expanded'})\n",
    "                wineMakerNotes = prodNotes.find('div', attrs = {'class': 'viewMoreModule_text'}).text\n",
    "                wineMakerNotes = wineMakerNotes.replace('\\n', ' ')\n",
    "                wineMakerNotes = unicodedata.normalize('NFKD', wineMakerNotes)\n",
    "                self.prodData['Winemaker_Description'] = wineMakerNotes\n",
    "            except Exception:\n",
    "                self.prodData['Winemaker_Description'] = 'nan'\n",
    "            \n",
    "            try:\n",
    "                #professional reviews\n",
    "                prodProfessionalReviews = self.productPageSoup.find('div', attrs = {'class': 'viewMoreModule_text viewMoreModule-reviews'})\n",
    "                self.prodData['Critical_Reviews'] = []\n",
    "                for row in prodProfessionalReviews.find_all('div', attrs = {'class': 'pipProfessionalReviews_list'}):\n",
    "                    reviewer_name = row.find('div', attrs = {'class': 'pipProfessionalReviews_authorName'}).text\n",
    "                    reviewer_rating = row.find('span', attrs = {'class': 'wineRatings_rating'}).text\n",
    "                    reviewer_text = row.find('div', attrs = {'class': 'pipSecContent_copy'}).text\n",
    "                    reviewer_text = reviewer_text.replace('\\n', ' ')\n",
    "                    reviewer_text = unicodedata.normalize('NFKD', reviewer_text)                             \n",
    "                    self.prodData['Critical_Reviews'].append(f'({reviewer_name}; {reviewer_rating}; {reviewer_text})')\n",
    "            except Exception:\n",
    "                print('review parse failed')\n",
    "                self.prodData['Critical_Reviews'] = 'nan'\n",
    "\n",
    "            #write data to disk\n",
    "            self.writeProductData()\n",
    "            self.siteMap[self.searchVarietal][self.productURL]['parseStatus'] = 'success'\n",
    "        except Exception:\n",
    "            self.siteMap[self.searchVarietal][self.productURL]['parseStatus'] = 'fail'\n",
    "            print('{} parse failed'.format(self.productURL))\n",
    "\n",
    "    #validated\n",
    "    def writeProductData(self):\n",
    "        try:\n",
    "            with open(self.filePath, 'a') as file:\n",
    "                file.write('{}|{}|{}|{}|{}|{}|{}|{}|{}\\n'.format(self.productURL,\n",
    "                                                                 self.prodData['Product_Name'],\n",
    "                                                                 self.prodData['Product_Variety'],\n",
    "                                                                 self.prodData['Product_Origin'],\n",
    "                                                                 self.prodData['Product_Family'],\n",
    "                                                                 self.prodData['User_Avg_Rating'],\n",
    "                                                                 self.prodData['User_Rating_Count'],\n",
    "                                                                 self.prodData['Winemaker_Description'],\n",
    "                                                                 self.prodData['Critical_Reviews']))\n",
    "            file.close()\n",
    "            self.pullCount += 1\n",
    "            self.siteMap[self.searchVarietal][self.productURL]['writeStatus'] = 'success'\n",
    "        except Exception:\n",
    "            self.siteMap[self.searchVarietal][self.productURL]['writeStatus'] = 'fail'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8265d6dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "wine_com_scraper = Scraper()\n",
    "wine_com_scraper.scrape()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
