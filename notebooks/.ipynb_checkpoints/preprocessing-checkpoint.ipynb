{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing\n",
    "\n",
    "### Summary\n",
    "The code contained within this notebook and sister file \"preprocessing.py\" are intended to perform the bulk of the preprocessing required for nlp modeling tasks. Given that web scraping can result in large dumps of data, this analysis opt to utilize SpaCy to support data cleaning and parsing efforts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from spacy.lang.en.stop_words import STOP_WORDS as stopwords\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#directory locations\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "raw_folder = parent_directory + '/data/wine-com/raw/'\n",
    "processed_folder = parent_directory + '/data/wine-com/processed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1676752918.122732.txt\n"
     ]
    }
   ],
   "source": [
    "file_data = dict()\n",
    "\n",
    "for filename in os.listdir(raw_folder):\n",
    "    print(filename)\n",
    "    file_data[filename] = dict()\n",
    "    with open(raw_folder + filename) as file:\n",
    "        header = next(file)\n",
    "        file_data[filename]['lines'] = file.readlines()\n",
    "        \n",
    "    file_data[filename]['data'] = []\n",
    "\n",
    "    for line in file_data[filename]['lines']:\n",
    "        values = line.strip().split('|')\n",
    "        if len(values) == 7:\n",
    "            row = {\n",
    "                'url': values[0],\n",
    "                'name': values[1],\n",
    "                'variety': values[2],\n",
    "                'origin': values[3],\n",
    "                'type': values[4],\n",
    "                'description': values[5],\n",
    "                'reviews': values[6]\n",
    "            }\n",
    "            file_data[filename]['data'].append(row)\n",
    "            \n",
    "    # Convert the list of dictionaries to a Pandas DataFrame\n",
    "    file_data[filename]['df'] = pd.DataFrame(file_data[filename]['data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class preprocessText(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.nlp = spacy.load('en_core_web_sm', disable=['parser', 'ner'])\n",
    "    \n",
    "    def transform(self, X):\n",
    "        docs = self.nlp.pipe(X, disable=[\"parser\", \"ner\"])\n",
    "        cleaned_docs = []\n",
    "        for doc in docs:\n",
    "            cleaned_tokens = [token.text for token in doc if not token.is_punct]\n",
    "            cleaned_doc = \" \".join(cleaned_tokens)\n",
    "            cleaned_docs.append(cleaned_doc)\n",
    "        return cleaned_docs\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = preprocessText()\n",
    "\n",
    "for filename in file_data:\n",
    "    file_data[filename]['df']['description'] = processor.transform(file_data[filename]['df']['description'])\n",
    "    file_data[filename]['df']['reviews'] = processor.transform(file_data[filename]['df']['reviews'])\n",
    "    file_data[filename]['df'].to_csv(processed_folder + filename, \n",
    "                                     sep='|', \n",
    "                                     index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
