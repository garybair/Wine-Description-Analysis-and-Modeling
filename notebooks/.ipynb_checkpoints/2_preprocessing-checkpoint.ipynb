{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing - Wine.com\n",
    "\n",
    "### Summary\n",
    "The code contained within this file aims to reconcile and process data scraped from Wine.com.\n",
    "\n",
    "This implementation observes the following considerations:\n",
    "1. Given that scrapes can and will fail, this script assumes that all files present in the raw folder are to be processed and combined into a master record. Redundant records and processing are prevented through the use of a site map that tracks successfully parsed and written pages.\n",
    "2. Ideally, this data would best be stored in a database with a mininum of two tables, product info and critical reviews, however as this analysis is predominantly interested in review data, a SQL style left join has been deemed adequate for this analysis despite the effect on the overall file size. This is a case of not letting perfect get in the way of good enough or having an \"agile\" mindset.\n",
    "3. The preprocess_text.py file contains the preprocess_text Class to be used in pipeline when addressing novel data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sklearn'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 8\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstring\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m----> 8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbase\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m TransformerMixin\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mnltk\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcorpus\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m stopwords\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sklearn'"
     ]
    }
   ],
   "source": [
    "#imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import time\n",
    "from sklearn.base import TransformerMixin\n",
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#set directory locations\n",
    "current_directory = os.getcwd()\n",
    "parent_directory = os.path.dirname(current_directory)\n",
    "raw_folder = parent_directory + '/data/wine-com/raw/'\n",
    "processed_folder = parent_directory + '/data/wine-com/processed/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1677386048.5362737.txt loading\n",
      "1677417882.7459548.txt loading\n"
     ]
    }
   ],
   "source": [
    "#instantiate data structure to hold initial raw data/critical critical reviews\n",
    "data = dict()\n",
    "review_data = dict()\n",
    "#iterate through wine.com raw data folder\n",
    "for filename in os.listdir(raw_folder):\n",
    "    #determine if file has already been processed - if true pass\n",
    "    if filename not in os.listdir(processed_folder):\n",
    "        print(f'{filename} loading')\n",
    "        data[filename] = dict()\n",
    "        review_data[filename] = dict()\n",
    "        #open file readlines to raw dictionary\n",
    "        with open(raw_folder + filename, newline='\\r\\n') as file:\n",
    "            header = next(file)\n",
    "            data[filename]['lines'] = file.readlines()\n",
    "        #create list to hold parsed records\n",
    "        data[filename]['data'] = []\n",
    "        review_data[filename]['data'] = []\n",
    "        #iterate through records to parse raw data\n",
    "        for line in data[filename]['lines']:\n",
    "            values = line.strip().split('|')\n",
    "            #while redundancy is built into the scraper to ensure that the correct format is achieved,\n",
    "            #this prevents rogue html from causing a bad data load\n",
    "            if len(values) == 9:\n",
    "                row = {\n",
    "                    'product_url': values[0],\n",
    "                    'product_name': values[1],\n",
    "                    'product_variety': values[2],\n",
    "                    'product_origin': values[3],\n",
    "                    'product_family': values[4],\n",
    "                    'user_avg_rating': values[5],\n",
    "                    'user_rating_count': values[6],\n",
    "                    'winemaker_description': values[7]\n",
    "                    #'critical_reviews' is parsed loaded below\n",
    "                }\n",
    "                #write to list to be loaded to dataframe\n",
    "                data[filename]['data'].append(row)\n",
    "                \n",
    "                #parse critical reviews - should be loaded as list\n",
    "                reviews = values[8].split(';')\n",
    "                for review in reviews:\n",
    "                    try:\n",
    "                        reviewer_name, reviewer_rating, reviewer_text = review.split(',')\n",
    "                        review = {\n",
    "                        'product_url': values[0],\n",
    "                        'reviewer_name': reviewer_name,\n",
    "                        'reviewer_rating': reviewer_rating,\n",
    "                        'reviewer_text': reviewer_text\n",
    "                        }\n",
    "                        \n",
    "                        review_data[filename]['data'].append(review)\n",
    "                    except Exception:\n",
    "                        pass      \n",
    "                    \n",
    "        # Convert the list of dictionaries to a Pandas DataFrame\n",
    "        data[filename]['unmerged_df'] = pd.DataFrame(data[filename]['data'])\n",
    "        review_data[filename]['df'] = pd.DataFrame(review_data[filename]['data'])\n",
    "        \n",
    "        #perform sql-style left join of review data onto main df\n",
    "        data[filename]['merged_df'] = pd.merge(data[filename]['unmerged_df'], \n",
    "                                               review_data[filename]['df'], \n",
    "                                               on='product_url', \n",
    "                                               how='left')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a text preprocessor class is created to manage preprocessing of text fields\n",
    "# inheriting from TransformerMixin of Sklearn, this should allow for tying into large sklearn pipeline\n",
    "class preprocess_text(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "    \n",
    "    def transform(self, X):\n",
    "        if isinstance(X, pd.Series):\n",
    "            #standardize type\n",
    "            X = X.astype(str)\n",
    "            # lower text\n",
    "            X = X.str.lower()\n",
    "            # remove punctuation\n",
    "            X = X.str.replace('\\W+', ' ', regex = True)\n",
    "            # tokenize text into individual words\n",
    "            X = X.str.split()\n",
    "            # remove stopwords\n",
    "            X = X.apply(lambda x: [word for word in x if word not in (self.stop_words)])\n",
    "            # join words\n",
    "            X = X.apply(lambda x: ' '.join(x))\n",
    "            return X\n",
    "        elif isinstance(X, pd.DataFrame):\n",
    "            colnames = X.columns\n",
    "            for col in colnames:\n",
    "                #standardize type\n",
    "                X[col] = X[col].astype(str)\n",
    "                # convert text to lowercase\n",
    "                X[col] = X[col].str.lower()\n",
    "                # remove punctuation\n",
    "                X[col] = X[col].str.replace('\\W+', ' ', regex = True)\n",
    "                # tokenize text into individual words\n",
    "                X[col] = X[col].str.split()\n",
    "                # remove stopwords\n",
    "                X[col] = X[col].apply(lambda x: [word for word in x if word not in (self.stop_words)])\n",
    "                # join words\n",
    "                X[col] = X[col].apply(lambda x: ' '.join(x))\n",
    "            return X\n",
    "        else:\n",
    "            return X\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        return self"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean & Write Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "processor = preprocess_text()\n",
    "\n",
    "output_path = processed_folder + f'{time.time()}.txt'\n",
    "\n",
    "for filename in data:\n",
    "    data[filename]['merged_df']['winemaker_description'] = processor.transform(data[filename]['merged_df']['winemaker_description'])\n",
    "    data[filename]['merged_df']['reviewer_text'] = processor.transform(data[filename]['merged_df']['reviewer_text'])\n",
    "    data[filename]['merged_df'].to_csv(output_path,\n",
    "                                       mode='a',\n",
    "                                       header = not os.path.exists(output_path),\n",
    "                                       sep = '|',\n",
    "                                       line_terminator = '\\r\\n',\n",
    "                                       index=False)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
